# Comparing `tmp/toktokenizer-0.1.0.tar.gz` & `tmp/toktokenizer-0.1.1.tar.gz`

## Comparing `toktokenizer-0.1.0.tar` & `toktokenizer-0.1.1.tar`

### file list

```diff
@@ -1,9 +1,14 @@
--rw-r--r--   0        0        0      423 1970-01-01 00:00:00.000000 toktokenizer-0.1.0/Cargo.toml
--rw-r--r--   0      501       20      512 2024-05-10 20:23:50.000000 toktokenizer-0.1.0/README.md
--rw-r--r--   0      501       20     9800 2024-05-10 20:10:27.000000 toktokenizer-0.1.0/src/lib.rs
--rw-r--r--   0      501       20     1752 2024-05-10 20:21:47.000000 toktokenizer-0.1.0/tests.py
--rw-r--r--   0      501       20   570348 2024-05-10 20:21:18.000000 toktokenizer-0.1.0/wikibpe.json
--rw-r--r--   0      501       20   970835 2024-05-10 20:20:56.000000 toktokenizer-0.1.0/wikibpe50k.json
--rw-r--r--   0      501       20    16087 2024-05-10 20:10:06.000000 toktokenizer-0.1.0/Cargo.lock
--rw-r--r--   0      501       20      392 2024-05-10 20:09:08.000000 toktokenizer-0.1.0/pyproject.toml
--rw-r--r--   0        0        0      843 1970-01-01 00:00:00.000000 toktokenizer-0.1.0/PKG-INFO
+-rw-r--r--   0        0        0      406 1970-01-01 00:00:00.000000 toktokenizer-0.1.1/Cargo.toml
+-rw-r--r--   0      501       20      512 2024-05-10 20:23:50.000000 toktokenizer-0.1.1/README.md
+-rw-r--r--   0      501       20     1040 2024-05-10 20:48:47.000000 toktokenizer-0.1.1/examples/bpe.py
+-rw-r--r--   0      501       20     7773 2024-05-10 20:48:47.000000 toktokenizer-0.1.1/examples/tiny_shakespeare_pairs.json
+-rw-r--r--   0      501       20     7703 2024-05-10 21:41:41.000000 toktokenizer-0.1.1/performance.png
+-rw-r--r--   0      501       20    14296 2024-05-10 21:41:40.000000 toktokenizer-0.1.1/plots.ipynb
+-rw-r--r--   0      501       20     9998 2024-05-10 21:59:32.000000 toktokenizer-0.1.1/src/lib.rs
+-rw-r--r--   0      501       20      978 2024-05-10 20:49:37.000000 toktokenizer-0.1.1/tests/tests/test_bpe.py
+-rw-r--r--   0      501       20     1752 2024-05-10 20:21:47.000000 toktokenizer-0.1.1/tests.py
+-rw-r--r--   0      501       20   570348 2024-05-10 20:21:18.000000 toktokenizer-0.1.1/wikibpe.json
+-rw-r--r--   0      501       20   970835 2024-05-10 20:20:56.000000 toktokenizer-0.1.1/wikibpe50k.json
+-rw-r--r--   0      501       20    14874 2024-05-10 22:04:46.000000 toktokenizer-0.1.1/Cargo.lock
+-rw-r--r--   0      501       20      392 2024-05-10 20:09:08.000000 toktokenizer-0.1.1/pyproject.toml
+-rw-r--r--   0        0        0      843 1970-01-01 00:00:00.000000 toktokenizer-0.1.1/PKG-INFO
```

### Comparing `toktokenizer-0.1.0/README.md` & `toktokenizer-0.1.1/README.md`

 * *Files identical despite different names*

### Comparing `toktokenizer-0.1.0/src/lib.rs` & `toktokenizer-0.1.1/src/lib.rs`

 * *Files 3% similar despite different names*

```diff
@@ -69,16 +69,16 @@
 //         i += 1;
 //     }
 // }
 
 #[pyclass]
 pub struct BPETokenizer {
     pub normalizer: DefaultNormalizer,
-    encoder: Map<(Rank, Rank), Rank>, //TODO: make private later
-    decoder: RefCell<Option<Map<Rank, (Rank, Rank)>>>,
+    pub encoder: Map<(Rank, Rank), Rank>, //TODO: make private later
+    pub decoder: RefCell<Option<Map<Rank, (Rank, Rank)>>>,
 }
 
 // impl Tokenizer for BPETokenizer {
 // fn decode(&self, _input_ids: &[Rank]) -> String {
 //     //need to apply merge-outs in reverse order
 //     let decoder: Map<Rank, (Rank, Rank)> = match self.decoder.borrow_mut().take() {
 //         Some(d) => d,
@@ -205,14 +205,24 @@
     #[classmethod]
     pub fn from_pretrained(cls: &Bound<'_, PyType>, file: &str) -> PyResult<Self> {
         let mut tok = Self::new();
         tok.load_encoder(file)?;
         Ok(tok)
     }
 
+    #[getter]
+    pub fn n_vocab(&self) -> usize {
+        self.encoder.len()
+    }
+
+    #[getter]
+    pub fn encoder(&self) -> Map<(Rank, Rank), Rank> {
+        self.encoder.clone()
+    }
+
     pub fn preprocess(&self, text: &str) -> String {
         self.normalizer.normalize(text)
     }
 
     pub fn load_encoder(&mut self, file: &str) -> PyResult<()> {
         let encoder_str = fs::read_to_string(file)?;
```

### Comparing `toktokenizer-0.1.0/tests.py` & `toktokenizer-0.1.1/tests.py`

 * *Files identical despite different names*

### Comparing `toktokenizer-0.1.0/wikibpe.json` & `toktokenizer-0.1.1/wikibpe.json`

 * *Files identical despite different names*

### Comparing `toktokenizer-0.1.0/wikibpe50k.json` & `toktokenizer-0.1.1/wikibpe50k.json`

 * *Files identical despite different names*

### Comparing `toktokenizer-0.1.0/Cargo.lock` & `toktokenizer-0.1.1/Cargo.lock`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,12 @@
 # This file is automatically @generated by Cargo.
 # It is not intended for manual editing.
 version = 3
 
 [[package]]
-name = "ahash"
-version = "0.8.11"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e89da841a80418a9b391ebaea17f5c112ffaaa96f621d2c285b5174da76b9011"
-dependencies = [
- "cfg-if",
- "getrandom",
- "once_cell",
- "version_check",
- "zerocopy",
-]
-
-[[package]]
 name = "anyhow"
 version = "1.0.83"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "25bdb32cbbdce2b519a9cd7df3a678443100e265d5e25ca763b7572a5104f5f3"
 
 [[package]]
 name = "autocfg"
@@ -67,25 +54,14 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "acdd7c62a3665c7f6830a51635d9ac9b23ed385797f70a83bb8bafe9c572ab2b"
 dependencies = [
  "winapi",
 ]
 
 [[package]]
-name = "getrandom"
-version = "0.2.15"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c4567c8db10ae91089c99af84c68c38da3ec2f087c3f82960bcdbf3656b6f4d7"
-dependencies = [
- "cfg-if",
- "libc",
- "wasi",
-]
-
-[[package]]
 name = "heck"
 version = "0.4.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "95505c38b4572b2d910cecb0281560f54b440a19336cbbcb27bf6ce6adc6f5a8"
 
 [[package]]
 name = "indoc"
@@ -367,17 +343,16 @@
 name = "target-lexicon"
 version = "0.12.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e1fc403891a21bcfb7c37834ba66a547a8f402146eba7265b5a6d88059c9ff2f"
 
 [[package]]
 name = "toktokenizer"
-version = "0.1.0"
+version = "0.1.1"
 dependencies = [
- "ahash",
  "pyo3",
  "rustc-hash",
  "serde",
  "serde_json",
  "tqdm",
 ]
 
@@ -401,20 +376,14 @@
 [[package]]
 name = "unindent"
 version = "0.2.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c7de7d73e1754487cb58364ee906a499937a0dfabd86bcb980fa99ec8c8fa2ce"
 
 [[package]]
-name = "version_check"
-version = "0.9.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "49874b5167b65d7193b8aba1567f5c7d93d001cafc34600cee003eda787e483f"
-
-[[package]]
 name = "wasi"
 version = "0.11.0+wasi-snapshot-preview1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"
 
 [[package]]
 name = "winapi"
@@ -563,27 +532,7 @@
 checksum = "ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538"
 
 [[package]]
 name = "windows_x86_64_msvc"
 version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bec47e5bfd1bff0eeaf6d8b485cc1074891a197ab4225d504cb7a1ab88b02bf0"
-
-[[package]]
-name = "zerocopy"
-version = "0.7.34"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ae87e3fcd617500e5d106f0380cf7b77f3c6092aae37191433159dda23cfb087"
-dependencies = [
- "zerocopy-derive",
-]
-
-[[package]]
-name = "zerocopy-derive"
-version = "0.7.34"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "15e934569e47891f7d9411f1a451d947a60e000ab3bd24fbb970f000387d1b3b"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn",
-]
```

### Comparing `toktokenizer-0.1.0/PKG-INFO` & `toktokenizer-0.1.1/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.3
 Name: toktokenizer
-Version: 0.1.0
+Version: 0.1.1
 Classifier: Programming Language :: Rust
 Classifier: Programming Language :: Python :: Implementation :: CPython
 Classifier: Programming Language :: Python :: Implementation :: PyPy
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
 
 # ðŸª™ toktokenizer
```

