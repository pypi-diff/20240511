# Comparing `tmp/gradient_free_optimizers-1.3.0-py3-none-any.whl.zip` & `tmp/gradient_free_optimizers-1.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,131 +1,130 @@
-Zip file size: 103078 bytes, number of entries: 129
--rw-rw-r--  2.0 unx     1384 b- defN 23-Apr-11 08:41 gradient_free_optimizers/__init__.py
--rw-rw-r--  2.0 unx     4393 b- defN 23-Apr-11 05:49 gradient_free_optimizers/high_lvl_api.py
--rw-rw-r--  2.0 unx     1455 b- defN 23-Jan-19 10:42 gradient_free_optimizers/memory.py
+Zip file size: 102051 bytes, number of entries: 128
+-rw-rw-r--  2.0 unx     1384 b- defN 24-May-11 14:47 gradient_free_optimizers/__init__.py
+-rw-rw-r--  2.0 unx     4393 b- defN 24-May-02 15:59 gradient_free_optimizers/high_lvl_api.py
+-rw-rw-r--  2.0 unx     1544 b- defN 24-May-11 14:47 gradient_free_optimizers/memory.py
 -rw-rw-r--  2.0 unx     2763 b- defN 22-Jul-26 13:24 gradient_free_optimizers/print_info.py
 -rw-rw-r--  2.0 unx     2900 b- defN 23-Jan-05 08:23 gradient_free_optimizers/progress_bar.py
--rw-rw-r--  2.0 unx     1045 b- defN 23-Feb-28 10:35 gradient_free_optimizers/results_manager.py
--rw-rw-r--  2.0 unx     5315 b- defN 23-Apr-11 08:41 gradient_free_optimizers/search.py
+-rw-rw-r--  2.0 unx     1045 b- defN 23-Nov-19 14:31 gradient_free_optimizers/results_manager.py
+-rw-rw-r--  2.0 unx     5038 b- defN 24-May-02 15:59 gradient_free_optimizers/search.py
 -rw-rw-r--  2.0 unx      478 b- defN 23-Feb-28 10:35 gradient_free_optimizers/search_statistics.py
--rw-rw-r--  2.0 unx     2263 b- defN 23-Apr-03 16:27 gradient_free_optimizers/stop_run.py
+-rw-rw-r--  2.0 unx     2289 b- defN 24-May-11 14:47 gradient_free_optimizers/stop_run.py
 -rw-rw-r--  2.0 unx      710 b- defN 21-Aug-23 09:01 gradient_free_optimizers/times_tracker.py
--rw-rw-r--  2.0 unx      782 b- defN 23-Apr-03 14:58 gradient_free_optimizers/utils.py
--rw-rw-r--  2.0 unx     1498 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/__init__.py
--rw-rw-r--  2.0 unx      589 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/base_optimizer.py
+-rw-rw-r--  2.0 unx      782 b- defN 24-May-02 15:59 gradient_free_optimizers/utils.py
+-rw-rw-r--  2.0 unx     1498 b- defN 24-Feb-08 18:01 gradient_free_optimizers/optimizers/__init__.py
+-rw-rw-r--  2.0 unx      589 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/base_optimizer.py
 -rw-rw-r--  2.0 unx      207 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/core_optimizer/__init__.py
--rw-rw-r--  2.0 unx     5728 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/core_optimizer/converter.py
--rw-rw-r--  2.0 unx     2530 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py
--rw-rw-r--  2.0 unx     5085 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
--rw-rw-r--  2.0 unx     3758 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py
+-rw-rw-r--  2.0 unx     6180 b- defN 24-May-10 13:09 gradient_free_optimizers/optimizers/core_optimizer/converter.py
+-rw-rw-r--  2.0 unx     2610 b- defN 24-May-11 14:47 gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py
+-rw-rw-r--  2.0 unx     5085 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
+-rw-rw-r--  2.0 unx     3758 b- defN 24-May-09 11:34 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py
+-rw-rw-r--  2.0 unx       78 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/core_optimizer/parameter_tracker/__init__.py
+-rw-rw-r--  2.0 unx      665 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/core_optimizer/parameter_tracker/stochastic_hill_climbing.py
 -rw-rw-r--  2.0 unx      256 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/exp_opt/__init__.py
 -rw-rw-r--  2.0 unx     2210 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py
--rw-rw-r--  2.0 unx      966 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
+-rw-rw-r--  2.0 unx      966 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
 -rw-rw-r--  2.0 unx      567 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/global_opt/__init__.py
--rw-rw-r--  2.0 unx     4522 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py
--rw-rw-r--  2.0 unx     2193 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py
--rw-rw-r--  2.0 unx     3284 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/pattern_search.py
--rw-rw-r--  2.0 unx     3429 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/powells_method.py
--rw-rw-r--  2.0 unx     1121 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py
--rw-rw-r--  2.0 unx      622 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/random_search.py
--rw-rw-r--  2.0 unx      167 b- defN 23-Feb-27 16:52 gradient_free_optimizers/optimizers/grid/__init__.py
--rw-rw-r--  2.0 unx     4414 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/grid/grid_search.py
+-rw-rw-r--  2.0 unx     4522 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py
+-rw-rw-r--  2.0 unx     2193 b- defN 24-May-10 07:41 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py
+-rw-rw-r--  2.0 unx     3284 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/global_opt/pattern_search.py
+-rw-rw-r--  2.0 unx     3439 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/global_opt/powells_method.py
+-rw-rw-r--  2.0 unx     1121 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py
+-rw-rw-r--  2.0 unx      622 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/global_opt/random_search.py
+-rw-rw-r--  2.0 unx      167 b- defN 23-Dec-16 05:55 gradient_free_optimizers/optimizers/grid/__init__.py
+-rw-rw-r--  2.0 unx     4283 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/grid/diagonal_grid_search.py
+-rw-rw-r--  2.0 unx     1274 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/grid/grid_search.py
+-rw-rw-r--  2.0 unx     1201 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/grid/orthogonal_grid_search.py
 -rw-rw-r--  2.0 unx      591 b- defN 21-Dec-07 17:30 gradient_free_optimizers/optimizers/local_opt/__init__.py
--rw-rw-r--  2.0 unx     5694 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
--rw-rw-r--  2.0 unx     2081 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
--rw-rw-r--  2.0 unx      939 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py
--rw-rw-r--  2.0 unx      971 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py
--rw-rw-r--  2.0 unx     2097 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py
--rw-rw-r--  2.0 unx      452 b- defN 22-Jul-26 13:24 gradient_free_optimizers/optimizers/pop_opt/__init__.py
+-rw-rw-r--  2.0 unx     5694 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
+-rw-rw-r--  2.0 unx     2081 b- defN 24-May-09 11:34 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
+-rw-rw-r--  2.0 unx      939 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py
+-rw-rw-r--  2.0 unx      917 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py
+-rw-rw-r--  2.0 unx     1783 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py
+-rw-rw-r--  2.0 unx      452 b- defN 24-Feb-08 18:01 gradient_free_optimizers/optimizers/pop_opt/__init__.py
 -rw-rw-r--  2.0 unx      309 b- defN 22-Jan-02 09:46 gradient_free_optimizers/optimizers/pop_opt/_individual.py
 -rw-rw-r--  2.0 unx     1478 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/pop_opt/_particle.py
 -rw-rw-r--  2.0 unx     1546 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/pop_opt/_spiral.py
--rw-rw-r--  2.0 unx     2644 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py
--rw-rw-r--  2.0 unx     3013 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py
--rw-rw-r--  2.0 unx     2401 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py
--rw-rw-r--  2.0 unx     2149 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py
--rw-rw-r--  2.0 unx     2377 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py
+-rw-rw-r--  2.0 unx     2644 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py
+-rw-rw-r--  2.0 unx     3013 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py
+-rw-rw-r--  2.0 unx     2401 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py
+-rw-rw-r--  2.0 unx     2149 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py
+-rw-rw-r--  2.0 unx     2377 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py
 -rw-rw-r--  2.0 unx      359 b- defN 21-Dec-07 17:30 gradient_free_optimizers/optimizers/smb_opt/__init__.py
--rw-rw-r--  2.0 unx     1540 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py
--rw-rw-r--  2.0 unx     2013 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py
--rw-rw-r--  2.0 unx     2614 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/sampling.py
--rw-rw-r--  2.0 unx     5087 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/smb_opt/smbo.py
+-rw-rw-r--  2.0 unx     1540 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py
+-rw-rw-r--  2.0 unx     2013 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py
+-rw-rw-r--  2.0 unx     2611 b- defN 24-May-09 13:43 gradient_free_optimizers/optimizers/smb_opt/sampling.py
+-rw-rw-r--  2.0 unx     5441 b- defN 24-May-11 14:47 gradient_free_optimizers/optimizers/smb_opt/smbo.py
 -rw-rw-r--  2.0 unx     3710 b- defN 22-Jan-31 13:47 gradient_free_optimizers/optimizers/smb_opt/surrogate_models.py
--rw-rw-r--  2.0 unx     2201 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py
+-rw-rw-r--  2.0 unx     2201 b- defN 24-May-02 15:59 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py
 -rw-rw-r--  2.0 unx      177 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/__init__.py
 -rw-rw-r--  2.0 unx     1206 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/expected_improvement.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/__init__.py
 -rw-rw-r--  2.0 unx     1716 b- defN 21-Dec-07 17:30 tests/_test_debug.py
--rw-rw-r--  2.0 unx      686 b- defN 23-Apr-03 14:58 tests/_test_examples.py
--rw-rw-r--  2.0 unx     5502 b- defN 23-Apr-03 14:58 tests/_test_memory.py
+-rw-rw-r--  2.0 unx      686 b- defN 24-May-02 15:59 tests/_test_examples.py
+-rw-rw-r--  2.0 unx     5502 b- defN 24-May-02 15:59 tests/_test_memory.py
 -rw-rw-r--  2.0 unx     1799 b- defN 21-Jan-14 11:38 tests/test_attributes.py
 -rw-rw-r--  2.0 unx    13523 b- defN 23-Feb-28 10:35 tests/test_converter.py
 -rw-rw-r--  2.0 unx     6593 b- defN 21-Dec-07 17:30 tests/test_early_stop.py
--rw-rw-r--  2.0 unx     1096 b- defN 21-Nov-30 09:18 tests/test_issue_15.py
+-rw-rw-r--  2.0 unx     1171 b- defN 24-May-09 13:43 tests/test_issue_15.py
 -rw-rw-r--  2.0 unx     1765 b- defN 21-Dec-07 17:30 tests/test_max_score.py
 -rw-rw-r--  2.0 unx     1748 b- defN 21-Dec-07 17:30 tests/test_objective_functions.py
 -rw-rw-r--  2.0 unx     3009 b- defN 21-Dec-07 17:30 tests/test_results.py
 -rw-rw-r--  2.0 unx     1283 b- defN 21-Jan-14 11:38 tests/test_verbosity.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-11 13:44 tests/local_test_performance/__init__.py
+-rw-rw-r--  2.0 unx     1133 b- defN 24-May-09 13:43 tests/local_test_performance/local_test_global_opt.py
+-rw-rw-r--  2.0 unx     2418 b- defN 24-May-09 13:43 tests/local_test_performance/local_test_grid_search.py
+-rw-rw-r--  2.0 unx     1155 b- defN 24-May-11 13:44 tests/local_test_performance/local_test_local_opt.py
+-rw-rw-r--  2.0 unx     1940 b- defN 24-May-09 13:43 tests/local_test_performance/local_test_pop_opt.py
+-rw-rw-r--  2.0 unx     1913 b- defN 24-May-09 13:43 tests/local_test_performance/local_test_smb_opt.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-02 15:59 tests/test_empty_output/__init__.py
+-rw-rw-r--  2.0 unx     1756 b- defN 24-May-11 14:47 tests/test_empty_output/non_verbose.py
+-rw-rw-r--  2.0 unx      691 b- defN 24-May-11 14:47 tests/test_empty_output/test_empty_output.py
+-rw-rw-r--  2.0 unx      516 b- defN 24-May-02 15:59 tests/test_empty_output/verbose.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_optimizers/__init__.py
--rw-rw-r--  2.0 unx     5431 b- defN 23-Feb-28 10:46 tests/test_optimizers/_parametrize.py
--rw-rw-r--  2.0 unx      938 b- defN 23-Apr-03 14:58 tests/test_optimizers/_test_max_time.py
--rw-rw-r--  2.0 unx     1735 b- defN 23-Apr-03 14:58 tests/test_optimizers/_test_memory_warm_start.py
+-rw-rw-r--  2.0 unx     5431 b- defN 24-May-09 11:38 tests/test_optimizers/_parametrize.py
+-rw-rw-r--  2.0 unx      938 b- defN 24-May-02 15:59 tests/test_optimizers/_test_max_time.py
+-rw-rw-r--  2.0 unx     1735 b- defN 24-May-02 15:59 tests/test_optimizers/_test_memory_warm_start.py
 -rw-rw-r--  2.0 unx     1475 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_backend_api.py
 -rw-rw-r--  2.0 unx     1881 b- defN 22-Jul-26 13:24 tests/test_optimizers/test_best_results.py
--rw-rw-r--  2.0 unx     3195 b- defN 23-Apr-11 08:41 tests/test_optimizers/test_constr_opt.py
+-rw-rw-r--  2.0 unx     3195 b- defN 24-May-02 15:59 tests/test_optimizers/test_constr_opt.py
 -rw-rw-r--  2.0 unx     6818 b- defN 22-Oct-19 17:34 tests/test_optimizers/test_early_stop.py
--rw-rw-r--  2.0 unx     1839 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_exploration.py
 -rw-rw-r--  2.0 unx     1775 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_inf_nan.py
 -rw-rw-r--  2.0 unx     3050 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_initializers.py
--rw-rw-r--  2.0 unx     1547 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_large_search_space.py
+-rw-rw-r--  2.0 unx     1547 b- defN 24-May-02 15:59 tests/test_optimizers/test_large_search_space.py
 -rw-rw-r--  2.0 unx     1744 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_max_score.py
 -rw-rw-r--  2.0 unx     5610 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_multiple_searches.py
 -rw-rw-r--  2.0 unx      370 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_names.py
--rw-rw-r--  2.0 unx     1624 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_opt_algos_simple.py
+-rw-rw-r--  2.0 unx     1656 b- defN 24-May-09 13:43 tests/test_optimizers/test_opt_algos_simple.py
 -rw-rw-r--  2.0 unx     1794 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_random_seed.py
--rw-rw-r--  2.0 unx     3918 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_random_state.py
+-rw-rw-r--  2.0 unx     4121 b- defN 24-May-09 13:43 tests/test_optimizers/test_random_state.py
 -rw-rw-r--  2.0 unx     1511 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_results.py
 -rw-rw-r--  2.0 unx      632 b- defN 22-Oct-22 07:41 tests/test_optimizers/test_search_space_.py
 -rw-rw-r--  2.0 unx      704 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_search_step.py
--rw-rw-r--  2.0 unx     1560 b- defN 23-Apr-11 08:25 tests/test_optimizers/test_search_tracker.py
+-rw-rw-r--  2.0 unx     1560 b- defN 24-May-03 05:38 tests/test_optimizers/test_search_tracker.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/__init__.py
 -rw-rw-r--  2.0 unx      598 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/_base_para_test.py
--rw-rw-r--  2.0 unx     3400 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py
+-rw-rw-r--  2.0 unx     3459 b- defN 24-May-11 14:47 tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py
 -rw-rw-r--  2.0 unx     1397 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_evolution_strategy_para_init.py
--rw-rw-r--  2.0 unx     2818 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py
+-rw-rw-r--  2.0 unx     2877 b- defN 24-May-11 14:47 tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py
+-rw-rw-r--  2.0 unx      559 b- defN 24-May-09 13:43 tests/test_optimizers/test_parameter/test_grid_search_para_init.py
 -rw-rw-r--  2.0 unx      852 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_hill_climbing_para_init.py
+-rw-rw-r--  2.0 unx     2611 b- defN 24-May-11 14:47 tests/test_optimizers/test_parameter/test_lipschitz_para_init.py
 -rw-rw-r--  2.0 unx     1237 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_parallel_tempering_para_init.py
 -rw-rw-r--  2.0 unx     1305 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_pso_para_init.py
 -rw-rw-r--  2.0 unx      795 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_rand_rest_hill_climbing_para_init.py
 -rw-rw-r--  2.0 unx      870 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_random_annealing_para_init.py
 -rw-rw-r--  2.0 unx      950 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_simulated_annealing_para_init.py
--rw-rw-r--  2.0 unx      918 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_stochastic_hill_climbing_para_init.py
+-rw-rw-r--  2.0 unx      799 b- defN 24-May-09 13:43 tests/test_optimizers/test_parameter/test_stochastic_hill_climbing_para_init.py
 -rw-rw-r--  2.0 unx      790 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_tabu_search_init.py
--rw-rw-r--  2.0 unx     2757 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_tpe_para_init.py
--rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_parameters/__init__.py
--rw-rw-r--  2.0 unx      650 b- defN 23-Feb-07 15:42 tests/test_parameters/test_hill_climbing_para.py
--rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/__init__.py
--rw-rw-r--  2.0 unx      598 b- defN 21-Mar-11 04:59 tests/test_parameters/test_parameter_init/_base_para_test.py
--rw-rw-r--  2.0 unx     3400 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_bayesian_optimizer_para_init.py
--rw-rw-r--  2.0 unx     1397 b- defN 21-Jun-04 05:55 tests/test_parameters/test_parameter_init/test_evolution_strategy_para_init.py
--rw-rw-r--  2.0 unx     2818 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_forest_optimizer_para_init.py
--rw-rw-r--  2.0 unx      852 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/test_hill_climbing_para_init.py
--rw-rw-r--  2.0 unx     1237 b- defN 21-Jun-04 05:55 tests/test_parameters/test_parameter_init/test_parallel_tempering_para_init.py
--rw-rw-r--  2.0 unx     1305 b- defN 21-Jun-04 05:55 tests/test_parameters/test_parameter_init/test_pso_para_init.py
--rw-rw-r--  2.0 unx      795 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/test_rand_rest_hill_climbing_para_init.py
--rw-rw-r--  2.0 unx      870 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/test_random_annealing_para_init.py
--rw-rw-r--  2.0 unx      950 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/test_simulated_annealing_para_init.py
--rw-rw-r--  2.0 unx      918 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/test_stochastic_hill_climbing_para_init.py
--rw-rw-r--  2.0 unx      790 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/test_tabu_search_init.py
--rw-rw-r--  2.0 unx     2757 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_tpe_para_init.py
--rw-rw-r--  2.0 unx        0 b- defN 21-Nov-30 12:42 tests/test_performance/__init__.py
--rw-rw-r--  2.0 unx     1101 b- defN 22-Nov-11 16:15 tests/test_performance/test_global_opt.py
--rw-rw-r--  2.0 unx     2397 b- defN 22-Nov-11 16:14 tests/test_performance/test_grid_search.py
--rw-rw-r--  2.0 unx     1155 b- defN 21-Dec-07 17:30 tests/test_performance/test_local_opt.py
--rw-rw-r--  2.0 unx     1909 b- defN 23-Feb-28 10:46 tests/test_performance/test_pop_opt.py
--rw-rw-r--  2.0 unx     1882 b- defN 23-Feb-28 10:46 tests/test_performance/test_smb_opt.py
--rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx    26572 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       31 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    14112 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/RECORD
-129 files, 290498 bytes uncompressed, 79512 bytes compressed:  72.6%
+-rw-rw-r--  2.0 unx     2816 b- defN 24-May-11 14:47 tests/test_optimizers/test_parameter/test_tpe_para_init.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-09 13:43 tests/test_parameters/__init__.py
+-rw-rw-r--  2.0 unx      849 b- defN 24-May-11 14:47 tests/test_parameters/test_grid_search.py
+-rw-rw-r--  2.0 unx      675 b- defN 24-May-09 13:43 tests/test_parameters/test_hill_climbing.py
+-rw-rw-r--  2.0 unx     1010 b- defN 24-May-11 14:47 tests/test_parameters/test_replacement.py
+-rw-rw-r--  2.0 unx     5838 b- defN 24-May-09 13:43 tests/test_parameters/test_simulated_annealing.py
+-rw-rw-r--  2.0 unx     1832 b- defN 24-May-09 13:43 tests/test_parameters/test_stochastic_hill_climbing.py
+-rw-rw-r--  2.0 unx     1069 b- defN 24-May-11 14:47 gradient_free_optimizers-1.4.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    27452 b- defN 24-May-11 14:47 gradient_free_optimizers-1.4.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-May-11 14:47 gradient_free_optimizers-1.4.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       31 b- defN 24-May-11 14:47 gradient_free_optimizers-1.4.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    13837 b- defN 24-May-11 14:47 gradient_free_optimizers-1.4.0.dist-info/RECORD
+128 files, 290197 bytes uncompressed, 78997 bytes compressed:  72.8%
```

## zipnote {}

```diff
@@ -48,14 +48,20 @@
 
 Filename: gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
 Comment: 
 
 Filename: gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py
 Comment: 
 
+Filename: gradient_free_optimizers/optimizers/core_optimizer/parameter_tracker/__init__.py
+Comment: 
+
+Filename: gradient_free_optimizers/optimizers/core_optimizer/parameter_tracker/stochastic_hill_climbing.py
+Comment: 
+
 Filename: gradient_free_optimizers/optimizers/exp_opt/__init__.py
 Comment: 
 
 Filename: gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py
 Comment: 
 
 Filename: gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
@@ -81,17 +87,23 @@
 
 Filename: gradient_free_optimizers/optimizers/global_opt/random_search.py
 Comment: 
 
 Filename: gradient_free_optimizers/optimizers/grid/__init__.py
 Comment: 
 
+Filename: gradient_free_optimizers/optimizers/grid/diagonal_grid_search.py
+Comment: 
+
 Filename: gradient_free_optimizers/optimizers/grid/grid_search.py
 Comment: 
 
+Filename: gradient_free_optimizers/optimizers/grid/orthogonal_grid_search.py
+Comment: 
+
 Filename: gradient_free_optimizers/optimizers/local_opt/__init__.py
 Comment: 
 
 Filename: gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
 Comment: 
 
 Filename: gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
@@ -192,14 +204,44 @@
 
 Filename: tests/test_results.py
 Comment: 
 
 Filename: tests/test_verbosity.py
 Comment: 
 
+Filename: tests/local_test_performance/__init__.py
+Comment: 
+
+Filename: tests/local_test_performance/local_test_global_opt.py
+Comment: 
+
+Filename: tests/local_test_performance/local_test_grid_search.py
+Comment: 
+
+Filename: tests/local_test_performance/local_test_local_opt.py
+Comment: 
+
+Filename: tests/local_test_performance/local_test_pop_opt.py
+Comment: 
+
+Filename: tests/local_test_performance/local_test_smb_opt.py
+Comment: 
+
+Filename: tests/test_empty_output/__init__.py
+Comment: 
+
+Filename: tests/test_empty_output/non_verbose.py
+Comment: 
+
+Filename: tests/test_empty_output/test_empty_output.py
+Comment: 
+
+Filename: tests/test_empty_output/verbose.py
+Comment: 
+
 Filename: tests/test_optimizers/__init__.py
 Comment: 
 
 Filename: tests/test_optimizers/_parametrize.py
 Comment: 
 
 Filename: tests/test_optimizers/_test_max_time.py
@@ -216,17 +258,14 @@
 
 Filename: tests/test_optimizers/test_constr_opt.py
 Comment: 
 
 Filename: tests/test_optimizers/test_early_stop.py
 Comment: 
 
-Filename: tests/test_optimizers/test_exploration.py
-Comment: 
-
 Filename: tests/test_optimizers/test_inf_nan.py
 Comment: 
 
 Filename: tests/test_optimizers/test_initializers.py
 Comment: 
 
 Filename: tests/test_optimizers/test_large_search_space.py
@@ -273,17 +312,23 @@
 
 Filename: tests/test_optimizers/test_parameter/test_evolution_strategy_para_init.py
 Comment: 
 
 Filename: tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py
 Comment: 
 
+Filename: tests/test_optimizers/test_parameter/test_grid_search_para_init.py
+Comment: 
+
 Filename: tests/test_optimizers/test_parameter/test_hill_climbing_para_init.py
 Comment: 
 
+Filename: tests/test_optimizers/test_parameter/test_lipschitz_para_init.py
+Comment: 
+
 Filename: tests/test_optimizers/test_parameter/test_parallel_tempering_para_init.py
 Comment: 
 
 Filename: tests/test_optimizers/test_parameter/test_pso_para_init.py
 Comment: 
 
 Filename: tests/test_optimizers/test_parameter/test_rand_rest_hill_climbing_para_init.py
@@ -303,86 +348,38 @@
 
 Filename: tests/test_optimizers/test_parameter/test_tpe_para_init.py
 Comment: 
 
 Filename: tests/test_parameters/__init__.py
 Comment: 
 
-Filename: tests/test_parameters/test_hill_climbing_para.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/__init__.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/_base_para_test.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_bayesian_optimizer_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_evolution_strategy_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_forest_optimizer_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_hill_climbing_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_parallel_tempering_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_pso_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_rand_rest_hill_climbing_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_random_annealing_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_simulated_annealing_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_stochastic_hill_climbing_para_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_tabu_search_init.py
-Comment: 
-
-Filename: tests/test_parameters/test_parameter_init/test_tpe_para_init.py
-Comment: 
-
-Filename: tests/test_performance/__init__.py
-Comment: 
-
-Filename: tests/test_performance/test_global_opt.py
+Filename: tests/test_parameters/test_grid_search.py
 Comment: 
 
-Filename: tests/test_performance/test_grid_search.py
+Filename: tests/test_parameters/test_hill_climbing.py
 Comment: 
 
-Filename: tests/test_performance/test_local_opt.py
+Filename: tests/test_parameters/test_replacement.py
 Comment: 
 
-Filename: tests/test_performance/test_pop_opt.py
+Filename: tests/test_parameters/test_simulated_annealing.py
 Comment: 
 
-Filename: tests/test_performance/test_smb_opt.py
+Filename: tests/test_parameters/test_stochastic_hill_climbing.py
 Comment: 
 
-Filename: gradient_free_optimizers-1.3.0.dist-info/LICENSE
+Filename: gradient_free_optimizers-1.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: gradient_free_optimizers-1.3.0.dist-info/METADATA
+Filename: gradient_free_optimizers-1.4.0.dist-info/METADATA
 Comment: 
 
-Filename: gradient_free_optimizers-1.3.0.dist-info/WHEEL
+Filename: gradient_free_optimizers-1.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: gradient_free_optimizers-1.3.0.dist-info/top_level.txt
+Filename: gradient_free_optimizers-1.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: gradient_free_optimizers-1.3.0.dist-info/RECORD
+Filename: gradient_free_optimizers-1.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gradient_free_optimizers/__init__.py

```diff
@@ -1,12 +1,12 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-__version__ = "1.3.0"
+__version__ = "1.4.0"
 __license__ = "MIT"
 
 from .high_lvl_api import (
     HillClimbingOptimizer,
     StochasticHillClimbingOptimizer,
     RepulsingHillClimbingOptimizer,
     SimulatedAnnealingOptimizer,
```

## gradient_free_optimizers/memory.py

```diff
@@ -1,36 +1,39 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-import numpy as np
+
+import logging
 import pandas as pd
 
+from multiprocessing.managers import DictProxy
+
 
 class Memory:
-    def __init__(self, warm_start, conv, dict_proxy=None):
+    def __init__(self, warm_start, conv, memory=None):
         self.memory_dict = {}
         self.memory_dict_new = {}
         self.conv = conv
 
-        if dict_proxy is not None:
-            self.memory_dict = dict_proxy
+        if isinstance(self.memory, DictProxy):
+            self.memory_dict = memory
 
         if warm_start is None:
             return
 
         if not isinstance(warm_start, pd.DataFrame):
-            print("Memory warm start must be of type pandas.DataFrame")
-            print("Optimization will continue without memory warm start")
+            logging.warning("Memory warm start must be of type pandas.DataFrame")
+            logging.warning("Optimization will continue without memory warm start")
 
             return
 
         if len(warm_start) == 0:
-            print("Memory warm start has no values in current search space")
-            print("Optimization will continue without memory warm start")
+            logging.warning("Memory warm start has no values in current search space")
+            logging.warning("Optimization will continue without memory warm start")
 
             return
 
         self.memory_dict.update(self.conv.dataframe2memory_dict(warm_start))
 
     def memory(self, objective_function):
         def wrapper(para):
```

## gradient_free_optimizers/search.py

```diff
@@ -1,15 +1,13 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import time
 
-from multiprocessing.managers import DictProxy
-
 from .progress_bar import ProgressBarLVL0, ProgressBarLVL1
 from .times_tracker import TimesTracker
 from .search_statistics import SearchStatistics
 from .memory import Memory
 from .print_info import print_info
 from .stop_run import StopRun
 
@@ -137,21 +135,17 @@
                 self.nth_process, self.n_iter, self.objective_function
             )
         else:
             self.p_bar = ProgressBarLVL0(
                 self.nth_process, self.n_iter, self.objective_function
             )
 
-        if isinstance(self.memory, DictProxy):
-            self.mem = Memory(self.memory_warm_start, self.conv, dict_proxy=self.memory)
-            self.score = self.results_mang.score(
-                self.mem.memory(self.objective_function)
-            )
-        elif self.memory is True:
-            self.mem = Memory(self.memory_warm_start, self.conv)
+        self.mem = Memory(self.memory_warm_start, self.conv, memory=self.memory)
+
+        if self.memory not in [False, None]:
             self.score = self.results_mang.score(
                 self.mem.memory(self.objective_function)
             )
         else:
             self.score = self.results_mang.score(self.objective_function)
 
         self.n_inits_norm = min((self.init.n_inits - self.n_init_total), self.n_iter)
```

## gradient_free_optimizers/stop_run.py

```diff
@@ -1,11 +1,13 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
+
+import logging
 import time
 import numpy as np
 
 
 def time_exceeded(start_time, max_time):
     run_time = time.time() - start_time
     return max_time and run_time > max_time
@@ -13,15 +15,15 @@
 
 def score_exceeded(score_best, max_score):
     return max_score and score_best >= max_score
 
 
 def no_change(score_new_list, early_stopping):
     if "n_iter_no_change" not in early_stopping:
-        print(
+        logging.warning(
             "Warning n_iter_no_change-parameter must be set in order for early stopping to work"
         )
         return False
 
     n_iter_no_change = early_stopping["n_iter_no_change"]
     if len(score_new_list) <= n_iter_no_change:
         return False
```

## gradient_free_optimizers/optimizers/core_optimizer/converter.py

```diff
@@ -5,18 +5,33 @@
 import numpy as np
 import pandas as pd
 
 from functools import reduce
 from typing import Optional
 
 
+def check_numpy_array(search_space):
+    for para_name, dim_values in search_space.items():
+
+        def error_message(wrong_type):
+            return "\n Value in '{}' of search space dictionary must be of type array but is '{}' \n".format(
+                para_name, wrong_type
+            )
+
+        if not isinstance(dim_values, np.ndarray):
+            raise ValueError(error_message(type(dim_values)))
+
+
 class Converter:
     def __init__(self, search_space: dict, constraints: list = None) -> None:
+        check_numpy_array(search_space)
+
         self.n_dimensions = len(search_space)
         self.search_space = search_space
+
         if constraints is None:
             self.constraints = []
         else:
             self.constraints = constraints
 
         self.para_names = list(search_space.keys())
```

## gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py

```diff
@@ -1,7 +1,12 @@
+# Author: Simon Blanke
+# Email: simon.blanke@yahoo.com
+# License: MIT License
+
+
 import scipy
 import random
 import numpy as np
 
 from .search_tracker import SearchTracker
 from .converter import Converter
 from .init_positions import Initializer
```

## gradient_free_optimizers/optimizers/global_opt/powells_method.py

```diff
@@ -52,15 +52,15 @@
         center_pos = []
 
         search_space_1D = OrderedDict()
         for idx, para_name in enumerate(self.conv.para_names):
             if self.current_search_dim == idx:
                 # fill with range of values
                 search_space_pos = self.conv.search_space_positions[idx]
-                search_space_1D[para_name] = search_space_pos
+                search_space_1D[para_name] = np.array(search_space_pos)
 
                 min_pos.append(int(np.amin(search_space_pos)))
                 max_pos.append(int(np.amax(search_space_pos)))
                 center_pos.append(int(np.median(search_space_pos)))
             else:
                 # fill with single value
                 search_space_1D[para_name] = np.array([self.powells_pos[idx]])
```

## gradient_free_optimizers/optimizers/grid/grid_search.py

```diff
@@ -1,118 +1,42 @@
-# Author: ...
-# Email: ...
+# Author: Simon Blanke
+# Email: simon.blanke@yahoo.com
 # License: MIT License
 
-import numpy as np
-
-try:
-    from fractions import gcd
-except:
-    from math import gcd
-
 from ..base_optimizer import BaseOptimizer
+from .diagonal_grid_search import DiagonalGridSearchOptimizer
+from .orthogonal_grid_search import OrthogonalGridSearchOptimizer
 
 
 class GridSearchOptimizer(BaseOptimizer):
     name = "Grid Search"
     _name_ = "grid_search"
     __name__ = "GridSearchOptimizer"
 
     optimizer_type = "global"
     computationally_expensive = False
 
-    def __init__(self, *args, step_size=1, **kwargs):
+    def __init__(self, *args, step_size=1, direction="diagonal", **kwargs):
         super().__init__(*args, **kwargs)
-        self.initial_position = np.zeros((self.conv.n_dimensions,), dtype=int)
-        # current position mapped in [|0,search_space_size-1|] (1D)
-        self.high_dim_pointer = 0
-        # direction is a generator of our search space (prime with search_space_size)
-        self.direction = None
-        # step_size describes how many steps of size direction are jumped at each iteration
-        self.step_size = step_size
 
-    def get_direction(self):
-        """
-        Aim here is to generate a prime number of the search space size we call our direction.
-        As direction is prime with the search space size, we know it is
-        a generator of Z/(search_space_size*Z).
-        """
-        n_dims = self.conv.n_dimensions
-        search_space_size = self.conv.search_space_size
-
-        # Find prime number near search_space_size ** (1/n_dims)
-        dim_root = int(np.round(np.power(search_space_size, 1 / n_dims)))
-        is_prime = False
-
-        while not is_prime:
-            if gcd(int(search_space_size), int(dim_root)) == 1:
-                is_prime = True
-            else:
-                dim_root += -1
-
-        return dim_root
-
-    def grid_move(self):
-        """
-        We convert our pointer of Z/(search_space_size * Z) in a position of our search space.
-        This algorithm uses a bijection from Z/(search_space_size * Z) -> (Z/dim_1*Z)x...x(Z/dim_n*Z).
-        """
-        new_pos = []
-        dim_sizes = self.conv.dim_sizes
-        pointer = self.high_dim_pointer
-
-        # The coordinate of our new position for each dimension is
-        # the quotient of the pointer by the product of remaining dimensions
-        # Describes a bijection from Z/search_space_size*Z -> (Z/dim_1*Z)x...x(Z/dim_n*Z)
-        for dim in range(len(dim_sizes) - 1):
-            new_pos.append(pointer // np.prod(dim_sizes[dim + 1 :]) % dim_sizes[dim])
-            pointer = pointer % np.prod(dim_sizes[dim + 1 :])
-        new_pos.append(pointer)
+        self.step_size = step_size
+        self.direction = direction
 
-        return np.array(new_pos)
+        if direction == "orthogonal":
+            self.grid_search_opt = OrthogonalGridSearchOptimizer(
+                *args, step_size=step_size, **kwargs
+            )
+        elif direction == "diagonal":
+            self.grid_search_opt = DiagonalGridSearchOptimizer(
+                *args, step_size=step_size, **kwargs
+            )
+        else:
+            msg = ""
+            raise Exception(msg)
 
     @BaseOptimizer.track_new_pos
     def iterate(self):
-        # while loop for constraint opt
-        while True:
-            # If this is the first iteration:
-            # Generate the direction and return initial_position
-            if self.direction is None:
-                self.direction = self.get_direction()
-
-                pos_new = self.initial_position
-                if self.conv.not_in_constraint(pos_new):
-                    return pos_new
-                else:
-                    return self.move_random()
-
-            # If this is not the first iteration:
-            # Update high_dim_pointer by taking a step of size step_size * direction.
-
-            # Multiple passes are needed in order to observe the entire search space
-            # depending on the step_size parameter.
-            _, current_pass = self.nth_trial, self.high_dim_pointer % self.step_size
-            current_pass_finished = (
-                (self.nth_trial + 1) * self.step_size // self.conv.search_space_size
-                > self.nth_trial * self.step_size // self.conv.search_space_size
-            )
-            # Begin the next pass if current is finished.
-            if current_pass_finished:
-                self.high_dim_pointer = current_pass + 1
-            else:
-                # Otherwise update pointer in Z/(search_space_size*Z)
-                # using the prime step direction and step_size.
-                self.high_dim_pointer = (
-                    self.high_dim_pointer + self.step_size * self.direction
-                ) % self.conv.search_space_size
-
-            # Compute corresponding position in our search space.
-
-            pos_new = self.grid_move()
-            pos_new = self.conv2pos(pos_new)
-
-            if self.conv.not_in_constraint(pos_new):
-                return pos_new
+        return self.grid_search_opt.iterate()
 
     @BaseOptimizer.track_new_score
     def evaluate(self, score_new):
-        BaseOptimizer.evaluate(self, score_new)
+        self.grid_search_opt.evaluate(score_new)
```

## gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py

```diff
@@ -14,21 +14,19 @@
     __name__ = "SimulatedAnnealingOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
 
     def __init__(self, *args, annealing_rate=0.97, start_temp=1, **kwargs):
         super().__init__(*args, **kwargs)
+
         self.annealing_rate = annealing_rate
         self.start_temp = start_temp
         self.temp = start_temp
 
-    def _accept_default(self):
-        return np.exp(-self._score_norm_default() / self.temp)
-
-    def _accept_adapt(self):
-        return self._score_norm_adapt() / self.temp
+    def _p_accept_default(self):
+        # the 'minus' is omitted, because we maximize a score
+        return np.exp(self._exponent)
 
     def evaluate(self, score_new):
         StochasticHillClimbingOptimizer.evaluate(self, score_new)
-
-        self.temp = self.temp * self.annealing_rate
+        self.temp *= self.annealing_rate
```

## gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py

```diff
@@ -1,76 +1,64 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-import random
+
 import numpy as np
+from random import random
 
 from . import HillClimbingOptimizer
+from ..core_optimizer.parameter_tracker.stochastic_hill_climbing import ParameterTracker
 
 
-class StochasticHillClimbingOptimizer(HillClimbingOptimizer):
+class StochasticHillClimbingOptimizer(HillClimbingOptimizer, ParameterTracker):
     name = "Stochastic Hill Climbing"
     _name_ = "stochastic_hill_climbing"
     __name__ = "StochasticHillClimbingOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
 
-    def __init__(self, *args, p_accept=0.1, norm_factor=1, **kwargs):
+    def __init__(self, *args, p_accept=0.5, **kwargs):
         super().__init__(*args, **kwargs)
-        self.p_accept = p_accept
-        self.norm_factor = norm_factor
 
-        if self.norm_factor == "adaptive":
-            self._accept = self._accept_adapt
-            self.diff_max = 0
-        else:
-            self._accept = self._accept_default
+        self.p_accept = p_accept
+        self.temp = 1
 
+    @ParameterTracker.considered_transitions
     def _consider(self, p_accept):
-        rand = random.uniform(0, self.p_accept)
+        if p_accept >= random():
+            self._execute_transition()
 
-        if rand > p_accept:
-            self._new2current()
+    @ParameterTracker.transitions
+    def _execute_transition(self):
+        self._new2current()
 
-    def _score_norm_default(self):
+    @property
+    def _normalized_energy_state(self):
         denom = self.score_current + self.score_new
 
         if denom == 0:
             return 1
         elif abs(denom) == np.inf:
             return 0
         else:
-            return self.norm_factor * (self.score_current - self.score_new) / denom
+            return (self.score_new - self.score_current) / denom
 
-    def _score_norm_adapt(self):
-        diff = abs(self.score_current - self.score_new)
-        if self.diff_max < diff:
-            self.diff_max = diff
-
-        denom = self.diff_max + diff
-
-        if denom == 0:
-            return 1
-        elif abs(denom) == np.inf:
-            return 0
+    @property
+    def _exponent(self):
+        if self.temp == 0:
+            return -np.inf
         else:
-            return abs(self.diff_max - diff) / denom
-
-    def _accept_default(self):
-        return np.exp(-self._score_norm_default())
+            return self._normalized_energy_state / self.temp
 
-    def _accept_adapt(self):
-        return self._score_norm_adapt()
+    def _p_accept_default(self):
+        return self.p_accept * 2 / (1 + np.exp(self._exponent))
 
     def _transition(self, score_new):
         if score_new <= self.score_current:
-            p_accept = self._accept()
+            p_accept = self._p_accept_default()
             self._consider(p_accept)
 
     def evaluate(self, score_new):
-        HillClimbingOptimizer.evaluate(self, score_new)
-
-        # self._evaluate_new2current(score_new)
         self._transition(score_new)
-        # self._evaluate_current2best()
+        HillClimbingOptimizer.evaluate(self, score_new)
```

## gradient_free_optimizers/optimizers/smb_opt/sampling.py

```diff
@@ -47,27 +47,26 @@
 
         while abs(search_space_size) > self.max_sample_size:
             n_samples_array = []
             for idx, dim_size in enumerate(np.nditer(dim_sizes_temp)):
                 array_diff_ = random.randint(1, dim_size)
                 n_samples_array.append(array_diff_)
 
-                sub = max(int(dim_size - (dim_size ** 0.999)), 1)
+                sub = max(int(dim_size - (dim_size**0.999)), 1)
                 dim_sizes_temp[idx] = np.maximum(1, dim_size - sub)
 
             search_space_size = np.array(n_samples_array).prod()
             if search_space_size == 0:
                 search_space_size = np.inf
 
         return n_samples_array
 
     def random_choices(self, n_samples_array):
         pos_space = []
         for n_samples, dim_size in zip(n_samples_array, self.conv.dim_sizes):
-
             if dim_size > self.dim_max_sample_size:
                 pos_space.append(
                     np.random.randint(low=1, high=dim_size, size=n_samples)
                 )
             else:
                 pos_space.append(
                     np.random.choice(dim_size, size=n_samples, replace=False)
```

## gradient_free_optimizers/optimizers/smb_opt/smbo.py

```diff
@@ -2,43 +2,46 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 from ..local_opt import HillClimbingOptimizer
 from .sampling import InitialSampler
 
+import logging
 import numpy as np
 
 np.seterr(divide="ignore", invalid="ignore")
 
 
 class SMBO(HillClimbingOptimizer):
     def __init__(
         self,
         *args,
         warm_start_smbo=None,
         max_sample_size=10000000,
         sampling={"random": 1000000},
+        replacement=True,
         **kwargs
     ):
         super().__init__(*args, **kwargs)
 
         self.warm_start_smbo = warm_start_smbo
         self.max_sample_size = max_sample_size
         self.sampling = sampling
+        self.replacement = replacement
 
         self.sampler = InitialSampler(self.conv, max_sample_size)
 
         self.init_warm_start_smbo(warm_start_smbo)
 
     def init_warm_start_smbo(self, search_data):
         if search_data is not None:
             # filter out nan and inf
             warm_start_smbo = search_data[
-                ~search_data.isin([np.nan, np.inf, -np.inf]).any(1)
+                ~search_data.isin([np.nan, np.inf, -np.inf]).any(axis=1)
             ]
 
             # filter out elements that are not in search space
             int_idx_list = []
             for para_name in self.conv.para_names:
                 search_data_dim = warm_start_smbo[para_name].values
                 search_space_dim = self.conv.search_space[para_name]
@@ -118,42 +121,49 @@
             warning_message0 = "\n Warning:"
             warning_message1 = (
                 "\n search space size of "
                 + str(self.conv.search_space_size)
                 + " exceeding recommended limit."
             )
             warning_message3 = "\n Reduce search space size for better performance."
-            print(warning_message0 + warning_message1 + warning_message3)
+            logging.warning(warning_message0 + warning_message1 + warning_message3)
 
     @track_X_sample
     def init_pos(self):
         return super().init_pos()
 
     @HillClimbingOptimizer.track_new_pos
     @track_X_sample
     def iterate(self):
         return self._propose_location()
 
+    def _remove_position(self, position):
+        mask = np.all(self.all_pos_comb == position, axis=1)
+        self.all_pos_comb = self.all_pos_comb[np.invert(mask)]
+
     @HillClimbingOptimizer.track_new_score
     @track_y_sample
     def evaluate(self, score_new):
         self._evaluate_new2current(score_new)
         self._evaluate_current2best()
 
+        if not self.replacement:
+            self._remove_position(self.pos_new)
+
     @HillClimbingOptimizer.track_new_score
     @track_y_sample
     def evaluate_init(self, score_new):
         self._evaluate_new2current(score_new)
         self._evaluate_current2best()
 
     def _propose_location(self):
         try:
             self._training()
         except ValueError:
-            print(
+            logging.warning(
                 "Warning: training sequential model failed. Performing random iteration instead."
             )
             return self.move_random()
 
         exp_imp = self._expected_improvement()
 
         index_best = list(exp_imp.argsort()[::-1])
```

## tests/test_issue_15.py

```diff
@@ -3,20 +3,21 @@
 
 
 """ --- test search spaces with mixed int/float types --- """
 
 
 def test_mixed_type_search_space_0():
     def objective_function(para):
-        assert isinstance(para["x1"], int)
+        print("type x1", type(para["x1"]))
+        assert isinstance(para["x1"], (int, np.int64))
 
         return 1
 
     search_space = {
-        "x1": range(10, 20),
+        "x1": np.arange(10, 20),
     }
 
     opt = RandomSearchOptimizer(search_space)
     opt.search(objective_function, n_iter=10000)
 
 
 def test_mixed_type_search_space_1():
@@ -31,19 +32,19 @@
 
     opt = RandomSearchOptimizer(search_space)
     opt.search(objective_function, n_iter=10000)
 
 
 def test_mixed_type_search_space_2():
     def objective_function(para):
-        assert isinstance(para["x1"], int)
+        assert isinstance(para["x1"], (int, np.int64))
         assert isinstance(para["x2"], float)
 
         return 1
 
     search_space = {
-        "x1": range(10, 20),
+        "x1": np.arange(10, 20),
         "x2": np.arange(1, 2, 0.1),
     }
 
     opt = RandomSearchOptimizer(search_space)
     opt.search(objective_function, n_iter=10000)
```

## tests/test_optimizers/test_opt_algos_simple.py

```diff
@@ -17,15 +17,15 @@
     ParticleSwarmOptimizer,
     EvolutionStrategyOptimizer,
     BayesianOptimizer,
     TreeStructuredParzenEstimators,
     ForestOptimizer,
 )
 
-from surfaces.test_functions import SphereFunction
+from surfaces.test_functions.mathematical import SphereFunction
 
 optimizers = (
     "Optimizer",
     [
         (HillClimbingOptimizer),
         (StochasticHillClimbingOptimizer),
         (RepulsingHillClimbingOptimizer),
@@ -49,12 +49,12 @@
 
 sphere_function = SphereFunction(n_dim=2, metric="score")
 
 
 @pytest.mark.parametrize(*optimizers)
 def test_opt_algos_0(Optimizer):
     opt = Optimizer(sphere_function.search_space())
-    opt.search(sphere_function, n_iter=15)
+    opt.search(sphere_function.objective_function, n_iter=15)
 
     _ = opt.best_para
     _ = opt.best_score
     _ = opt.search_data
```

## tests/test_optimizers/test_random_state.py

```diff
@@ -1,15 +1,15 @@
 import pytest
 import time
 import numpy as np
 import pandas as pd
 
 
 from ._parametrize import optimizers_non_deterministic as optimizers
-from surfaces.test_functions import AckleyFunction
+from surfaces.test_functions.mathematical import AckleyFunction
 from gradient_free_optimizers import DirectAlgorithm
 
 ackkley_function = AckleyFunction()
 
 
 def objective_function(para):
     score = -(para["x0"] * para["x0"] + para["x1"] * para["x1"])
@@ -29,21 +29,21 @@
 n_last = n_iter - n_random
 
 
 @pytest.mark.parametrize(*optimizers)
 def test_random_state_0(Optimizer):
     opt0 = Optimizer(search_space, initialize={"random": n_random}, random_state=1)
     opt0.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     opt1 = Optimizer(search_space, initialize={"random": n_random}, random_state=1)
     opt1.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     print("\n opt0.search_data \n", opt0.search_data)
     print("\n opt1.search_data \n", opt1.search_data)
 
     n_last_scores0 = list(opt0.search_data["score"].values)[-n_last:]
@@ -52,41 +52,41 @@
     assert abs(np.sum(n_last_scores0) - np.sum(n_last_scores1)) < err
 
 
 @pytest.mark.parametrize(*optimizers)
 def test_random_state_1(Optimizer):
     opt0 = Optimizer(search_space, initialize={"random": n_random}, random_state=10)
     opt0.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     opt1 = Optimizer(search_space, initialize={"random": n_random}, random_state=10)
     opt1.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     n_last_scores0 = list(opt0.search_data["score"].values)[-n_last:]
     n_last_scores1 = list(opt1.search_data["score"].values)[-n_last:]
 
     assert abs(np.sum(n_last_scores0) - np.sum(n_last_scores1)) < err
 
 
 @pytest.mark.parametrize(*optimizers)
 def test_random_state_2(Optimizer):
     opt0 = Optimizer(search_space, initialize={"random": n_random}, random_state=1)
     opt0.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     opt1 = Optimizer(search_space, initialize={"random": n_random}, random_state=10)
     opt1.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     print("\n opt0.search_data \n", opt0.search_data)
     print("\n opt1.search_data \n", opt1.search_data)
 
     n_last_scores0 = list(opt0.search_data["score"].values)[-n_last:]
@@ -96,23 +96,23 @@
 
 
 def test_random_state_direct():
     opt0 = DirectAlgorithm(
         search_space, initialize={"random": n_random}, random_state=1
     )
     opt0.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     opt1 = DirectAlgorithm(
         search_space, initialize={"random": n_random}, random_state=10
     )
     opt1.search(
-        ackkley_function,
+        ackkley_function.objective_function,
         n_iter=n_iter,
     )
 
     print("\n opt0.search_data \n", opt0.search_data)
     print("\n opt1.search_data \n", opt1.search_data)
 
     n_last_scores0 = list(opt0.search_data["score"].values)[-n_last:]
@@ -120,18 +120,18 @@
 
     assert abs(np.sum(n_last_scores0) - np.sum(n_last_scores1)) < err
 
 
 @pytest.mark.parametrize(*optimizers)
 def test_no_random_state_0(Optimizer):
     opt0 = Optimizer(search_space, initialize={"random": n_random})
-    opt0.search(ackkley_function, n_iter=n_iter)
+    opt0.search(ackkley_function.objective_function, n_iter=n_iter)
 
     opt1 = Optimizer(search_space, initialize={"random": n_random})
-    opt1.search(ackkley_function, n_iter=n_iter)
+    opt1.search(ackkley_function.objective_function, n_iter=n_iter)
 
     print("\n opt0.search_data \n", opt0.search_data)
     print("\n opt1.search_data \n", opt1.search_data)
 
     n_last_scores0 = list(opt0.search_data["score"].values)[-n_last:]
     n_last_scores1 = list(opt1.search_data["score"].values)[-n_last:]
```

## tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py

```diff
@@ -114,14 +114,16 @@
     ({"sampling": False}),
     ({"sampling": {"random": 1}}),
     ({"sampling": {"random": 100000000}}),
     ({"rand_rest_p": 0}),
     ({"rand_rest_p": 0.5}),
     ({"rand_rest_p": 1}),
     ({"rand_rest_p": 10}),
+    ({"replacement": True}),
+    ({"replacement": False}),
 ]
 
 
 pytest_wrapper = ("opt_para", bayesian_optimizer_para)
 
 
 @pytest.mark.parametrize(*pytest_wrapper)
```

## tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py

```diff
@@ -94,14 +94,16 @@
     ({"sampling": False}),
     ({"sampling": {"random": 1}}),
     ({"sampling": {"random": 100000000}}),
     ({"rand_rest_p": 0}),
     ({"rand_rest_p": 0.5}),
     ({"rand_rest_p": 1}),
     ({"rand_rest_p": 10}),
+    ({"replacement": True}),
+    ({"replacement": False}),
 ]
 
 
 pytest_wrapper = ("opt_para", dto_para)
 
 
 @pytest.mark.parametrize(*pytest_wrapper)
```

## tests/test_optimizers/test_parameter/test_stochastic_hill_climbing_para_init.py

```diff
@@ -19,18 +19,14 @@
 
 
 stochastic_hill_climbing_para = hill_climbing_para + [
     ({"p_accept": 0.01}),
     ({"p_accept": 0.5}),
     ({"p_accept": 1}),
     ({"p_accept": 10}),
-    ({"norm_factor": 0.1}),
-    ({"norm_factor": 0.5}),
-    ({"norm_factor": 0.9}),
-    ({"norm_factor": "adaptive"}),
 ]
 
 
 pytest_wrapper = ("opt_para", stochastic_hill_climbing_para)
 
 
 @pytest.mark.parametrize(*pytest_wrapper)
```

## tests/test_optimizers/test_parameter/test_tpe_para_init.py

```diff
@@ -91,14 +91,16 @@
     ({"sampling": False}),
     ({"sampling": {"random": 1}}),
     ({"sampling": {"random": 100000000}}),
     ({"rand_rest_p": 0}),
     ({"rand_rest_p": 0.5}),
     ({"rand_rest_p": 1}),
     ({"rand_rest_p": 10}),
+    ({"replacement": True}),
+    ({"replacement": False}),
 ]
 
 
 pytest_wrapper = ("opt_para", tpe_para)
 
 
 @pytest.mark.parametrize(*pytest_wrapper)
```

## Comparing `tests/test_parameters/test_hill_climbing_para.py` & `tests/test_parameters/test_hill_climbing.py`

 * *Files 23% similar despite different names*

```diff
@@ -17,16 +17,18 @@
 search_space = {
     "x": np.arange(-10, 11, 1),
     "y": np.arange(-10, 11, 1),
 }
 
 
 def test_epsilon_0():
+    epsilon = 1 / np.inf
+
     opt = HillClimbingOptimizer(
-        search_space, initialize={"vertices": 1}, epsilon=0.000001
+        search_space, initialize={"vertices": 1}, epsilon=epsilon
     )
     opt.search(parabola_function, n_iter=100)
 
     search_data = opt.search_data
     scores = search_data["score"].values
 
     assert np.all(scores == -200)
```

## Comparing `tests/test_parameters/test_parameter_init/test_bayesian_optimizer_para_init.py` & `tests/test_optimizers/test_parameter/test_lipschitz_para_init.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,17 +3,15 @@
 # License: MIT License
 
 import time
 import pytest
 import random
 import numpy as np
 
-from gradient_free_optimizers import BayesianOptimizer
-from sklearn.gaussian_process import GaussianProcessRegressor
-from sklearn.gaussian_process.kernels import Matern, WhiteKernel, RBF
+from gradient_free_optimizers import LipschitzOptimizer
 from ._base_para_test import _base_para_test_func
 from gradient_free_optimizers import RandomSearchOptimizer
 
 
 def objective_function_nan(para):
     rand = random.randint(0, 1)
 
@@ -72,64 +70,38 @@
 search_data2 = opt2.search_data
 search_data3 = opt3.search_data
 search_data4 = opt4.search_data
 search_data5 = opt5.search_data
 search_data6 = opt6.search_data
 
 
-class GPR:
-    def __init__(self):
-        nu_param = 0.5
-        matern = Matern(
-            # length_scale=length_scale_param,
-            # length_scale_bounds=length_scale_bounds_param,
-            nu=nu_param,
-        )
-
-        self.gpr = GaussianProcessRegressor(
-            kernel=matern + RBF() + WhiteKernel(), n_restarts_optimizer=1
-        )
-
-    def fit(self, X, y):
-        self.gpr.fit(X, y)
-
-    def predict(self, X, return_std=False):
-        return self.gpr.predict(X, return_std=return_std)
-
-
-bayesian_optimizer_para = [
-    ({"gpr": GPR()}),
-    ({"xi": 0.001}),
-    ({"xi": 0.5}),
-    ({"xi": 0.9}),
+lipschitz_para = [
     ({"warm_start_smbo": None}),
     ({"warm_start_smbo": search_data1}),
     ({"warm_start_smbo": search_data2}),
     ({"warm_start_smbo": search_data3}),
     ({"warm_start_smbo": search_data4}),
     ({"warm_start_smbo": search_data5}),
     ({"warm_start_smbo": search_data6}),
     ({"max_sample_size": 10000000}),
     ({"max_sample_size": 10000}),
     ({"max_sample_size": 1000000000}),
     ({"sampling": False}),
     ({"sampling": {"random": 1}}),
     ({"sampling": {"random": 100000000}}),
-    ({"rand_rest_p": 0}),
-    ({"rand_rest_p": 0.5}),
-    ({"rand_rest_p": 1}),
-    ({"rand_rest_p": 10}),
+    ({"replacement": True}),
+    ({"replacement": False}),
 ]
 
 
-pytest_wrapper = ("opt_para", bayesian_optimizer_para)
+pytest_wrapper = ("opt_para", lipschitz_para)
 
 
 @pytest.mark.parametrize(*pytest_wrapper)
-def test_bayesian_para(opt_para):
-    _base_para_test_func(opt_para, BayesianOptimizer)
+def test_lipschitz_para(opt_para):
+    _base_para_test_func(opt_para, LipschitzOptimizer)
 
 
 def test_warm_start_0():
-    opt = BayesianOptimizer(search_space, warm_start_smbo=search_data1)
+    opt = LipschitzOptimizer(search_space, warm_start_smbo=search_data1)
 
     assert len(opt.X_sample) == 30
```

## Comparing `tests/test_parameters/test_parameter_init/test_hill_climbing_para_init.py` & `tests/test_optimizers/test_parameter/test_grid_search_para_init.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,36 +1,26 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import pytest
 
 
-from gradient_free_optimizers import HillClimbingOptimizer
+from gradient_free_optimizers import GridSearchOptimizer
 from ._base_para_test import _base_para_test_func
 
 
-hill_climbing_para = [
-    ({"epsilon": 0.0001}),
-    ({"epsilon": 1}),
-    ({"epsilon": 10}),
-    ({"epsilon": 10000}),
-    ({"distribution": "normal"}),
-    ({"distribution": "laplace"}),
-    ({"distribution": "logistic"}),
-    ({"distribution": "gumbel"}),
-    ({"n_neighbours": 1}),
-    ({"n_neighbours": 10}),
-    ({"n_neighbours": 100}),
-    ({"rand_rest_p": 0}),
-    ({"rand_rest_p": 0.5}),
-    ({"rand_rest_p": 1}),
-    ({"rand_rest_p": 10}),
+grid_search_para = [
+    ({"step_size": 1}),
+    ({"step_size": 10}),
+    ({"step_size": 10000}),
+    ({"direction": "diagonal"}),
+    ({"direction": "orthogonal"}),
 ]
 
 
-pytest_wrapper = ("opt_para", hill_climbing_para)
+pytest_wrapper = ("opt_para", grid_search_para)
 
 
 @pytest.mark.parametrize(*pytest_wrapper)
-def test_hill_climbing_para(opt_para):
-    _base_para_test_func(opt_para, HillClimbingOptimizer)
+def test_grid_search_para(opt_para):
+    _base_para_test_func(opt_para, GridSearchOptimizer)
```

## Comparing `tests/test_performance/test_global_opt.py` & `tests/local_test_performance/local_test_global_opt.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import pytest
 from tqdm import tqdm
 import numpy as np
 
-from surfaces.test_functions import RastriginFunction
+from surfaces.test_functions.mathematical import RastriginFunction
 
 from gradient_free_optimizers import (
     RandomSearchOptimizer,
     RandomRestartHillClimbingOptimizer,
     RandomAnnealingOptimizer,
 )
 
@@ -31,15 +31,15 @@
     n_opts = 33
     n_iter = 100
 
     scores = []
     for rnd_st in tqdm(range(n_opts)):
         opt = Optimizer(search_space, initialize=initialize, random_state=rnd_st)
         opt.search(
-            ackley_function,
+            ackley_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         scores.append(opt.best_score)
     score_mean = np.array(scores).mean()
```

## Comparing `tests/test_performance/test_grid_search.py` & `tests/local_test_performance/local_test_grid_search.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,28 +2,28 @@
 from tqdm import tqdm
 import numpy as np
 import pandas as pd
 from functools import reduce
 
 from gradient_free_optimizers import GridSearchOptimizer
 
-from surfaces.test_functions import SphereFunction, RastriginFunction
+from surfaces.test_functions.mathematical import SphereFunction, RastriginFunction
 
 
 obj_func_l = (
-    "objective_function",
+    "test_function",
     [
         (SphereFunction(n_dim=1, metric="score")),
         (RastriginFunction(n_dim=1, metric="score")),
     ],
 )
 
 
 @pytest.mark.parametrize(*obj_func_l)
-def test_global_perf_0(objective_function):
+def test_global_perf_0(test_function):
     search_space = {"x0": np.arange(-10, 10, 0.1)}
     initialize = {"vertices": 2}
 
     print(
         "\n np.array(search_space.values()) \n",
         np.array(search_space.values()),
         np.array(search_space.values()).shape,
@@ -37,15 +37,15 @@
 
     scores = []
     for rnd_st in tqdm(range(n_opts)):
         opt = GridSearchOptimizer(
             search_space, initialize=initialize, random_state=rnd_st
         )
         opt.search(
-            objective_function,
+            test_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         scores.append(opt.best_score)
     score_mean = np.array(scores).mean()
@@ -53,24 +53,24 @@
     print("\n score_mean", score_mean)
     print("\n n_iter", n_iter)
 
     assert score_mean > -0.001
 
 
 obj_func_l = (
-    "objective_function",
+    "test_function",
     [
         (SphereFunction(n_dim=2, metric="score")),
         (RastriginFunction(n_dim=2, metric="score")),
     ],
 )
 
 
 @pytest.mark.parametrize(*obj_func_l)
-def test_global_perf_1(objective_function):
+def test_global_perf_1(test_function):
     search_space = {
         "x0": np.arange(-2, 1, 0.1),
         "x1": np.arange(-1, 2, 0.1),
     }
     initialize = {"vertices": 2}
 
     dim_sizes_list = [len(array) for array in search_space.values()]
@@ -81,15 +81,15 @@
 
     scores = []
     for rnd_st in tqdm(range(n_opts)):
         opt = GridSearchOptimizer(
             search_space, initialize=initialize, random_state=rnd_st
         )
         opt.search(
-            objective_function,
+            test_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         scores.append(opt.best_score)
     score_mean = np.array(scores).mean()
```

## Comparing `tests/test_performance/test_local_opt.py` & `tests/local_test_performance/local_test_local_opt.py`

 * *Files identical despite different names*

## Comparing `tests/test_performance/test_pop_opt.py` & `tests/local_test_performance/local_test_pop_opt.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import pytest
 from tqdm import tqdm
 import numpy as np
 
-from surfaces.test_functions import SphereFunction, RastriginFunction
+from surfaces.test_functions.mathematical import SphereFunction, RastriginFunction
 
 from gradient_free_optimizers import (
     ParallelTemperingOptimizer,
     ParticleSwarmOptimizer,
     EvolutionStrategyOptimizer,
     RandomSearchOptimizer,
 )
@@ -19,50 +19,50 @@
         (ParticleSwarmOptimizer),
         (EvolutionStrategyOptimizer),
     ],
 )
 
 
 obj_func_l = (
-    "objective_function",
+    "test_function",
     [
         (SphereFunction(n_dim=2, metric="score")),
         (RastriginFunction(n_dim=2, metric="score")),
     ],
 )
 
 
 @pytest.mark.parametrize(*obj_func_l)
 @pytest.mark.parametrize(*opt_pop_l)
-def test_pop_perf_0(Optimizer, objective_function):
+def test_pop_perf_0(Optimizer, test_function):
     search_space = {
         "x0": np.arange(-100, 101, 0.1),
         "x1": np.arange(-100, 101, 0.1),
     }
     initialize = {"vertices": 4, "random": 6}
 
     n_opts = 10
     n_iter = 1200
 
     scores = []
     scores_rnd = []
     for rnd_st in tqdm(range(n_opts)):
         opt = Optimizer(search_space, initialize=initialize, random_state=rnd_st)
         opt.search(
-            objective_function,
+            test_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         opt_rnd = RandomSearchOptimizer(
             search_space, initialize=initialize, random_state=rnd_st
         )
         opt_rnd.search(
-            objective_function,
+            test_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         scores.append(opt.best_score)
         scores_rnd.append(opt_rnd.best_score)
```

## Comparing `tests/test_performance/test_smb_opt.py` & `tests/local_test_performance/local_test_smb_opt.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import pytest
 from tqdm import tqdm
 import numpy as np
 
-from surfaces.test_functions import SphereFunction, RastriginFunction
+from surfaces.test_functions.mathematical import SphereFunction, RastriginFunction
 
 from gradient_free_optimizers import (
     BayesianOptimizer,
     TreeStructuredParzenEstimators,
     ForestOptimizer,
     RandomSearchOptimizer,
 )
@@ -19,50 +19,50 @@
         (TreeStructuredParzenEstimators),
         # (ForestOptimizer),
     ],
 )
 
 
 obj_func_l = (
-    "objective_function",
+    "test_function",
     [
         (SphereFunction(n_dim=2, metric="score")),
         (RastriginFunction(n_dim=2, metric="score")),
     ],
 )
 
 
 @pytest.mark.parametrize(*obj_func_l)
 @pytest.mark.parametrize(*opt_smbo_l)
-def test_smbo_perf_0(Optimizer, objective_function):
+def test_smbo_perf_0(Optimizer, test_function):
     search_space = {
         "x0": np.arange(-30, 101, 1),
         "x1": np.arange(-100, 31, 1),
     }
     initialize = {"vertices": 4, "random": 3}
 
     n_opts = 10
     n_iter = 20
 
     scores = []
     scores_rnd = []
     for rnd_st in tqdm(range(n_opts)):
         opt = Optimizer(search_space, initialize=initialize, random_state=rnd_st)
         opt.search(
-            objective_function,
+            test_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         opt_rnd = RandomSearchOptimizer(
             search_space, initialize=initialize, random_state=rnd_st
         )
         opt_rnd.search(
-            objective_function,
+            test_function.objective_function,
             n_iter=n_iter,
             memory=False,
             verbosity=False,
         )
 
         scores.append(opt.best_score)
         scores_rnd.append(opt_rnd.best_score)
```

## Comparing `gradient_free_optimizers-1.3.0.dist-info/LICENSE` & `gradient_free_optimizers-1.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `gradient_free_optimizers-1.3.0.dist-info/METADATA` & `gradient_free_optimizers-1.4.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gradient-free-optimizers
-Version: 1.3.0
+Version: 1.4.0
 Summary: 
 Home-page: https://github.com/SimonBlanke/Gradient-Free-Optimizers
 Author: Simon Blanke
 Author-email: simon.blanke@yahoo.com
 License: MIT
 Keywords: optimization
 Classifier: Programming Language :: Python :: 3
@@ -21,19 +21,19 @@
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: Science/Research
 Requires-Python: >=3.5
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: scipy (<2.0.0)
-Requires-Dist: numpy (<2.0.0,>=1.18.1)
-Requires-Dist: pandas (<2.0.0)
-Requires-Dist: scikit-learn (!=0.23.*,>=0.21)
-Requires-Dist: tqdm (<5.0.0,>=4.48.0)
+Requires-Dist: scipy <2.0.0
+Requires-Dist: numpy <2.0.0,>=1.18.1
+Requires-Dist: pandas
+Requires-Dist: scikit-learn !=0.23.*,>=0.21
+Requires-Dist: tqdm <5.0.0,>=4.48.0
 
 <p align="center">
   <br>
   <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers"><img src="./docs/images/gradient_logo_ink.png" height="280"></a>
   <br>
 </p>
 
@@ -846,14 +846,55 @@
 opt = HillClimbingOptimizer(search_space)
 opt.search(model, n_iter=50)
 ```
 
 </details>
 
 
+<details>
+<summary><b>Constrained  Optimization example</b></summary>
+
+```python
+import numpy as np
+from gradient_free_optimizers import RandomSearchOptimizer
+
+
+def convex_function(pos_new):
+    score = -(pos_new["x1"] * pos_new["x1"] + pos_new["x2"] * pos_new["x2"])
+    return score
+
+
+search_space = {
+    "x1": np.arange(-100, 101, 0.1),
+    "x2": np.arange(-100, 101, 0.1),
+}
+
+
+def constraint_1(para):
+    # only values in 'x1' higher than -5 are valid
+    return para["x1"] > -5
+
+
+# put one or more constraints inside a list
+constraints_list = [constraint_1]
+
+
+# pass list of constraints to the optimizer
+opt = RandomSearchOptimizer(search_space, constraints=constraints_list)
+opt.search(convex_function, n_iter=50)
+
+search_data = opt.search_data
+
+# the search-data does not contain any samples where x1 is equal or below -5
+print("\n search_data \n", search_data, "\n")
+```
+
+</details>
+
+
 <br>
 
 ## Roadmap
 
 
 <details>
 <summary><b>v0.3.0</b> :heavy_check_mark:</summary>
@@ -911,44 +952,41 @@
   - [x] add DIRECT algorithm
   - [x] automatically add random initial positions if necessary (often requested)
 
 </details>
 
 
 <details>
-<summary><b>v1.3.0</b> </summary>
+<summary><b>v1.3.0</b> :heavy_check_mark:</summary>
 
-  - [ ] add support for constrained optimization
+  - [x] add support for constrained optimization
 
 </details>
 
+
 <details>
-<summary><b>v1.4.0</b> </summary>
+<summary><b>v1.4.0</b> :heavy_check_mark:</summary>
 
-  - [ ] add API, testing and doc to (better) use GFO as backend-optimization package
-  - [ ] add Grid search paraneter that changes direction of search
-  - [ ] add Random search parameter that enables to avoid replacement of the sampling
-  - [ ] add SMBO parameter that enables to avoid replacement of the sampling
+  - [x] add Grid search parameter that changes direction of search
+  - [x] add SMBO parameter that enables to avoid replacement of the sampling
 
 </details>
 
+
 <details>
-<summary><b>v1.5.0</b> </summary>
+<summary><b>Future releases</b> </summary>
 
   - [ ] add Ant-colony optimization
+  - [ ] add API, testing and doc to (better) use GFO as backend-optimization package
+  - [ ] add Random search parameter that enables to avoid replacement of the sampling
+  - [ ] add other acquisition functions to smbo (Probability of improvement, Entropy search, ...)
 
 </details>
 
-<details>
-<summary><b>v2.0.0</b> </summary>
 
-  - [ ] add other acquisition functions to smbo (Probability of improvement, Entropy search, ...)
-  - [ ] ...
-
-</details>
 
 
 <br>
 
 ## Gradient Free Optimizers <=> Hyperactive
 
 Gradient-Free-Optimizers was created as the optimization backend of the [Hyperactive package](https://github.com/SimonBlanke/Hyperactive). Therefore the algorithms are exactly the same in both packages and deliver the same results. 
@@ -981,16 +1019,16 @@
   <tr>
     <td> Distributed computing </td>
     <td> not supported</td>
     <td> yes, via data sharing at runtime</td>
   </tr>
   <tr>
     <td> Visualization </td>
-    <td> not supported</td>
-    <td> yes, via a streamlit-dashboard</td>
+    <td> via Search-Data-Explorer</td>
+    <td> via Search-Data-Explorer and Progress Board</td>
   </tr>
   </tr>
 </table>
 
 
 
 <br>
```

## Comparing `gradient_free_optimizers-1.3.0.dist-info/RECORD` & `gradient_free_optimizers-1.4.0.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,129 +1,128 @@
-gradient_free_optimizers/__init__.py,sha256=1wuD77k1O2eOjewUOo1guDCmgr4piRONQxZ7NMeToaI,1384
+gradient_free_optimizers/__init__.py,sha256=m5hpWeAQS5W21jVFmsKLWPN8jPLTJZsMBUFlzUOJJAk,1384
 gradient_free_optimizers/high_lvl_api.py,sha256=GjuOoxUQtqA076XsPzvmr3mhtKhMuo4EgP4aNetPzp0,4393
-gradient_free_optimizers/memory.py,sha256=ep60wwNMSTbUioCV0MAajj69P68hqmOTzWwAN0uVJqA,1455
+gradient_free_optimizers/memory.py,sha256=gMLoijgYkWLLSdZ6xhRIlw-VWdY5eMLO-ZxY0KM9I1A,1544
 gradient_free_optimizers/print_info.py,sha256=I2b6hg5sAmyLhfCXZYPt69tX_NTsbP6o-JA7FnGn2MA,2763
 gradient_free_optimizers/progress_bar.py,sha256=fDnZ5CyNoykn2qnCvgY-ySl08Pup0sOI_ID6gWsjBB4,2900
 gradient_free_optimizers/results_manager.py,sha256=vQNaVLLWpfGlrfkCoESW64ZdDUX8DvJUdhHek_YCaro,1045
-gradient_free_optimizers/search.py,sha256=3sDWgHIxfnYGaLETr_QC832MHEFr4HUqfBLlqhJM6YM,5315
+gradient_free_optimizers/search.py,sha256=gTxfIhRIsc7Sc40WXOh6S-8X59PWMKpaSn6_4gKGEvs,5038
 gradient_free_optimizers/search_statistics.py,sha256=AcwdPH1kFnD5-OtAmlbwaXfLV-fi6EzFhk3uz3mXlns,478
-gradient_free_optimizers/stop_run.py,sha256=4u4DA4ilpzGL9L6NN2ky0QxwrLw03EwHguGIvGFLao8,2263
+gradient_free_optimizers/stop_run.py,sha256=t9AwRy_IKXITdBbHAmCZ1nv5mRuiOASXRHQNBgUHM_I,2289
 gradient_free_optimizers/times_tracker.py,sha256=jRB1xOgd2898Ty9OIGoG_a6oJ7gGW53s4GnmYZvp7as,710
 gradient_free_optimizers/utils.py,sha256=X2M6tJ-hJacQqAHcQstdI28jkB0hhUwyFA_w7h8j2pw,782
 gradient_free_optimizers/optimizers/__init__.py,sha256=lXsfj6shH7lY1DJSaJwiqdYg3chaC9OxOMI7DYXabzU,1498
 gradient_free_optimizers/optimizers/base_optimizer.py,sha256=7--DnGQeSCVJ71LNfFpbxOHobqd4VJqCi_5YtjfNwkY,589
 gradient_free_optimizers/optimizers/core_optimizer/__init__.py,sha256=FIDA8-LbTyFBE5hStaafJqpzK7cyrdvBRoJIbEFoe38,207
-gradient_free_optimizers/optimizers/core_optimizer/converter.py,sha256=0HbWniCRsGgBLYtZvXTIAE046DHnRWTNP_KyK59laAs,5728
-gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py,sha256=mXu0zBGF8yZJm-M2ju9eDJ5JdcbXaXMOb3XnMIMyzwg,2530
+gradient_free_optimizers/optimizers/core_optimizer/converter.py,sha256=KI-wxMj2WSr-spKqfidiQ6_s26JY7gz-o4VY42K5bmY,6180
+gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py,sha256=BMFHftOuXyQQN8xDzHIFC-CtkwZzS4cgYj1H8jwAlf0,2610
 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py,sha256=emNFgNEt88ycxSMwKIpaBlVvnPW-dcuO8JfaRO9784g,5085
 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py,sha256=aR_fy6J3jE2Ox29XNLfwmQ6dku8o36nPuFWZouYIEgU,3758
+gradient_free_optimizers/optimizers/core_optimizer/parameter_tracker/__init__.py,sha256=L7EAbp2wvJCp3RKWLE4SpUn0k59CkEBextqgMaS1_VI,78
+gradient_free_optimizers/optimizers/core_optimizer/parameter_tracker/stochastic_hill_climbing.py,sha256=-dBz8gzRGWZk4rld5zHto5ZoGjubNUpDgliM90nvZ0s,665
 gradient_free_optimizers/optimizers/exp_opt/__init__.py,sha256=LPWvfKVliMKw2_jQy1FHV-eyZM5MjbpKuL-q3JTuzQ0,256
 gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py,sha256=tRQJjUuumu-aYCMo_0Ulp2qI7Z-iWUq4jHh7T0HKT3Q,2210
 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py,sha256=yruRTFVczw4pD0r5C5LiDbdm5wo25rdQW6evtw7c7fM,966
 gradient_free_optimizers/optimizers/global_opt/__init__.py,sha256=kkL9j8TfPGzBufPKcbPHAQ53ju6HTs3J8PZdsaeqFl0,567
 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py,sha256=SBb45uolwSQziOtMEBGbe8CebfM44twdyECbZGHZS0s,4522
 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py,sha256=6XbCEe5v47TXtzq4b1RPNLQPCpCRrM2ghSCle8ri7ag,2193
 gradient_free_optimizers/optimizers/global_opt/pattern_search.py,sha256=GqIyC5lmkwhxqhcCf3qO30DsQBu-6D5JSy8wrLv532k,3284
-gradient_free_optimizers/optimizers/global_opt/powells_method.py,sha256=5xBtf0XIKs5rWA8UoSbKJuk7ehZtvLcPLadiuEQcp5w,3429
+gradient_free_optimizers/optimizers/global_opt/powells_method.py,sha256=Mwk8FCUBBpwXJwo2ucWjM4kNPhQOegyvxLviUDghQJY,3439
 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py,sha256=b9X11YdC7Ao-m2oAVFFy_9PYMgau_eFQqXdr7KtrhUg,1121
 gradient_free_optimizers/optimizers/global_opt/random_search.py,sha256=Ct5TBDGGVq7wuSN6ZWPOzrHnPssuP2Y-1NiqtUgZNP8,622
 gradient_free_optimizers/optimizers/grid/__init__.py,sha256=81XQ8K4ij1HYv3oH8egbwmbi1YMeH4Ej2hrDwkzeCRo,167
-gradient_free_optimizers/optimizers/grid/grid_search.py,sha256=k2xXIDNHMz1QoX22PEgwMrcuNeqYttMEV7JcE9hZksM,4414
+gradient_free_optimizers/optimizers/grid/diagonal_grid_search.py,sha256=-AGOn3_nxSCBVVl9IPt6_eey8ECmG9q7K6NmU9X2t_c,4283
+gradient_free_optimizers/optimizers/grid/grid_search.py,sha256=6gi9dOVQQb-FZWZpsrvAoeyKj_mI5c0wkKknXiBXe8o,1274
+gradient_free_optimizers/optimizers/grid/orthogonal_grid_search.py,sha256=O3g5djxHb7M28P-VpZU_DezIPWQnsL4NV2Du8hhGFJk,1201
 gradient_free_optimizers/optimizers/local_opt/__init__.py,sha256=mbXmx0NFu50CSQotpqcGvFcV2j0_tjdEExPFuoTBNGo,591
 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py,sha256=HifSeTKbOs1NJt044ksTyzSFC7Vog7YqmE9EAsgc1fw,5694
 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py,sha256=CXuED1WeEduyaOqsRBVIU1CNLegSIEbVuVtIpaaY1ts,2081
 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py,sha256=VyXimCdmukVXccdX2t7XuD8TV-uVKWvYtT-Q3_n5fZc,939
-gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py,sha256=eGlObRWMag8jJeoMzw13Vp10u6UyOPRdoavR1Mr4DqM,971
-gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py,sha256=FqyLOscIRzJbqkS0mjg-6n8JQn36YNFySy_FVbDXzKI,2097
+gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py,sha256=i5UxnxUQ0RwqnhgOau3AnRa1t9qTZeSnAiZgL1iPMNQ,917
+gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py,sha256=gsOr1iTXMJv607Xw4ouMyDxLb38I0J4g49pMqk_ZLi0,1783
 gradient_free_optimizers/optimizers/pop_opt/__init__.py,sha256=bNJlXHMb0ziYQLrvHIwfWRYdc0mTN_d8WWJc2LjDhTs,452
 gradient_free_optimizers/optimizers/pop_opt/_individual.py,sha256=jNG11NWoDpHI4x7h5Y5_CV_D0EckqkyqoPjf9MvzD6I,309
 gradient_free_optimizers/optimizers/pop_opt/_particle.py,sha256=f5iMsAXZXHK41EDAHdAp87VDh42BFL55mPe_evqLeUo,1478
 gradient_free_optimizers/optimizers/pop_opt/_spiral.py,sha256=J7Wai0jXaUnuLpSE0AvW2xJztjtXMuAcD0pBs6D0pLk,1546
 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py,sha256=y3FO6xQ7zsP27b7lRDwv8tVEYZoq_oSp0BH3j63TJ8I,2644
 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py,sha256=qse7FUMcQT5R9XRr26BqkVD3BQfjMdagz0DeuVyavHY,3013
 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py,sha256=p7b-gnyW8l62vGSn-NGlYlmNWxracfsv8GbiHq9_EJg,2401
 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py,sha256=7enLGKk50dqAsGMQeF1E_Px_grwGYwVpP665_0EXzQo,2149
 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py,sha256=8h8ISUPg4hGczbCrQIjjJqnh7I8yCnTSGgnvhtHEjDQ,2377
 gradient_free_optimizers/optimizers/smb_opt/__init__.py,sha256=ITLZ8DIKx0ZxLT_YpEKYO28TQYXcrUHQYzgDXUDbsgo,359
 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py,sha256=0yr0NODLJsm88gX2UA1hi0qHWwE-xJsrpuBm-Bl34BM,1540
 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py,sha256=tz6bavnIROEUXRCWBFJcLuec3uJzULuf3Nm1476nsJ8,2013
-gradient_free_optimizers/optimizers/smb_opt/sampling.py,sha256=1VV5mHL8mnqJSYyBi-AffZZMiLJVGjUg_b-O3RL-9XE,2614
-gradient_free_optimizers/optimizers/smb_opt/smbo.py,sha256=rcIFCx4G7ZZJ-hxa0oHvybVILY4mm0SisXp8-NobdDI,5087
+gradient_free_optimizers/optimizers/smb_opt/sampling.py,sha256=zHqPSAAdSBlZwvcVxWpT74y-WECDU_Wo1-R_BodaIJE,2611
+gradient_free_optimizers/optimizers/smb_opt/smbo.py,sha256=VeBGx3RzogeUiW7ARnCNebzQkMtLotVIZT0BXiYIZdQ,5441
 gradient_free_optimizers/optimizers/smb_opt/surrogate_models.py,sha256=ynQE209wwP7kEGnUk5-Zio7zo4rnVXCfU1y62zf2t8I,3710
 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py,sha256=jfeLSje8XpJX0v3xVPLXl0XCKMjSCcrr1N-5ze6CaW0,2201
 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/__init__.py,sha256=RfxV3A9aunpUJLu4AEnyE452OBaD1d5eRshKvTbxotI,177
 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/expected_improvement.py,sha256=Y6Zx8ZpfgDkwoYiHSntK5kv8_kQ5-HWOmKcn68EmhWs,1206
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/_test_debug.py,sha256=1485GY-WaPX7AVhqpS0eBst-yKvHiqU1asVP8Q2aPc0,1716
 tests/_test_examples.py,sha256=7QRuve6FWx7oiw39NU2Lzx1eAA3cN_Hn2POBDYpl2ig,686
 tests/_test_memory.py,sha256=h0Lvw_HCcF5aQm5EH2-0zYFDHo8QqYGFlnMUFbB8E3Q,5502
 tests/test_attributes.py,sha256=Qb9-eARszY_Zn62HEdOQ6V14OevDPWQucqOqioQ3GO8,1799
 tests/test_converter.py,sha256=mPfkl3Qxgn4xRlTUEraSwIpoftJp0D1bRv5ha9rcpWg,13523
 tests/test_early_stop.py,sha256=b5tgZZsgXM9hcWARlrOyBNrN13DV75njB6TWAJi90Nw,6593
-tests/test_issue_15.py,sha256=EgsCW85AzrEpAEwOsaJVULHClsyh90-mAHJWaOrYfPs,1096
+tests/test_issue_15.py,sha256=hd0-AekqR5qoBfkcD-Hr6ryI35vnODjIGmvrMzvHeJU,1171
 tests/test_max_score.py,sha256=HhSWV6H54grxWylyfQ2b6mBxcnvRXFl__tKebsvEDEY,1765
 tests/test_objective_functions.py,sha256=UmbmzORkhusZJeNcw4itnDIf91inntvI8WH2JeBaQGE,1748
 tests/test_results.py,sha256=8umHDJkTD0Vt1HcyU11nqroll2OMmI2SurBM5lmvuLU,3009
 tests/test_verbosity.py,sha256=jVZ18KCqn6tnYjoV3IZoe25k3vLjKcvabgmLCozkpgM,1283
+tests/local_test_performance/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tests/local_test_performance/local_test_global_opt.py,sha256=8rNCxxoFqMGxkuPmD-UIH0i5_5vnT3E1pXzod1J-IQ4,1133
+tests/local_test_performance/local_test_grid_search.py,sha256=KsuVv6v4Yd2956HOZ012nCt36QLM1pKCIZzU05i2uSA,2418
+tests/local_test_performance/local_test_local_opt.py,sha256=Ntob_sYLr5ibivJTPuvm5sw8neWTLK9iJYk3jICKHHg,1155
+tests/local_test_performance/local_test_pop_opt.py,sha256=Bgh3lHxE-owtWvksjjRbmc_T-pfGZ08d1pazFFAJ694,1940
+tests/local_test_performance/local_test_smb_opt.py,sha256=a-KNdvAgHwsKg-CVNJ2kCMdPKZnoODMTyT9pZ1SvGPo,1913
+tests/test_empty_output/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tests/test_empty_output/non_verbose.py,sha256=30EPpWHdxg5QX8uTBGA7BVyHCE6uqeqy4aRCyMtK20k,1756
+tests/test_empty_output/test_empty_output.py,sha256=7DnjDFGNmtQg2m9GunemSrk77emIF2dR-dztEqMKTvE,691
+tests/test_empty_output/verbose.py,sha256=IRXtaeJ_TWhn9r_mVVF5k3k58hAqxHfJNZ3hoqqiBN4,516
 tests/test_optimizers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_optimizers/_parametrize.py,sha256=ZvG0uQHSC_6DwWB52LwZea8l0336IKdorJEjfsbJGi0,5431
 tests/test_optimizers/_test_max_time.py,sha256=xoSOrHSosrI4iSpqYuLscrxJtx81-04OhEMlmFEffB8,938
 tests/test_optimizers/_test_memory_warm_start.py,sha256=IYo7b7sOhTTcOBx_6z9nAFcIicPVT2Wu4bImNmA6B7I,1735
 tests/test_optimizers/test_backend_api.py,sha256=sWTaLWIZ4Z3Db49A5NMH3q6MGocQYCbGwVmUDrdBXXk,1475
 tests/test_optimizers/test_best_results.py,sha256=T7I9j0onwWkIWDX6n38CXtL_RW8Qi2Sx0655SvnV-vU,1881
 tests/test_optimizers/test_constr_opt.py,sha256=efybU5uV2CgUxoLSS-AedQD3n5b-wjXtLqgDJuWi45U,3195
 tests/test_optimizers/test_early_stop.py,sha256=GFrLve1iJI-k5lrXSisjHlqFoEFtn3diArNsK4eeLTo,6818
-tests/test_optimizers/test_exploration.py,sha256=A69x23HH0LyySF0hW3BbiRlqnUObkgD7L5Jm2sRS2hY,1839
 tests/test_optimizers/test_inf_nan.py,sha256=QctRomsGDLaZGHvrA7hggbwiqYjDZsQ0PwiA_rCfJeQ,1775
 tests/test_optimizers/test_initializers.py,sha256=hMp_5PtVBtTTHfKm4CuxEy28QMjBnSxvaQZB6kNxgW0,3050
 tests/test_optimizers/test_large_search_space.py,sha256=H0I8aVQZ7_-wrZpLnVdO0E6sAId-qCqpnKKqGkPlImw,1547
 tests/test_optimizers/test_max_score.py,sha256=C-ZFQqCUKJuvq-7TkyG5UfvBGwZTm6mlTSBHDcwTTvw,1744
 tests/test_optimizers/test_multiple_searches.py,sha256=ao1oRqwvXVDXI7F6OfBcTkb_1Qw-grMe5XFQOkuTPwQ,5610
 tests/test_optimizers/test_names.py,sha256=Fg4ZFuPGKRM3CrNJKZjTEyTdkgV3ryOp2SgQ3K9yBNQ,370
-tests/test_optimizers/test_opt_algos_simple.py,sha256=fQ54OjSHA9DaPbgSu_3APs1r-r7DVBEy9yn8LHH-020,1624
+tests/test_optimizers/test_opt_algos_simple.py,sha256=f5axHePTRt3dDYlSWvxfynROPJwvxTpwtseL9TkyM98,1656
 tests/test_optimizers/test_random_seed.py,sha256=a1kWryh_9kDtvTPmcqSbmIAN0sPmFRtbbWxpzUZ-V-g,1794
-tests/test_optimizers/test_random_state.py,sha256=gFzAqmBjuTFDF4PKzgIku0tSdFR2sGFUf9XmNzxhh6o,3918
+tests/test_optimizers/test_random_state.py,sha256=mMyS5Zz0U7MWH1fyH_bfAjx5v20Kahl7WlcgjEpL4r8,4121
 tests/test_optimizers/test_results.py,sha256=fMs69Wn5Cwx849AAMe2nQaT2Uj8gIkPjoxBkGtpcHQo,1511
 tests/test_optimizers/test_search_space_.py,sha256=OFjqgBgvmGKYw9gkambxit17iX8nmf0h6Y5b1LeoQZ4,632
 tests/test_optimizers/test_search_step.py,sha256=INfOg7ZKzHxMUH8RW-StvFYvVvIqYMHtVnKjDmBBfpo,704
 tests/test_optimizers/test_search_tracker.py,sha256=0zOZZ2Xm6youu55BbRsXq2c9FK_ZH6DKehprHASGd0E,1560
 tests/test_optimizers/test_parameter/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_optimizers/test_parameter/_base_para_test.py,sha256=9Csihf9ia7Q8W2AwTNePRUuQV07CLqUSqnIA-O6ga6c,598
-tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py,sha256=NGFPsepusEcJoUT93dhHiT4Q84zLcRwZCBzTHgI2phY,3400
+tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py,sha256=LQRzMVYo4DvrEfHz-5ysgHtihMF1XfC8gt3aucGMCT4,3459
 tests/test_optimizers/test_parameter/test_evolution_strategy_para_init.py,sha256=fTeUjfLCOek2AzinlIAB6zVBWFgIV4Wyq2laRjeUAZk,1397
-tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py,sha256=YKatwnWWIqlPdfoMSpsVr4-BHBNq1xrdblnW09eXgWw,2818
+tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py,sha256=EYp-XODtOCmfWZLv8O_Y8LVP342o1YJJECJXfzOsono,2877
+tests/test_optimizers/test_parameter/test_grid_search_para_init.py,sha256=vC_LbCmFvKyXnLCsrr2-OJ5Ekz7kWdbCNsZUhythcvs,559
 tests/test_optimizers/test_parameter/test_hill_climbing_para_init.py,sha256=dm10AZMLvyKm5n65lDt3X51VlqpGhCdvogMNfAmX65Q,852
+tests/test_optimizers/test_parameter/test_lipschitz_para_init.py,sha256=IOXezrZHHI5VGM37u0SfqFHckkFVEy_pn_QPw5DvtVY,2611
 tests/test_optimizers/test_parameter/test_parallel_tempering_para_init.py,sha256=ZtGGtAIs5zUW3e9YksfmJsN9ggjH08X8MjCVnFX07zs,1237
 tests/test_optimizers/test_parameter/test_pso_para_init.py,sha256=9LOB8GBCCpHgsGDJlu3JstaMIo2jyf9CwhY0ad5IKNg,1305
 tests/test_optimizers/test_parameter/test_rand_rest_hill_climbing_para_init.py,sha256=lR0X-3_eI4DI-RVGAIu_0A-VHX9mdr75rRnKrRc3GMk,795
 tests/test_optimizers/test_parameter/test_random_annealing_para_init.py,sha256=tStlYcFanGAR5lF0S1mwtxOZCbBQGJaTcOIj4ZwzzqE,870
 tests/test_optimizers/test_parameter/test_simulated_annealing_para_init.py,sha256=s0vPaYyWJY3-f9b3tCnctY9YTssr1Ghy3EkZ3_nxO5w,950
-tests/test_optimizers/test_parameter/test_stochastic_hill_climbing_para_init.py,sha256=fK1qC0ZWb0lqSj7-EPgP8m39Fs4ikrbHMQcfL_KDRrc,918
+tests/test_optimizers/test_parameter/test_stochastic_hill_climbing_para_init.py,sha256=9rBt0iNM6QDeYusbJ1NVdkz_nMHUTQVMZ9FelxNMRuU,799
 tests/test_optimizers/test_parameter/test_tabu_search_init.py,sha256=VzrqqPtU3NZOCxVU1TQW8vbA71FSI8f8ZxZ9BqyORoc,790
-tests/test_optimizers/test_parameter/test_tpe_para_init.py,sha256=bGY4yVqdkBVNKh5LvWW_KANo3uGIvj7YbsHSTiQxKlw,2757
+tests/test_optimizers/test_parameter/test_tpe_para_init.py,sha256=bBlK0W0znfE1Ypy0VZcbPZz8JGJIH9kHrRSnS3au0NU,2816
 tests/test_parameters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/test_parameters/test_hill_climbing_para.py,sha256=qBk43pDviBhNtFQx-LUz9NVoMqB3B1p17d6tv4vVO30,650
-tests/test_parameters/test_parameter_init/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/test_parameters/test_parameter_init/_base_para_test.py,sha256=9Csihf9ia7Q8W2AwTNePRUuQV07CLqUSqnIA-O6ga6c,598
-tests/test_parameters/test_parameter_init/test_bayesian_optimizer_para_init.py,sha256=NGFPsepusEcJoUT93dhHiT4Q84zLcRwZCBzTHgI2phY,3400
-tests/test_parameters/test_parameter_init/test_evolution_strategy_para_init.py,sha256=fTeUjfLCOek2AzinlIAB6zVBWFgIV4Wyq2laRjeUAZk,1397
-tests/test_parameters/test_parameter_init/test_forest_optimizer_para_init.py,sha256=YKatwnWWIqlPdfoMSpsVr4-BHBNq1xrdblnW09eXgWw,2818
-tests/test_parameters/test_parameter_init/test_hill_climbing_para_init.py,sha256=dm10AZMLvyKm5n65lDt3X51VlqpGhCdvogMNfAmX65Q,852
-tests/test_parameters/test_parameter_init/test_parallel_tempering_para_init.py,sha256=ZtGGtAIs5zUW3e9YksfmJsN9ggjH08X8MjCVnFX07zs,1237
-tests/test_parameters/test_parameter_init/test_pso_para_init.py,sha256=9LOB8GBCCpHgsGDJlu3JstaMIo2jyf9CwhY0ad5IKNg,1305
-tests/test_parameters/test_parameter_init/test_rand_rest_hill_climbing_para_init.py,sha256=lR0X-3_eI4DI-RVGAIu_0A-VHX9mdr75rRnKrRc3GMk,795
-tests/test_parameters/test_parameter_init/test_random_annealing_para_init.py,sha256=tStlYcFanGAR5lF0S1mwtxOZCbBQGJaTcOIj4ZwzzqE,870
-tests/test_parameters/test_parameter_init/test_simulated_annealing_para_init.py,sha256=s0vPaYyWJY3-f9b3tCnctY9YTssr1Ghy3EkZ3_nxO5w,950
-tests/test_parameters/test_parameter_init/test_stochastic_hill_climbing_para_init.py,sha256=fK1qC0ZWb0lqSj7-EPgP8m39Fs4ikrbHMQcfL_KDRrc,918
-tests/test_parameters/test_parameter_init/test_tabu_search_init.py,sha256=VzrqqPtU3NZOCxVU1TQW8vbA71FSI8f8ZxZ9BqyORoc,790
-tests/test_parameters/test_parameter_init/test_tpe_para_init.py,sha256=bGY4yVqdkBVNKh5LvWW_KANo3uGIvj7YbsHSTiQxKlw,2757
-tests/test_performance/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/test_performance/test_global_opt.py,sha256=AfZSULL_Xd6ujhdCJlE_kN0BwVa7OFCUzu0rIUkXMx4,1101
-tests/test_performance/test_grid_search.py,sha256=KanTgPTI7XKWY5tUFSFA0jO6vmWCD6UlLm2gjDkJMPM,2397
-tests/test_performance/test_local_opt.py,sha256=Ntob_sYLr5ibivJTPuvm5sw8neWTLK9iJYk3jICKHHg,1155
-tests/test_performance/test_pop_opt.py,sha256=humLFMMbvZ_mJJLN3rUyop07MTp65Cmm4BEvKCReVNY,1909
-tests/test_performance/test_smb_opt.py,sha256=nMeWlSXMT5kBL1rlxeBza0vlvWOOWj0loFyiYHm81GA,1882
-gradient_free_optimizers-1.3.0.dist-info/LICENSE,sha256=RsfzUEwfUDjkq2Uk3ATEXNtAOa2WqG-6zBP_S5ERJ5I,1069
-gradient_free_optimizers-1.3.0.dist-info/METADATA,sha256=tlNEEMb5jSWqDiu2GsARMAwtzUGnq5xTSt4dkGOzKg4,26572
-gradient_free_optimizers-1.3.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-gradient_free_optimizers-1.3.0.dist-info/top_level.txt,sha256=vvSJQaQ3s7mZx_5ZoYAS3sBswwsRh2xhcx1vmlwhjrQ,31
-gradient_free_optimizers-1.3.0.dist-info/RECORD,,
+tests/test_parameters/test_grid_search.py,sha256=Qfd0zXRgOxDA778-zRJmqDSZozxz-eHMoCBVpq7BVo0,849
+tests/test_parameters/test_hill_climbing.py,sha256=7RcBznoT55QrTFuReV7tSpIq1uHYLm3vkeiqvELgoBc,675
+tests/test_parameters/test_replacement.py,sha256=mMcp62C0FeeBE01jP-Ic2-hHNsPk_Qt8UaynPFjmBCM,1010
+tests/test_parameters/test_simulated_annealing.py,sha256=M73shVPiViKZM-iAREk1CdRSRcmg190MdjARV_yRc4k,5838
+tests/test_parameters/test_stochastic_hill_climbing.py,sha256=d2rABk4cENv3pXGNCn2S6hDoTu2wlVKiggdWd0EYKIY,1832
+gradient_free_optimizers-1.4.0.dist-info/LICENSE,sha256=RsfzUEwfUDjkq2Uk3ATEXNtAOa2WqG-6zBP_S5ERJ5I,1069
+gradient_free_optimizers-1.4.0.dist-info/METADATA,sha256=tJSbT-5LxPI8j6Uci8llN57dMiWliTQREv_VaniyIEQ,27452
+gradient_free_optimizers-1.4.0.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+gradient_free_optimizers-1.4.0.dist-info/top_level.txt,sha256=vvSJQaQ3s7mZx_5ZoYAS3sBswwsRh2xhcx1vmlwhjrQ,31
+gradient_free_optimizers-1.4.0.dist-info/RECORD,,
```

