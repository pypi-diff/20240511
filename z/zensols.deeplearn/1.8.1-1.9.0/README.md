# Comparing `tmp/zensols.deeplearn-1.8.1-py3-none-any.whl.zip` & `tmp/zensols.deeplearn-1.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,75 +1,75 @@
-Zip file size: 162675 bytes, number of entries: 73
--rw-rw-r--  2.0 unx       43 b- defN 23-Aug-17 04:30 zensols/dataframe/__init__.py
--rw-rw-r--  2.0 unx     2290 b- defN 23-Aug-17 04:30 zensols/dataframe/config.py
--rw-rw-r--  2.0 unx     8983 b- defN 23-Aug-17 04:30 zensols/dataframe/stash.py
--rw-rw-r--  2.0 unx      139 b- defN 23-Aug-17 04:30 zensols/dataset/__init__.py
--rw-rw-r--  2.0 unx     6619 b- defN 23-Aug-17 04:30 zensols/dataset/dimreduce.py
--rw-rw-r--  2.0 unx     3266 b- defN 23-Aug-17 04:30 zensols/dataset/interface.py
--rw-rw-r--  2.0 unx     3211 b- defN 23-Aug-17 04:30 zensols/dataset/leaveout.py
--rw-rw-r--  2.0 unx     8343 b- defN 23-Aug-17 04:30 zensols/dataset/outlier.py
--rw-rw-r--  2.0 unx    10965 b- defN 23-Aug-17 04:30 zensols/dataset/split.py
--rw-rw-r--  2.0 unx     9725 b- defN 23-Aug-17 04:30 zensols/dataset/stash.py
--rw-rw-r--  2.0 unx      202 b- defN 23-Aug-17 04:30 zensols/deeplearn/__init__.py
--rw-rw-r--  2.0 unx    16052 b- defN 23-Aug-17 04:30 zensols/deeplearn/domain.py
--rw-rw-r--  2.0 unx     8475 b- defN 23-Aug-17 04:30 zensols/deeplearn/observer.py
--rw-rw-r--  2.0 unx    22127 b- defN 23-Aug-17 04:30 zensols/deeplearn/torchconfig.py
--rw-rw-r--  2.0 unx     5141 b- defN 23-Aug-17 04:30 zensols/deeplearn/torchtype.py
--rw-rw-r--  2.0 unx      208 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/__init__.py
--rw-rw-r--  2.0 unx    21605 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/domain.py
--rw-rw-r--  2.0 unx     1569 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/interface.py
--rw-rw-r--  2.0 unx     7202 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/mapping.py
--rw-rw-r--  2.0 unx     2904 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/meta.py
--rw-rw-r--  2.0 unx     1738 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/multi.py
--rw-rw-r--  2.0 unx    15680 b- defN 23-Aug-17 04:30 zensols/deeplearn/batch/stash.py
--rw-rw-r--  2.0 unx       19 b- defN 23-Aug-17 04:30 zensols/deeplearn/cli/__init__.py
--rw-rw-r--  2.0 unx    31746 b- defN 23-Aug-17 04:30 zensols/deeplearn/cli/app.py
--rw-rw-r--  2.0 unx      212 b- defN 23-Aug-17 04:30 zensols/deeplearn/dataframe/__init__.py
--rw-rw-r--  2.0 unx     3720 b- defN 23-Aug-17 04:30 zensols/deeplearn/dataframe/batch.py
--rw-rw-r--  2.0 unx     1122 b- defN 23-Aug-17 04:30 zensols/deeplearn/dataframe/util.py
--rw-rw-r--  2.0 unx     9426 b- defN 23-Aug-17 04:30 zensols/deeplearn/dataframe/vectorize.py
--rw-rw-r--  2.0 unx      359 b- defN 23-Aug-17 04:30 zensols/deeplearn/layer/__init__.py
--rw-rw-r--  2.0 unx     9186 b- defN 23-Aug-17 04:30 zensols/deeplearn/layer/conv.py
--rw-rw-r--  2.0 unx    17314 b- defN 23-Aug-17 04:30 zensols/deeplearn/layer/crf.py
--rw-rw-r--  2.0 unx     8331 b- defN 23-Aug-17 04:30 zensols/deeplearn/layer/linear.py
--rw-rw-r--  2.0 unx     4297 b- defN 23-Aug-17 04:30 zensols/deeplearn/layer/recur.py
--rw-rw-r--  2.0 unx     7346 b- defN 23-Aug-17 04:30 zensols/deeplearn/layer/recurcrf.py
--rw-rw-r--  2.0 unx      491 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/__init__.py
--rw-rw-r--  2.0 unx     4613 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/analyze.py
--rw-rw-r--  2.0 unx    10586 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/batchiter.py
--rw-rw-r--  2.0 unx    36975 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/executor.py
--rw-rw-r--  2.0 unx    32127 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/facade.py
--rw-rw-r--  2.0 unx    10075 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/format.py
--rw-rw-r--  2.0 unx    11383 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/manager.py
--rw-rw-r--  2.0 unx      958 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/meta.py
--rw-rw-r--  2.0 unx     8506 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/module.py
--rw-rw-r--  2.0 unx      516 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/optimizer.py
--rw-rw-r--  2.0 unx     5010 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/pack.py
--rw-rw-r--  2.0 unx     3816 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/pred.py
--rw-rw-r--  2.0 unx     7204 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/sequence.py
--rw-rw-r--  2.0 unx    10544 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/trainmng.py
--rw-rw-r--  2.0 unx     3710 b- defN 23-Aug-17 04:30 zensols/deeplearn/model/wgtexecutor.py
--rw-rw-r--  2.0 unx     3865 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/batch.conf
--rw-rw-r--  2.0 unx      329 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/cli-pack.conf
--rw-rw-r--  2.0 unx     1004 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/cli.conf
--rw-rw-r--  2.0 unx      297 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/default.conf
--rw-rw-r--  2.0 unx     4391 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/model.conf
--rw-rw-r--  2.0 unx      316 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/obj.conf
--rw-rw-r--  2.0 unx     1015 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/observer.conf
--rw-rw-r--  2.0 unx      399 b- defN 23-Aug-17 04:30 zensols/deeplearn/resources/torch.conf
--rw-rw-r--  2.0 unx      269 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/__init__.py
--rw-rw-r--  2.0 unx     1371 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/compare.py
--rw-rw-r--  2.0 unx    34990 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/domain.py
--rw-rw-r--  2.0 unx     8940 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/manager.py
--rw-rw-r--  2.0 unx     3631 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/plot.py
--rw-rw-r--  2.0 unx    13719 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/pred.py
--rw-rw-r--  2.0 unx     5130 b- defN 23-Aug-17 04:30 zensols/deeplearn/result/report.py
--rw-rw-r--  2.0 unx      198 b- defN 23-Aug-17 04:30 zensols/deeplearn/vectorize/__init__.py
--rw-rw-r--  2.0 unx     6704 b- defN 23-Aug-17 04:30 zensols/deeplearn/vectorize/domain.py
--rw-rw-r--  2.0 unx    13990 b- defN 23-Aug-17 04:30 zensols/deeplearn/vectorize/manager.py
--rw-rw-r--  2.0 unx     2061 b- defN 23-Aug-17 04:30 zensols/deeplearn/vectorize/util.py
--rw-rw-r--  2.0 unx    14594 b- defN 23-Aug-17 04:30 zensols/deeplearn/vectorize/vectorizers.py
--rw-rw-r--  2.0 unx    10565 b- defN 23-Aug-17 04:30 zensols.deeplearn-1.8.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Aug-17 04:30 zensols.deeplearn-1.8.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       52 b- defN 23-Aug-17 04:30 zensols.deeplearn-1.8.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6571 b- defN 23-Aug-17 04:30 zensols.deeplearn-1.8.1.dist-info/RECORD
-73 files, 530572 bytes uncompressed, 152169 bytes compressed:  71.3%
+Zip file size: 163630 bytes, number of entries: 73
+-rw-rw-r--  2.0 unx       43 b- defN 23-Dec-06 00:20 zensols/dataframe/__init__.py
+-rw-rw-r--  2.0 unx     2290 b- defN 23-Dec-06 00:20 zensols/dataframe/config.py
+-rw-rw-r--  2.0 unx     9078 b- defN 23-Dec-06 00:20 zensols/dataframe/stash.py
+-rw-rw-r--  2.0 unx      139 b- defN 23-Dec-06 00:20 zensols/dataset/__init__.py
+-rw-rw-r--  2.0 unx     6618 b- defN 23-Dec-06 00:20 zensols/dataset/dimreduce.py
+-rw-rw-r--  2.0 unx     3266 b- defN 23-Dec-06 00:20 zensols/dataset/interface.py
+-rw-rw-r--  2.0 unx     3211 b- defN 23-Dec-06 00:20 zensols/dataset/leaveout.py
+-rw-rw-r--  2.0 unx     8342 b- defN 23-Dec-06 00:20 zensols/dataset/outlier.py
+-rw-rw-r--  2.0 unx    10967 b- defN 23-Dec-06 00:20 zensols/dataset/split.py
+-rw-rw-r--  2.0 unx     9733 b- defN 23-Dec-06 00:20 zensols/dataset/stash.py
+-rw-rw-r--  2.0 unx      202 b- defN 23-Dec-06 00:20 zensols/deeplearn/__init__.py
+-rw-rw-r--  2.0 unx    16052 b- defN 23-Dec-06 00:20 zensols/deeplearn/domain.py
+-rw-rw-r--  2.0 unx     8475 b- defN 23-Dec-06 00:20 zensols/deeplearn/observer.py
+-rw-rw-r--  2.0 unx    22566 b- defN 23-Dec-06 00:20 zensols/deeplearn/torchconfig.py
+-rw-rw-r--  2.0 unx     5141 b- defN 23-Dec-06 00:20 zensols/deeplearn/torchtype.py
+-rw-rw-r--  2.0 unx      208 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/__init__.py
+-rw-rw-r--  2.0 unx    21604 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/domain.py
+-rw-rw-r--  2.0 unx     1569 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/interface.py
+-rw-rw-r--  2.0 unx     7202 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/mapping.py
+-rw-rw-r--  2.0 unx     2904 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/meta.py
+-rw-rw-r--  2.0 unx     1738 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/multi.py
+-rw-rw-r--  2.0 unx    15695 b- defN 23-Dec-06 00:20 zensols/deeplearn/batch/stash.py
+-rw-rw-r--  2.0 unx       19 b- defN 23-Dec-06 00:20 zensols/deeplearn/cli/__init__.py
+-rw-rw-r--  2.0 unx    31746 b- defN 23-Dec-06 00:20 zensols/deeplearn/cli/app.py
+-rw-rw-r--  2.0 unx      212 b- defN 23-Dec-06 00:20 zensols/deeplearn/dataframe/__init__.py
+-rw-rw-r--  2.0 unx     3721 b- defN 23-Dec-06 00:20 zensols/deeplearn/dataframe/batch.py
+-rw-rw-r--  2.0 unx     1122 b- defN 23-Dec-06 00:20 zensols/deeplearn/dataframe/util.py
+-rw-rw-r--  2.0 unx     9427 b- defN 23-Dec-06 00:20 zensols/deeplearn/dataframe/vectorize.py
+-rw-rw-r--  2.0 unx      359 b- defN 23-Dec-06 00:20 zensols/deeplearn/layer/__init__.py
+-rw-rw-r--  2.0 unx     9186 b- defN 23-Dec-06 00:20 zensols/deeplearn/layer/conv.py
+-rw-rw-r--  2.0 unx    17314 b- defN 23-Dec-06 00:20 zensols/deeplearn/layer/crf.py
+-rw-rw-r--  2.0 unx     8331 b- defN 23-Dec-06 00:20 zensols/deeplearn/layer/linear.py
+-rw-rw-r--  2.0 unx     4297 b- defN 23-Dec-06 00:20 zensols/deeplearn/layer/recur.py
+-rw-rw-r--  2.0 unx     7346 b- defN 23-Dec-06 00:20 zensols/deeplearn/layer/recurcrf.py
+-rw-rw-r--  2.0 unx      491 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/__init__.py
+-rw-rw-r--  2.0 unx     4613 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/analyze.py
+-rw-rw-r--  2.0 unx    10586 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/batchiter.py
+-rw-rw-r--  2.0 unx    36975 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/executor.py
+-rw-rw-r--  2.0 unx    32169 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/facade.py
+-rw-rw-r--  2.0 unx    10075 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/format.py
+-rw-rw-r--  2.0 unx    11383 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/manager.py
+-rw-rw-r--  2.0 unx      958 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/meta.py
+-rw-rw-r--  2.0 unx     8506 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/module.py
+-rw-rw-r--  2.0 unx      516 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/optimizer.py
+-rw-rw-r--  2.0 unx     5010 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/pack.py
+-rw-rw-r--  2.0 unx     3816 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/pred.py
+-rw-rw-r--  2.0 unx     7204 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/sequence.py
+-rw-rw-r--  2.0 unx    10544 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/trainmng.py
+-rw-rw-r--  2.0 unx     3710 b- defN 23-Dec-06 00:20 zensols/deeplearn/model/wgtexecutor.py
+-rw-rw-r--  2.0 unx     3865 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/batch.conf
+-rw-rw-r--  2.0 unx      329 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/cli-pack.conf
+-rw-rw-r--  2.0 unx     1004 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/cli.conf
+-rw-rw-r--  2.0 unx      315 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/default.conf
+-rw-rw-r--  2.0 unx     4391 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/model.conf
+-rw-rw-r--  2.0 unx      316 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/obj.conf
+-rw-rw-r--  2.0 unx     1015 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/observer.conf
+-rw-rw-r--  2.0 unx      461 b- defN 23-Dec-06 00:20 zensols/deeplearn/resources/torch.conf
+-rw-rw-r--  2.0 unx      269 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/__init__.py
+-rw-rw-r--  2.0 unx     1371 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/compare.py
+-rw-rw-r--  2.0 unx    34989 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/domain.py
+-rw-rw-r--  2.0 unx     8939 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/manager.py
+-rw-rw-r--  2.0 unx     3782 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/plot.py
+-rw-rw-r--  2.0 unx    13689 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/pred.py
+-rw-rw-r--  2.0 unx     5188 b- defN 23-Dec-06 00:20 zensols/deeplearn/result/report.py
+-rw-rw-r--  2.0 unx      198 b- defN 23-Dec-06 00:20 zensols/deeplearn/vectorize/__init__.py
+-rw-rw-r--  2.0 unx     8251 b- defN 23-Dec-06 00:20 zensols/deeplearn/vectorize/domain.py
+-rw-rw-r--  2.0 unx    13990 b- defN 23-Dec-06 00:20 zensols/deeplearn/vectorize/manager.py
+-rw-rw-r--  2.0 unx     2061 b- defN 23-Dec-06 00:20 zensols/deeplearn/vectorize/util.py
+-rw-rw-r--  2.0 unx    14956 b- defN 23-Dec-06 00:20 zensols/deeplearn/vectorize/vectorizers.py
+-rw-rw-r--  2.0 unx    11041 b- defN 23-Dec-06 00:20 zensols.deeplearn-1.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Dec-06 00:20 zensols.deeplearn-1.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       52 b- defN 23-Dec-06 00:20 zensols.deeplearn-1.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6571 b- defN 23-Dec-06 00:20 zensols.deeplearn-1.9.0.dist-info/RECORD
+73 files, 533814 bytes uncompressed, 153124 bytes compressed:  71.3%
```

## zipnote {}

```diff
@@ -201,20 +201,20 @@
 
 Filename: zensols/deeplearn/vectorize/util.py
 Comment: 
 
 Filename: zensols/deeplearn/vectorize/vectorizers.py
 Comment: 
 
-Filename: zensols.deeplearn-1.8.1.dist-info/METADATA
+Filename: zensols.deeplearn-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: zensols.deeplearn-1.8.1.dist-info/WHEEL
+Filename: zensols.deeplearn-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: zensols.deeplearn-1.8.1.dist-info/top_level.txt
+Filename: zensols.deeplearn-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: zensols.deeplearn-1.8.1.dist-info/RECORD
+Filename: zensols.deeplearn-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## zensols/dataframe/stash.py

```diff
@@ -32,18 +32,18 @@
 class DataframeError(APIError):
     """Thrown for dataframe stash issues."""
 
 
 @dataclass
 class DataframeStash(ReadOnlyStash, Deallocatable, Writable,
                      PrimeableStash, metaclass=ABCMeta):
-    """A factory stash that uses a Pandas data frame from which to load.  It uses
-    the data frame index as the keys and :class:`pandas.Series` as values.  The
-    dataframe is usually constructed by reading a file (i.e.CSV) and doing some
-    transformation before using it in an implementation of this stash.
+    """A factory stash that uses a Pandas data frame from which to load.  It
+    uses the data frame index as the keys and :class:`pandas.Series` as values.
+    The dataframe is usually constructed by reading a file (i.e.CSV) and doing
+    some transformation before using it in an implementation of this stash.
 
     The dataframe created by :meth:`_get_dataframe` must have a string or
     integer index since keys for all stashes are of type :class:`str`.  The
     index will be mapped to a string if it is an int automatically.
 
     """
     dataframe_path: Path = field()
@@ -129,38 +129,41 @@
 
     def deallocate(self):
         super().deallocate()
         self._keys_by_split.deallocate()
 
     def _create_keys_for_split(self, split_name: str, df: pd.DataFrame) -> \
             Iterable[str]:
-        """Generate an iterable of string keys.  It is expected this method to be
-        potentially very expensive, so the results are cached to disk.  This
+        """Generate an iterable of string keys.  It is expected this method to
+        be potentially very expensive, so the results are cached to disk.  This
         implementation returns the dataframe index.
 
         :param split_name: the name of the split (i.e. ``train`` vs ``test``)
         :param df: the data frame for the grouping of keys from CSV of data
 
         """
         return df.index
 
     def _get_counts_by_key(self) -> Dict[str, int]:
         sc = self.split_col
-        return dict(self.dataframe.groupby([sc])[sc].count().items())
+        return dict(self.dataframe.groupby(sc)[sc].count().items())
 
     @persisted('_split_names')
     def _get_split_names(self) -> Set[str]:
         return set(self.dataframe[self.split_col].unique())
 
     @persisted('_keys_by_split')
     def _get_keys_by_split(self) -> Dict[str, Tuple[str]]:
         keys_by_split = OrderedDict()
         split_col = self.split_col
-        for split, df in self.dataframe.groupby([split_col]):
-            logger.info(f'parsing keys for {split}')
+        split: str
+        df: pd.DataFrame
+        for split, df in self.dataframe.groupby(split_col):
+            if logger.isEnabledFor(logging.INFO):
+                logger.info(f'parsing keys for {split}')
             keys = self._create_keys_for_split(split, df)
             keys_by_split[split] = tuple(keys)
         return keys_by_split
 
     def clear(self):
         super().clear()
         self.clear_keys()
@@ -205,15 +208,15 @@
 
     def _prepare_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
         n_train = self.distribution['train']
         n_test = self.distribution['test']
         n_val = self.distribution['validate']
         n_test_val = n_test + n_val
         n_test = n_test / n_test_val
-        train, test_val = train_test_split(df, test_size=1-n_train)
+        train, test_val = train_test_split(df, test_size=1 - n_train)
         test, val = train_test_split(test_val, test_size=n_test)
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'split dataframe: train: {train.size}, ' +
                          f'test: {test.size}, validation: {val.size}')
         # pandas complains about modifying a slice
         train = train.copy()
         test = test.copy()
@@ -224,16 +227,16 @@
         df = pd.concat([train, test, val], ignore_index=False)
         df = super()._prepare_dataframe(df)
         return df
 
 
 @dataclass
 class DefaultDataframeStash(SplitKeyDataframeStash):
-    """A default implementation of :class:`.DataframeSplitStash` that creates the
-    Pandas dataframe by simply reading it from a specificed CSV file.  The
+    """A default implementation of :class:`.DataframeSplitStash` that creates
+    the Pandas dataframe by simply reading it from a specificed CSV file.  The
     index is a string type appropriate for a stash.
 
     """
     input_csv_path: Path = field()
     """A path to the CSV of the source data."""
 
     def _get_dataframe(self) -> pd.DataFrame:
```

## zensols/dataset/dimreduce.py

```diff
@@ -109,16 +109,16 @@
         else:
             X: np.ndarray = data
         return X
 
 
 @dataclass
 class DecomposeDimensionReducer(DimensionReducer):
-    """A dimensionality reducer that uses eigenvector decomposition such as PCA or
-    SVD.
+    """A dimensionality reducer that uses eigenvector decomposition such as PCA
+    or SVD.
 
     """
     _DICTABLE_ATTRIBUTES = DimensionReducer._DICTABLE_ATTRIBUTES | \
         {'description'}
 
     def __post_init__(self):
         assert self.is_decompose_method(self.reduction_meth)
@@ -130,16 +130,16 @@
         :see: :obj:`reduction_meth`
 
         """
         return reduction_meth == 'pca' or reduction_meth == 'svd'
 
     def get_components(self, data: np.ndarray = None,
                        one_dir: bool = True) -> Tuple[np.ndarray, np.ndarray]:
-        """Create a start and end points that make the PCA component, which is useful
-        for rendering lines for visualization.
+        """Create a start and end points that make the PCA component, which is
+        useful for rendering lines for visualization.
 
         :param: use in place of the :obj:`data` for component calculation using
                 the (already) trained model
 
         :param one_dir: whether or not to create components one way from the
                         mean, or two way (forward and backward) from the mean
 
@@ -179,11 +179,11 @@
         evs = []
         for i, ev in enumerate(model.explained_variance_ratio_):
             evs.append(ev)
             tot_ev += ev
         noise: float = None
         if hasattr(model, 'noise_variance_'):
             noise = model.noise_variance_
-        return {'components':  len(model.components_),
+        return {'components': len(model.components_),
                 'noise': noise,
                 'total_variance': tot_ev,
                 'explained_varainces': evs}
```

## zensols/dataset/leaveout.py

```diff
@@ -12,37 +12,37 @@
 from pathlib import Path
 from zensols.persist import persisted, PersistedWork, Stash
 from . import DatasetError, SplitKeyContainer
 
 
 @dataclass
 class LeaveNOutSplitKeyContainer(SplitKeyContainer):
-    """A split key container that leaves one out of the dataset.  By default, this
-    creates a dataset that has one data point for validation, another for test,
-    and the rest of the data for training.
+    """A split key container that leaves one out of the dataset.  By default,
+    this creates a dataset that has one data point for validation, another for
+    test, and the rest of the data for training.
 
     """
     delegate: Stash = field()
     """The source for keys to generate the splits."""
 
     distribution: Dict[str, int] = field(
         default_factory=lambda: {'train': -1, 'validation': 1, 'test': 1})
-    """The number of data points by each split type.  If the value is an integer,
-    that number of data points are used.  Otherwise, if it is a float, then
-    that percentage of the entire key set is used.
+    """The number of data points by each split type.  If the value is an
+    integer, that number of data points are used.  Otherwise, if it is a float,
+    then that percentage of the entire key set is used.
 
     """
     shuffle: bool = field(default=True)
-    """If ``True``, shuffle the keys obtained from :obj:`delegate` before creating
-    the splits.
+    """If ``True``, shuffle the keys obtained from :obj:`delegate` before
+    creating the splits.
 
     """
     path: Path = field(default=None)
-    """If not ``None``, persist the keys after shuffling (if enabled) to the path
-    specified, for reproducibility of key partitions.
+    """If not ``None``, persist the keys after shuffling (if enabled) to the
+    path specified, for reproducibility of key partitions.
 
     """
     def __post_init__(self):
         path = '_key_queue' if self.path is None else self.path
         self._key_queue = PersistedWork(path, self, mkdir=True)
         self._iter = 0
```

## zensols/dataset/outlier.py

```diff
@@ -166,16 +166,16 @@
         md: np.ndarray = np.sqrt(dist.diagonal())
 
         C = self._set_chi_threshold(significance)
         return self._select_indicies(md, C)
 
     def robust_mahalanobis(self, significance: float = 0.001,
                            random_state: int = 0) -> np.ndarray:
-        """Like :meth:`mahalanobis` but use a robust mean and covarance matrix by
-        sampling the dataset.
+        """Like :meth:`mahalanobis` but use a robust mean and covarance matrix
+        by sampling the dataset.
 
         :param significance: 1 - the Chi^2 percent point function (inverse of
                              cdf / percentiles) outlier threshold; reasonable
                              values include 2.5%, 1%, 0.01%); if `None` use
                              :obj:`threshold` or :obj:`proportion`
 
         :return: indexes in to :obj:`data` rows (indexes of a dataframe) of the
@@ -207,16 +207,16 @@
         # distances: shape: (R,)
         md: np.ndarray = np.sqrt(dist.diagonal())
 
         C = self._set_chi_threshold(significance)
         return self._select_indicies(md, C)
 
     def __call__(self, *args, **kwargs) -> np.ndarray:
-        """Return the output of the method provided by :obj:`default_method`.  All
-        (keyword) arguments are passed on to the respective method.
+        """Return the output of the method provided by :obj:`default_method`.
+        All (keyword) arguments are passed on to the respective method.
 
         :return: indexes in to :obj:`data` rows (indexes of a dataframe) of the
                  outliers
 
         """
         meth = getattr(self, self.default_method)
         return meth(*args, **kwargs)
```

## zensols/dataset/split.py

```diff
@@ -169,15 +169,15 @@
         end = len(dists)
         for name, dist in dists:
             end = start + int((klen * dist))
             by_name[name] = tuple(keys[start:end])
             start = end
         by_name[last[0]] = keys[start:]
         for k, v in by_name.items():
-            print(k, len(v), len(v)/klen)
+            print(k, len(v), len(v) / klen)
         assert sum(map(len, by_name.values())) == klen
         return by_name
 
 
 @dataclass
 class StratifiedStashSplitKeyContainer(StashSplitKeyContainer):
     """Like :class:`.StashSplitKeyContainer` but data is stratified by a label
```

## zensols/dataset/stash.py

```diff
@@ -59,16 +59,16 @@
 
     def _add_keys(self, split_name: str, to_populate: Dict[str, str],
                   keys: List[str]):
         to_populate[split_name] = tuple(keys)
 
     @persisted('_keys_by_split')
     def _get_keys_by_split(self) -> Dict[str, Tuple[str]]:
-        """Return keys by split type (i.e. ``train`` vs ``test``) for only those keys
-        available by the delegate backing stash.
+        """Return keys by split type (i.e. ``train`` vs ``test``) for only those
+        keys available by the delegate backing stash.
 
         """
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug('creating in memory available keys data structure')
         with time('created key data structures', logging.DEBUG):
             delegate_keys = set(self.delegate.keys())
             avail_kbs = OrderedDict()
@@ -83,16 +83,16 @@
             return avail_kbs
 
     def _get_counts_by_key(self) -> Dict[str, int]:
         return dict(map(lambda i: (i[0], len(i[1])),
                         self.keys_by_split.items()))
 
     def check_key_consistent(self) -> bool:
-        """Return if the :obj:`split_container` have the same key count divisiion as
-        this stash's split counts.
+        """Return if the :obj:`split_container` have the same key count
+        divisiion as this stash's split counts.
 
         """
         return self.counts_by_key == self.split_container.counts_by_key
 
     def keys(self) -> Iterable[str]:
         self.prime()
         if logger.isEnabledFor(logging.DEBUG):
@@ -144,16 +144,16 @@
                 logger.debug(f'deallocating: {len(splits)} stash data splits')
             for v in splits:
                 self._try_deallocate(v, recursive=True)
         self._splits.deallocate()
         super().deallocate()
 
     def clear_keys(self):
-        """Clear any cache state for keys, and keys by split.  It does this by clearing
-        the key state for stash, and then the :meth:`clear` of the
+        """Clear any cache state for keys, and keys by split.  It does this by
+        clearing the key state for stash, and then the :meth:`clear` of the
         :obj:`split_container`.
 
         """
         self.split_container.clear()
         self._keys_by_split.clear()
 
     def clear(self):
@@ -173,15 +173,16 @@
         return self.split_container.split_names
 
     def _get_split_name(self) -> str:
         return self._inst_split_name
 
     @persisted('_splits')
     def _get_splits(self) -> Dict[str, Stash]:
-        """Return an instance of ta stash that contains only the data for a split.
+        """Return an instance of ta stash that contains only the data for a
+        split.
 
         :param split: the name of the split of the instance to get
                       (i.e. ``train``, ``test``).
 
         """
         self.prime()
         stashes = OrderedDict()
@@ -212,17 +213,17 @@
         if include_delegate and isinstance(self.delegate, Writable):
             self._write_line('delegate:', depth, writer)
             self.delegate.write(depth + 1, writer)
 
 
 @dataclass
 class SortedDatasetSplitStash(DatasetSplitStash):
-    """A sorted version of a :class:`DatasetSplitStash`, where keys, values, items
-    and iterations are sorted by key.  This is important for reproducibility of
-    results.
+    """A sorted version of a :class:`DatasetSplitStash`, where keys, values,
+    items and iterations are sorted by key.  This is important for
+    reproducibility of results.
 
     An alternative is to use :class:`.DatasetSplitStash` with an instance of
     :class:`.StashSplitKeyContainer` set as the :obj:`delegate` since the key
     container keeps key ordering consistent.
 
     Any shuffling of the dataset, for the sake of training on non-uniform data,
     needs to come *before* using this class.  This class also sorts the keys in
```

## zensols/deeplearn/torchconfig.py

```diff
@@ -1,12 +1,12 @@
 """CUDA access and utility module.
 
 """
+from __future__ import annotations
 __author__ = 'Paul Landes'
-
 from typing import Dict, Iterable, Any, Tuple, Union, Type, List
 import sys
 import logging
 import gc
 from io import TextIOBase
 import random
 import torch
@@ -19,16 +19,16 @@
 from zensols.persist import persisted, PersistableContainer, PersistedWork
 from . import TorchTypes
 
 logger = logging.getLogger(__name__)
 
 
 class CudaInfo(Writable):
-    """A utility class that provides information about the CUDA configuration for
-    the current (hardware) environment.
+    """A utility class that provides information about the CUDA configuration
+    for the current (hardware) environment.
 
     """
     @property
     @persisted('_gpu_available', cache_global=True)
     def gpu_available(self) -> bool:
         return cuda.is_available()
 
@@ -67,18 +67,18 @@
             self._write_object(self.get_devices(True), depth + 1, writer)
 
     def __str__(self):
         return f'CUDA devices: {self.num_devices}'
 
 
 class TorchConfig(PersistableContainer, Writable):
-    """A utility class that provides access to CUDA APIs.  It provides information
-    on the current CUDA configuration and convenience methods to create, copy
-    and modify tensors.  These are handy for any given CUDA configuration and
-    can back off to the CPU when CUDA isn't available.
+    """A utility class that provides access to CUDA APIs.  It provides
+    information on the current CUDA configuration and convenience methods to
+    create, copy and modify tensors.  These are handy for any given CUDA
+    configuration and can back off to the CPU when CUDA isn't available.
 
     """
     _CPU_DEVICE: str = 'cpu'
     _RANDOM_SEED: dict = None
     _CPU_WARN: bool = False
 
     def __init__(self, use_gpu: bool = True, data_type: type = torch.float32,
@@ -110,15 +110,16 @@
         self._cpu_device_pw = PersistedWork(
             '_cpu_device_pw', self, cache_global=True)
         self._cpu_device_pw._mark_deallocated()
         self._cuda_device_index = cuda_device_index
 
     @persisted('_init_device_pw')
     def _init_device(self) -> torch.device:
-        """Attempt to initialize CUDA, and if successful, return the CUDA device.
+        """Attempt to initialize CUDA, and if successful, return the CUDA
+        device.
 
         """
         is_avail = torch.cuda.is_available()
         use_gpu = self.use_gpu and is_avail
         logger.debug(f'use cuda: {self.use_gpu}, is avail: {is_avail}')
         if use_gpu:
             if logger.isEnabledFor(logging.DEBUG):
@@ -135,16 +136,16 @@
             self.use_gpu = False
             self._cuda_device_index = None
         return device
 
     @property
     @persisted('_cpu_device_pw')
     def cpu_device(self) -> torch.device:
-        """Return the CPU CUDA device, which is the device type configured to utilize
-        the CPU (rather than the GPU).
+        """Return the CPU CUDA device, which is the device type configured to
+        utilize the CPU (rather than the GPU).
 
         """
         return torch.device(self._CPU_DEVICE)
 
     @classmethod
     def cpu_device_name(cls) -> str:
         """The string name of the torch CPU device."""
@@ -192,31 +193,42 @@
         """Return all cuda devices.
 
         """
         return tuple(map(lambda n: torch.device('cuda', n),
                          range(torch.cuda.device_count())))
 
     @property
+    def cuda_configs(self) -> Tuple[TorchConfig]:
+        """Return a new set of configurations, one for each CUDA device."""
+        def map_dev(device: int) -> TorchConfig:
+            return TorchConfig(
+                use_gpu=self.use_gpu,
+                data_type=self.data_type,
+                cuda_device_index=device)
+
+        return tuple(map(map_dev, range(torch.cuda.device_count())))
+
+    @property
     def cuda_device_index(self) -> Union[int, None]:
-        """Return the CUDA device index if CUDA is being used for this configuration.
-        Otherwise return ``None``.
+        """Return the CUDA device index if CUDA is being used for this
+        configuration.  Otherwise return ``None``.
 
         """
         device = self.device
         if device.type == 'cuda':
             return device.index
 
     @cuda_device_index.setter
     def cuda_device_index(self, device: int):
         """Set the CUDA device index for this configuration."""
         self.device = torch.device('cuda', device)
 
     def same_device(self, tensor_or_model) -> bool:
-        """Return whether or not a tensor or model is in the same memory space as this
-        configuration instance.
+        """Return whether or not a tensor or model is in the same memory space
+        as this configuration instance.
 
         """
         device = self.device
         return hasattr(tensor_or_model, 'device') and \
             tensor_or_model.device == device
 
     @staticmethod
@@ -249,43 +261,44 @@
         objs: List[Tensor] = cls.in_memory_tensors()
         for obj in objs:
             if filter_device is None or filter_device == obj.device:
                 writer.write(
                     f'{type(obj)}: {tuple(obj.shape)} on {obj.device}\n')
 
     def write_device_tensors(self, writer: TextIOBase = sys.stdout):
-        """Like :meth:`write_in_memory_tensors`, but filter on this instance's device.
+        """Like :meth:`write_in_memory_tensors`, but filter on this instance's
+        device.
 
         :param filter_device: if given, write only tensors matching this device
 
         :see: :class:`~zensols.deeplearn.torchconfig.TorchConfig`
 
         """
         self.write_in_memory_tensors(writer=writer, filter_device=self.device)
 
     @staticmethod
     def empty_cache():
-        """Empty the CUDA torch cache.  This releases memory in the GPU and should not
-        be necessary to call for normal use cases.
+        """Empty the CUDA torch cache.  This releases memory in the GPU and
+        should not be necessary to call for normal use cases.
 
         """
         torch.cuda.empty_cache()
 
     @property
     def info(self) -> CudaInfo:
         """Return the CUDA information, which include specs of the device.
 
         """
         self._init_device()
         return CudaInfo()
 
     @property
     def tensor_class(self) -> Type[torch.dtype]:
-        """Return the class type based on the current configuration of this instance.
-        For example, if using ``torch.float32`` on the GPU,
+        """Return the class type based on the current configuration of this
+        instance.  For example, if using ``torch.float32`` on the GPU,
         ``torch.cuda.FloatTensor`` is returned.
 
         """
         return TorchTypes.get_tensor_class(self.data_type, self.using_cpu)
 
     @property
     def numpy_data_type(self) -> Type[torch.dtype]:
@@ -293,31 +306,32 @@
         ``data_type``.
 
         """
         return TorchTypes.get_numpy_type(self.data_type)
 
     def to(self, tensor_or_model: Union[nn.Module, Tensor]) -> \
             Union[nn.Module, Tensor]:
-        """Copy the tensor or model to the device this to that of this configuration.
+        """Copy the tensor or model to the device this to that of this
+        configuration.
 
         """
         if not self.same_device(tensor_or_model):
             tensor_or_model = tensor_or_model.to(self.device)
         if isinstance(tensor_or_model, nn.Module) and \
            hasattr(tensor_or_model, 'dtype') and \
            tensor_or_model.dtype != self.data_type:
             tensor_or_model.type(self.data_type)
         return tensor_or_model
 
     @classmethod
     def to_cpu_deallocate(cls, *arrs: Tuple[Tensor]) -> \
             Union[Tuple[Tensor], Tensor]:
-        """Safely copy detached memory to the CPU and delete local instance (possibly
-        GPU) memory to speed up resource deallocation.  If the tensor is
-        already on the CPU, it's simply passed back.  Otherwise the tensor is
+        """Safely copy detached memory to the CPU and delete local instance
+        (possibly GPU) memory to speed up resource deallocation.  If the tensor
+        is already on the CPU, it's simply passed back.  Otherwise the tensor is
         deleted.
 
         This method is robust with ``None``, which are skipped and substituted
         as ``None`` in the output.
 
         :param arrs: the tensors the copy to the CPU (if not already)
 
@@ -348,16 +362,16 @@
 
         """
         if 'dtype' not in kwargs:
             kwargs['dtype'] = self.data_type
         kwargs['device'] = self.device
 
     def from_iterable(self, array: Iterable[Any]) -> Tensor:
-        """Return a one dimenstional tensor created from ``array`` using the type and
-        device in the current instance configuration.
+        """Return a one dimenstional tensor created from ``array`` using the
+        type and device in the current instance configuration.
 
         """
         cls = self.tensor_class
         if not isinstance(array, tuple) and not isinstance(array, list):
             array = tuple(array)
         return cls(array)
 
@@ -387,16 +401,15 @@
     def sparse(self, indicies: Tuple[int], values: Tuple[float],
                shape: Tuple[int, int]):
         """Create a sparce tensor from indexes and values.
 
         """
         i = torch.LongTensor(indicies)
         v = torch.FloatTensor(values)
-        cls = TorchTypes.get_sparse_class(self.data_type)
-        return cls(i, v, shape, device=self.device)
+        return torch.sparse_coo_tensor(i, v, shape, dtype=self.data_type)
 
     def is_sparse(self, arr: Tensor) -> bool:
         """Return whether or not a tensor a sparse.
         """
         return arr.layout == torch.sparse_coo
 
     def empty(self, *args, **kwargs) -> Tensor:
@@ -417,16 +430,16 @@
         """Return a new tensor of zeros using ``torch.ones``.
 
         """
         self._populate_defaults(kwargs)
         return torch.ones(*args, **kwargs)
 
     def from_numpy(self, arr: np.ndarray) -> Tensor:
-        """Return a new tensor generated from a numpy aray using ``torch.from_numpy``.
-        The array type is converted if necessary.
+        """Return a new tensor generated from a numpy aray using
+        ``torch.from_numpy``.  The array type is converted if necessary.
 
         """
         tarr = torch.from_numpy(arr)
         if arr.dtype != self.numpy_data_type:
             tarr = tarr.type(self.data_type)
         return self.to(tarr)
 
@@ -442,31 +455,31 @@
         """
         if self.data_type != arr.dtype:
             arr = arr.type(self.data_type)
         return arr
 
     @property
     def float_type(self) -> Type:
-        """Return the float type that represents this configuration, converting to the
-        corresponding precision from integer if necessary.
+        """Return the float type that represents this configuration, converting
+        to the corresponding precision from integer if necessary.
 
         :return: the float that represents this data, or ``None`` if neither
                  float nor int
 
         """
         dtype = self.data_type
         if TorchTypes.is_int(dtype):
             return TorchTypes.int_to_float(dtype)
         elif TorchTypes.is_float(dtype):
             return dtype
 
     @property
     def int_type(self) -> Type:
-        """Return the int type that represents this configuration, converting to the
-        corresponding precision from integer if necessary.
+        """Return the int type that represents this configuration, converting to
+        the corresponding precision from integer if necessary.
 
         :return: the int that represents this data, or ``None`` if neither
                  int nor float
 
         """
         dtype = self.data_type
         if TorchTypes.is_float(dtype):
@@ -501,16 +514,16 @@
 
         """
         pad = self._cross_entropy_pad()
         return pad.repeat(size)
 
     @classmethod
     def get_random_seed(cls: Type) -> int:
-        """Get the cross system random seed, meaning the seed applied to CUDA and the
-        Python *random* library.
+        """Get the cross system random seed, meaning the seed applied to CUDA
+        and the Python *random* library.
 
         """
         if cls._RANDOM_SEED is not None:
             return cls._RANDOM_SEED['seed']
 
     @classmethod
     def get_random_seed_context(cls: Type) -> Dict[str, Any]:
```

## zensols/deeplearn/torchtype.py

```diff
@@ -5,17 +5,17 @@
 
 from typing import List, Dict, Type
 import torch
 import numpy as np
 
 
 class TorchTypes(object):
-    """A utility class to convert betwen numpy and torch classes.  It also provides
-    metadata for types that make other conversions, such as same precision
-    cross types (i.e. int64 -> float64).
+    """A utility class to convert betwen numpy and torch classes.  It also
+    provides metadata for types that make other conversions, such as same
+    precision cross types (i.e. int64 -> float64).
 
     """
     TYPES = [{'desc': '32-bit floating point',
               'name': 'float32',
               'types': set([torch.float32, torch.float]),
               'numpy': np.float32,
               'sparse': torch.sparse.FloatTensor,
```

## zensols/deeplearn/batch/domain.py

```diff
@@ -1,15 +1,14 @@
-from __future__ import annotations
 """This file contains a stash used to load an embedding layer.  It creates
 features in batches of matrices and persists matrix only (sans features) for
 efficient retrival.
 
 """
+from __future__ import annotations
 __author__ = 'Paul Landes'
-
 from typing import Tuple, List, Any, Dict, Union, Set
 from dataclasses import dataclass, field
 from abc import ABCMeta, abstractmethod
 import sys
 import logging
 from io import TextIOBase
 import torch
```

## zensols/deeplearn/batch/stash.py

```diff
@@ -1,13 +1,12 @@
-from __future__ import annotations
 """This file contains a stash used to load an embedding layer.
 
 """
+from __future__ import annotations
 __author__ = 'Paul Landes'
-
 from typing import Tuple, List, Any, Dict, Set, Iterable, Type
 from dataclasses import dataclass, InitVar, field
 from abc import ABCMeta
 import sys
 import logging
 import collections
 from functools import reduce
@@ -147,24 +146,26 @@
             self.data_point_id_sets_path, self, mkdir=True)
         self.priming = False
         self.decoded_attributes = decoded_attributes
         self._update_comp_stash_attribs()
 
     @property
     def decoded_attributes(self) -> Set[str]:
-        """The attributes to decode.  Only these are avilable to the model regardless
-        of what was created during encoding time; if None, all are available
+        """The attributes to decode.  Only these are avilable to the model
+        regardless of what was created during encoding time; if None, all are
+        available
 
         """
         return self._decoded_attributes
 
     @decoded_attributes.setter
     def decoded_attributes(self, attribs: Set[str]):
-        """The attributes to decode.  Only these are avilable to the model regardless
-        of what was created during encoding time; if None, all are available
+        """The attributes to decode.  Only these are avilable to the model
+        regardless of what was created during encoding time; if None, all are
+        available
 
         """
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'setting decoded attributes: {attribs}')
         self._decoded_attributes = attribs
         if isinstance(self.delegate, BatchDirectoryCompositeStash):
             self.delegate.load_keys = attribs
@@ -194,16 +195,16 @@
                     if field.attr in attrib_keeps:
                         vec = vec_mng[field.feature_id]
                         by_attrib[field.attr] = BatchFieldMetadata(field, vec)
         return BatchMetadata(self.data_point_type, self.batch_type,
                              mapping, by_attrib)
 
     def _update_comp_stash_attribs(self):
-        """Update the composite stash grouping if we're using one and if this class is
-        already configured.
+        """Update the composite stash grouping if we're using one and if this
+        class is already configured.
 
         """
         if isinstance(self.delegate, BatchDirectoryCompositeStash):
             meta: BatchMetadata = self.batch_metadata
             meta_attribs: Set[str] = set(
                 map(lambda f: f.attr, meta.mapping.get_attributes()))
             groups: Tuple[Set[str]] = self.delegate.groups
```

## zensols/deeplearn/dataframe/batch.py

```diff
@@ -18,17 +18,18 @@
 from zensols.deeplearn.dataframe import DataframeFeatureVectorizerManager
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class DataframeBatchStash(BatchStash):
-    """A stash used for batches of data using :class:`.DataframeBatch` instances.
-    This stash uses an instance of :class:`.DataframeFeatureVectorizerManager`
-    to vectorize the data in the batches.
+    """A stash used for batches of data using :class:`.DataframeBatch`
+    instances.  This stash uses an instance of
+    :class:`.DataframeFeatureVectorizerManager` to vectorize the data in the
+    batches.
 
     """
     @property
     def feature_vectorizer_manager(self) -> DataframeFeatureVectorizerManager:
         managers = tuple(self.vectorizer_manager_set.values())
         if len(managers) != 1:
             raise BatchError('Exected only one vector manager but got: ' +
@@ -56,28 +57,28 @@
     Pandas dataframe.  When created, column is saved as an attribute in the
     instance.
 
     """
     row: InitVar[pd.Series]
 
     def __post_init__(self, row: pd.Series):
-        for name, val in row.iteritems():
+        for name, val in row.items():
             if logger.isEnabledFor(logging.DEBUG):
                 logger.debug(f'setting attrib: {name}={val}')
             setattr(self, name, val)
 
 
 @dataclass
 class DataframeBatch(Batch):
     """A batch of data that contains instances of :class:`.DataframeDataPoint`,
     each of which has the row data from the dataframe.
 
     """
     def _get_batch_feature_mappings(self) -> BatchFeatureMapping:
-        """Use the dataframe based vectorizer manager 
+        """Use the dataframe based vectorizer manager.
 
         """
         df_vec_mng: DataframeFeatureVectorizerManager = \
             self.batch_stash.feature_vectorizer_manager
         return df_vec_mng.batch_feature_mapping
 
     def get_features(self) -> torch.Tensor:
@@ -88,16 +89,16 @@
                  *feaure size* is the number of all features vectorized; that
                  is, a data instance for each row in the batch, is a flattened
                  set of features that represent the respective row from the
                  dataframe
 
         """
         def magic_shape(name: str) -> torch.Tensor:
-            """Return a tensor that has two dimenions of the data (the first always with
-            size 1 since it is a row of data).
+            """Return a tensor that has two dimenions of the data (the first
+            always with size 1 since it is a row of data).
 
             """
             arr = attrs[name]
             if len(arr.shape) == 1:
                 arr = arr.unsqueeze(dim=1)
             return arr
```

## zensols/deeplearn/dataframe/vectorize.py

```diff
@@ -116,34 +116,34 @@
         """
         logger.debug('constructing metadata')
         df = self.stash.dataframe
         skip = set([self.stash.split_col, self.label_col])
         labels = tuple(df[self.label_col].unique())
         cont = set()
         desc = {}
-        for name, dtype in df.dtypes.iteritems():
+        for name, dtype in df.dtypes.items():
             if name in skip:
                 continue
-            if dtype == np.object:
+            if dtype == object:
                 desc[name] = tuple(df[name].unique())
             else:
                 cont.add(name)
         return DataframeMetadata(
             self.prefix, self.label_col, labels, cont, desc)
 
     @property
     def label_attribute_name(self) -> str:
         """Return the label attribute.
 
         """
         return f'{self.prefix}label'
 
     def column_to_feature_id(self, col: str) -> str:
-        """Generate a feature id from the column name.  This just attaches the prefix
-        to the column name.
+        """Generate a feature id from the column name.  This just attaches the
+        prefix to the column name.
 
         """
         return f'{self.prefix}{col}'
 
     def _filter_columns(self, cols: Tuple[str]) -> Iterable[str]:
         """Return an interable of the columns to use as features based on
         ``include_columns`` and ``exclude_columns``.
@@ -170,16 +170,16 @@
             config_factory=self.config_factory,
             manager=self,
             feature_id=label_col,
             categories=label_values,
             optimize_bools=False)
 
     def _create_feature_vectorizers(self) -> List[FeatureVectorizer]:
-        """Create a vectorizer, one for each column/feature, included as a feature
-        type based on :meth:`_filter_columns`.
+        """Create a vectorizer, one for each column/feature, included as a
+        feature type based on :meth:`_filter_columns`.
 
         """
         vecs = []
         meta = self.dataset_metadata
         for col in meta.continuous:
             vec = AttributeEncodableFeatureVectorizer(
                 manager=self,
@@ -211,15 +211,16 @@
             logger.debug(f'adding vectorizer: {vec.feature_id}')
             vectorizers[vec.feature_id] = vec
         return vectorizers
 
     @property
     @persisted('_batch_feature_mapping')
     def batch_feature_mapping(self) -> BatchFeatureMapping:
-        """Return the mapping for :class:`zensols.deeplearn.batch.Batch` instances.
+        """Return the mapping for :class:`zensols.deeplearn.batch.Batch`
+        instances.
 
         """
         def create_fileld_mapping(col: str) -> FieldFeatureMapping:
             feature_id = self.column_to_feature_id(col)
             return FieldFeatureMapping(col, feature_id, True)
 
         meta = self.dataset_metadata
```

## zensols/deeplearn/model/facade.py

```diff
@@ -655,15 +655,16 @@
         :see: :meth:`get_predictions_factory`
 
         :param args: arguments passed to :meth:`get_predictions_factory`
 
         :param kwargs: arguments passed to :meth:`get_predictions_factory`
 
         """
-        df_fac = self.get_predictions_factory(*args, **kwargs)
+        df_fac: PredictionsDataFrameFactory = self.get_predictions_factory(
+            *args, **kwargs)
         return df_fac.dataframe
 
     def write_predictions(self, lines: int = 10):
         """Print the predictions made during the test phase of the model
         execution.
 
         :param lines: the number of lines of the predictions data frame to be
```

## zensols/deeplearn/resources/default.conf

```diff
@@ -1,10 +1,11 @@
 [deeplearn_default]
 batch_dir = ${default:data_dir}/batch
 temporary_dir = ${default:data_dir}/model
 results_dir = ${default:root_dir}/results
 model_name = Unnamed
+fp_precision = 32
 
 [batch_stash]
 # the number of data instances per batch, and the first dimension of each
 # tensor given to the model
 batch_size = 200
```

## zensols/deeplearn/resources/torch.conf

```diff
@@ -1,14 +1,14 @@
 ## PyTorch configuration indicates where to use the GPU vs CPU and default
 ## types
 
 # CPU based configuration
 [torch_config]
 class_name = zensols.deeplearn.TorchConfig
 use_gpu = False
-data_type = eval({'import': ['torch']}): torch.float32
+data_type = eval({'import': ['torch']}): torch.float${deeplearn_default:fp_precision}
 
 # GPU based confgiuration
 [gpu_torch_config]
 class_name = zensols.deeplearn.TorchConfig
 use_gpu = True
-data_type = eval({'import': ['torch']}): torch.float32
+data_type = eval({'import': ['torch']}): torch.float${deeplearn_default:fp_precision}
```

## zensols/deeplearn/result/domain.py

```diff
@@ -1,12 +1,11 @@
 """Contains contain classes for results generated from training and testing a
 model.
 
 """
-
 from __future__ import annotations
 __author__ = 'Paul Landes'
 from typing import (
     List, Dict, Set, Iterable, Any, Type, Tuple, Callable, ClassVar
 )
 from dataclasses import dataclass, field, InitVar
 from enum import Enum
@@ -232,15 +231,15 @@
 
     @property
     def n_correct(self) -> int:
         """The number or correct predictions for the classification.
 
         """
         is_eq = np.equal(self.labels, self.predictions)
-        return self._protect(lambda: np.count_nonzero(is_eq == True))
+        return self._protect(lambda: np.count_nonzero(is_eq is True))
 
     def create_metrics(self, average: str) -> ScoreMetrics:
         """Create a score metrics with the given average.
 
         """
         return ScoreMetrics(self.labels, self.predictions, average)
```

## zensols/deeplearn/result/manager.py

```diff
@@ -1,13 +1,12 @@
-from __future__ import annotations
 """A class that persists results in various formats.
 
 """
+from __future__ import annotations
 __author__ = 'Paul Landes'
-
 from typing import Tuple, Iterable, Set
 from dataclasses import dataclass, field
 import logging
 import re
 import pickle
 import shutil
 from pathlib import Path
@@ -144,16 +143,16 @@
     def __post_init__(self):
         self.prefix = self.to_file_name(self.name)
         super().__post_init__(self.prefix)
 
     @property
     @persisted('_read_stash')
     def results_stash(self) -> Stash:
-        """Return a stash that provides access to previous results (not just the last
-        results).  The stash iterates over the model results directory with
+        """Return a stash that provides access to previous results (not just the
+        last results).  The stash iterates over the model results directory with
         :class:`.ArchivedResult` values.
 
         """
         return _ArchivedResultStash(self, DirectoryStash(path=self.path))
 
     @staticmethod
     def to_file_name(name: str) -> str:
@@ -209,16 +208,16 @@
         if self.save_json:
             self.save_json_result(result)
         if self.save_plot:
             self.save_plot_result(result)
 
     def get_grapher(self, figsize: Tuple[int, int] = (15, 5),
                     title: str = None) -> ModelResultGrapher:
-        """Return an instance of a model grapher.  This class can plot results of
-        ``res`` using ``matplotlib``.
+        """Return an instance of a model grapher.  This class can plot results
+        of ``res`` using ``matplotlib``.
 
         :see: :class:`.ModelResultGrapher`
 
         """
         title = self.name if title is None else title
         path = self.get_next_graph_path()
         return ModelResultGrapher(title, figsize, save_path=path)
```

## zensols/deeplearn/result/plot.py

```diff
@@ -1,13 +1,12 @@
 """Provides a class to graph the results.
 
 """
 __author__ = 'Paul Landes'
 
-
 from typing import List, Tuple
 from dataclasses import dataclass, field
 import logging
 from pathlib import Path
 import math
 import matplotlib.pyplot as plt
 import numpy as np
@@ -18,28 +17,33 @@
 
 
 @dataclass
 class ModelResultGrapher(object):
     """Graphs the an instance of ``ModelResult``.  This creates subfigures,
     one for each of the results given as input to ``plot``.
 
-    :param name: the name that goes in the title of the graph
-    :param figsize: the size of the top level figure (not the panes)
-    :param split_types: the splits to graph (list of size 2); defaults to
-                        ``[DatasetSplitType.train, DatasetSplitType.validation]``
-    :param title: the title format used to create each sub pane graph.
-
     :see: plot
 
     """
     name: str = field(default=None)
-    figsize: Tuple[int, int] = (15, 5)
-    split_types: List[DatasetSplitType] = None
+    """The name that goes in the title of the graph."""
+
+    figsize: Tuple[int, int] = field(default=(15, 5))
+    """the size of the top level figure (not the panes)"""
+
+    split_types: List[DatasetSplitType] = field(default=None)
+    """The splits to graph (list of size 2); defaults to
+    ``[DatasetSplitType.train, DatasetSplitType.validation]``.
+
+    """
     title: str = None
+    """The title format used to create each sub pane graph."""
+
     save_path: Path = field(default=None)
+    """Where the plot is saved."""
 
     def __post_init__(self):
         if self.split_types is None:
             self.split_types = [DatasetSplitType.train,
                                 DatasetSplitType.validation]
         if self.title is None:
             self.title = ('Figure {r.name} ' +
@@ -47,14 +51,15 @@
                           '{r.last_test.converged_epoch.metrics})')
 
     def _render_title(self, cont: ModelResult) -> str:
         lr = cont.model_settings['learning_rate']
         return self.title.format(**{'r': cont, 'learning_rate': lr})
 
     def plot(self, containers: List[ModelResult]):
+        """Create a plot for results ``containers``."""
         name = containers[0].name if self.name is None else self.name
         ncols = min(2, len(containers))
         nrows = math.ceil(len(containers) / ncols)
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'plot grid: {nrows} X {ncols}')
         fig, axs = plt.subplots(
             ncols=ncols, nrows=nrows, sharex=True, figsize=self.figsize)
@@ -87,14 +92,16 @@
             col += 1
             if col == ncols:
                 col = 0
                 row += 1
         plt.legend(tuple(map(lambda e: e[0], es)))
 
     def show(self):
+        """Render and display the plot."""
         plt.show()
 
     def save(self):
+        """Save the plot to disk."""
         if logger.isEnabledFor(logging.INFO):
             logger.info(f'saving results graph to {self.save_path}')
         plt.savefig(self.save_path)
         plt.close()
```

## zensols/deeplearn/result/pred.py

```diff
@@ -25,16 +25,16 @@
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class PredictionsDataFrameFactory(object):
-    """Create a Pandas data frame containing results from a result as output from a
-    ``ModelExecutor``.  The data frame contains the feature IDs, labels,
+    """Create a Pandas data frame containing results from a result as output
+    from a ``ModelExecutor``.  The data frame contains the feature IDs, labels,
     predictions mapped back to their original value from the feature data item.
 
     Currently only classification models are supported.
 
     """
     METRIC_DESCRIPTIONS: ClassVar[Dict[str, str]] = frozendict({
         'wF1': 'weighted F1',
@@ -68,17 +68,17 @@
         'mPv': 'micro precision on the validation set',
         'mRv': 'micro recall on the validation set',
         'MF1v': 'macro F1 on the validation set',
         'MPv': 'macro precision on the validation set',
         'MRv': 'macro recall on the validation set',
         'accv': 'accuracy on the validation set',
 
-        'train_occurs': 'the number of data points used to train the model',
-        'test_occurs': 'the number of data points used to test the model',
-        'validation_occurs': 'the number of data points used to validate the model',
+        'train_occurs': 'the number of data points used to train',
+        'test_occurs': 'the number of data points used to test',
+        'validation_occurs': 'the number of data points used to validate',
 
         'label': 'the model class',
         'name': 'the model or result set name',
         'file': 'the directory name of the results',
         'start': 'when the test started',
         'train_duration': 'the time it took to train the model in HH:MM:SS',
         'converged': 'the last epoch with the lowest loss',
@@ -155,16 +155,16 @@
 
     epoch_result: EpochResult = field(default=None)
     """The epoch containing the results.  If none given, take it from the test
     results..
 
     """
     label_vectorizer_name: str = field(default=None)
-    """The name of the vectorizer that encodes the labels, which is used to reverse
-    map from integers to their original string nominal values.
+    """The name of the vectorizer that encodes the labels, which is used to
+    reverse map from integers to their original string nominal values.
 
     """
     def __post_init__(self):
         if self.column_names is None:
             self.column_names = ('data',)
         if self.data_point_transform is None:
             self.data_point_transform = lambda dp: (str(dp),)
@@ -237,16 +237,16 @@
 
     def _create_dataframe(self, inv_trans: bool) -> pd.DataFrame:
         return pd.concat(self._batch_dataframe(inv_trans), ignore_index=True)
 
     @property
     @persisted('_dataframe')
     def dataframe(self) -> pd.DataFrame:
-        """The predictions and labels as a dataframe.  The first columns are generated
-        from ``data_point_tranform``, and the remaining columns are:
+        """The predictions and labels as a dataframe.  The first columns are
+        generated from ``data_point_tranform``, and the remaining columns are:
 
         - id: the ID of the feature (not batch) data item
         - label: the label given by the feature data item
         - pred: the prediction
         - correct: whether or not the prediction was correct
 
         """
```

## zensols/deeplearn/result/report.py

```diff
@@ -55,16 +55,18 @@
         if self.include_validation:
             cols.extend('wF1v wPv wRv mF1v mPv mRv MF1v MPv MRv accv'.split())
         cols.extend('train_occurs validation_occurs test_occurs'.split())
         dpt_key = 'n_total_data_points'
         arch_res: ArchivedResult
         for fname, arch_res in self.result_manager.results_stash.items():
             res: ModelResult = arch_res.model_result
-            train: DatasetResult = res.dataset_result.get(DatasetSplitType.train)
-            validate: DatasetResult = res.dataset_result.get(DatasetSplitType.validation)
+            train: DatasetResult = res.dataset_result.get(
+                DatasetSplitType.train)
+            validate: DatasetResult = res.dataset_result.get(
+                DatasetSplitType.validation)
             test: DatasetResult = res.dataset_result.get(DatasetSplitType.test)
             if train is not None:
                 dur = train.end_time - train.start_time
                 hours, remainder = divmod(dur.seconds, 3600)
                 minutes, seconds = divmod(remainder, 60)
                 dur = f'{hours:02}:{minutes:02}:{seconds:02}'
             if validate is not None:
@@ -82,15 +84,16 @@
                 row.extend([
                     tm.weighted.f1, tm.weighted.precision, tm.weighted.recall,
                     tm.micro.f1, tm.micro.precision, tm.micro.recall,
                     tm.macro.f1, tm.macro.precision, tm.macro.recall,
                     tm.accuracy])
                 if self.include_validation:
                     row.extend([
-                        vm.weighted.f1, vm.weighted.precision, vm.weighted.recall,
+                        vm.weighted.f1, vm.weighted.precision,
+                        vm.weighted.recall,
                         vm.micro.f1, vm.micro.precision, vm.micro.recall,
                         vm.macro.f1, vm.macro.precision, vm.macro.recall,
                         vm.accuracy])
                 row.extend([
                     train.statistics[dpt_key], validate.statistics[dpt_key],
                     test.statistics[dpt_key]])
                 rows.append(row)
```

## zensols/deeplearn/vectorize/domain.py

```diff
@@ -1,17 +1,18 @@
 """Vectorization base classes and basic functionality.
 
 """
 __author__ = 'Paul Landes'
 
-from typing import Tuple, Any, Union
+from typing import Tuple, Any, Union, ClassVar
 from dataclasses import dataclass, field
 from abc import abstractmethod, ABCMeta
 import logging
 import sys
+import warnings
 from io import TextIOBase
 from scipy import sparse
 from scipy.sparse import csr_matrix
 import torch
 from torch import Tensor
 from zensols.persist import PersistableContainer
 from zensols.config import ConfigFactory, Writable
@@ -35,18 +36,20 @@
 
     """
     config_factory: ConfigFactory = field(repr=False)
     """The configuration factory that created this instance and used for
     serialization functions.
 
     """
-
     def __post_init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
+    def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
+        self._write_line(f'name: {self.name}', depth, writer)
+
 
 @dataclass
 class FeatureVectorizer(ConfigurableVectorization, metaclass=ABCMeta):
     """An asbstrct base class that transforms a Python object in to a PyTorch
     tensor.
 
     """
@@ -79,45 +82,48 @@
         """A short human readable name.
 
         :see: obj:`feature_id`
 
         """
         return self.DESCRIPTION
 
+    def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
+        super().write(depth, writer)
+        self._write_line(f'description: {self.description}', depth, writer)
+        self._write_line(f'feature_id: {self.feature_id}', depth, writer)
+        self._write_line(f'shape: {self.shape}', depth, writer)
+
     def __str__(self):
         return (f'{self.feature_id} ({type(self)}), ' +
                 f'desc={self.description}, shape: {self.shape}')
 
     def __repr__(self):
         return f'{self.__class__}: {self.__str__()}'
 
-    def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
-        self._write_line(str(self), depth, writer)
-
 
 @dataclass
 class FeatureContext(PersistableContainer):
     """Data created by coding and meant to be pickled on the file system.
 
     :see EncodableFeatureVectorizer.encode:
 
     """
     feature_id: str = field()
-    """The feature id of the :class:`.FeatureVectorizer` that created this context.
+    """The feature id of the :class:`.FeatureVectorizer` that created this
+    context.
 
     """
-
     def __str__(self):
         return f'{self.__class__.__name__} ({self.feature_id})'
 
 
 @dataclass
 class NullFeatureContext(FeatureContext):
-    """A no-op feature context used for cases such as prediction batches with data
-    points that have no labels.
+    """A no-op feature context used for cases such as prediction batches with
+    data points that have no labels.
 
     :see: :meth:`~zensols.deeplearn.batch.BatchStash.create_prediction`
 
     :see: :class:`~zensols.deeplearn.batch.Batch`
 
     """
     pass
@@ -133,14 +139,18 @@
     """The output tensor of the encoding phase."""
 
     def deallocate(self):
         super().deallocate()
         if hasattr(self, 'tensor'):
             del self.tensor
 
+    def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
+        super().write(depth, writer)
+        self._write_line(f'tensor shape: {self.tensor.shape}', depth, writer)
+
     def __str__(self):
         tstr = f'{self.tensor.shape}' if self.tensor is not None else '<none>'
         return f'{super().__str__()}: {tstr}'
 
     def __repr__(self):
         return self.__str__()
 
@@ -148,16 +158,19 @@
 @dataclass
 class SparseTensorFeatureContext(FeatureContext):
     """Contains data that was encded from a dense matrix as a sparse matrix and
     back.  Using torch sparse matrices currently lead to deadlocking in child
     proceesses, so use scipy :class:``csr_matrix`` is used instead.
 
     """
-    USE_SPARSE = True
+    USE_SPARSE: ClassVar[bool] = True
+    """Whether or not to enable sparse matrix serialization.  Otherwise, torch
+    tensors (no conversion) are used.
 
+    """
     sparse_data: Union[Tuple[Tuple[csr_matrix, int]], Tensor] = field()
     """The sparse array data."""
 
     @property
     def sparse_arr(self) -> Tuple[csr_matrix]:
         assert isinstance(self.sparse_data[0], tuple)
         return self.sparse_data[0]
@@ -181,23 +194,40 @@
     @classmethod
     def instance(cls, feature_id: str, arr: Tensor,
                  torch_config: TorchConfig):
         arr = arr.cpu()
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'encoding in to sparse tensor: {arr.shape}')
         if cls.USE_SPARSE:
-            sarr = cls.to_sparse(arr)
+            if arr.dtype == torch.float16:
+                raise VectorizerError(
+                    ('Type float 16 is not supported (scipy 1.9.3); set ' +
+                     'SparseTensorFeatureContext.USE_SPARSE to False or ' +
+                     'deeplearn_default.fp_precision higher'))
+            with warnings.catch_warnings():
+                # silence the numpy warningsin scipy package
+                #
+                # scipy/sparse/_sputils.py:44: DeprecationWarning:
+                # np.find_common_type is deprecated.  Please use
+                # `np.result_type` or `np.promote_types`.
+                warnings.filterwarnings(
+                    'ignore',
+                    message=r"^np.find_common_type is deprecated",
+                    category=DeprecationWarning)
+                sarr = cls.to_sparse(arr)
         else:
             sarr = arr
         return cls(feature_id, sarr)
 
     def to_tensor(self, torch_config: TorchConfig) -> Tensor:
         if isinstance(self.sparse_arr, Tensor):
             tarr = self.sparse_arr
         else:
+            narr: Tuple[csr_matrix]
+            tdim: int
             narr, tdim = self.sparse_data
             narrs = tuple(map(lambda sm: torch.from_numpy(sm.todense()), narr))
             if len(narrs) == 1:
                 tarr = narrs[0]
             else:
                 tarr = torch.stack(narrs)
             dim_diff = len(tarr.shape) - tdim
```

## zensols/deeplearn/vectorize/manager.py

```diff
@@ -1,13 +1,12 @@
-from __future__ import annotations
 """Vectorization base classes and basic functionality.
 
 """
+from __future__ import annotations
 __author__ = 'Paul Landes'
-
 from typing import Tuple, Any, Set, Dict, List, Iterable
 from dataclasses import dataclass, field
 from abc import abstractmethod, ABCMeta
 import logging
 import sys
 from itertools import chain
 import collections
@@ -22,16 +21,16 @@
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class EncodableFeatureVectorizer(FeatureVectorizer, metaclass=ABCMeta):
-    """This vectorizer splits transformation up in to encoding and decoding.  The
-    encoded state as a ``FeatureContext``, in cases where encoding is
+    """This vectorizer splits transformation up in to encoding and decoding.
+    The encoded state as a ``FeatureContext``, in cases where encoding is
     prohibitively expensive, is computed once and pickled to the file system.
     It is then loaded and finally decoded into a tensor.
 
     Examples include computing an encoding as indexes of a word embedding
     during the encoding phase.  Then generating the full embedding layer during
     decoding.  Note that this decoding is done with a ``TorchConfig`` so the
     output tensor goes directly to the GPU.
@@ -43,31 +42,31 @@
     """
     manager: FeatureVectorizerManager = field()
     """The manager used to create this vectorizer that has resources needed to
     encode and decode.
 
     """
     def transform(self, data: Any) -> Tensor:
-        """Use the output of the encoding as input to the decoding to directly produce
-        the output tensor ready to be used in testing, training, validation
-        etc.
+        """Use the output of the encoding as input to the decoding to directly
+        produce the output tensor ready to be used in testing, training,
+        validation etc.
 
         """
         context = self.encode(data)
         return self.decode(context)
 
     def encode(self, data: Any) -> FeatureContext:
         """Encode data to a context ready to (potentially) be pickled.
 
         """
         return self._encode(data)
 
     def decode(self, context: FeatureContext) -> Tensor:
-        """Decode a (potentially) unpickled context and return a tensor using the
-        manager's :obj:`torch_config`.
+        """Decode a (potentially) unpickled context and return a tensor using
+        the manager's :obj:`torch_config`.
 
         """
         arr: Tensor = None
         self._validate_context(context)
         if isinstance(context, NullFeatureContext):
             pass
         elif isinstance(context, MultiFeatureContext) and context.is_empty:
@@ -188,28 +187,28 @@
     def __post_init__(self):
         super().__post_init__()
         self.manager_set = None
         self._vectorizers_pw = PersistedWork('_vectorizers_pw', self)
 
     def transform(self, data: Any) -> \
             Tuple[Tensor, EncodableFeatureVectorizer]:
-        """Return a tuple of duples with the output tensor of a vectorizer and the
-        vectorizer that created the output.  Every vectorizer listed in
+        """Return a tuple of duples with the output tensor of a vectorizer and
+        the vectorizer that created the output.  Every vectorizer listed in
         ``feature_ids`` is used.
 
         """
         return tuple(map(lambda vec: (vec.transform(data), vec),
                          self._vectorizers.values()))
 
     @property
     @persisted('_vectorizers_pw')
     def _vectorizers(self) -> Dict[str, FeatureVectorizer]:
-        """Return a dictionary of all registered vectorizers.  This includes both
-        module and configured vectorizers.  The keys are the ``feature_id``s
-        and values are the contained vectorizers.
+        """Return a dictionary of all registered vectorizers.  This includes
+        both module and configured vectorizers.  The keys are the
+        ``feature_id``s and values are the contained vectorizers.
 
         """
         return self._create_vectorizers()
 
     def _create_vectorizers(self) -> Dict[str, FeatureVectorizer]:
         vectorizers = collections.OrderedDict()
         feature_ids = set()
@@ -229,31 +228,31 @@
             inst = conf_instances.get(feature_id)
             vectorizers[feature_id] = inst
         return vectorizers
 
     @property
     @persisted('_feature_ids')
     def feature_ids(self) -> Set[str]:
-        """Get the feature ids supported by this manager, which are the keys of the
-        vectorizer.
+        """Get the feature ids supported by this manager, which are the keys of
+        the vectorizer.
 
         :see: :class:`.FeatureVectorizerManager`
 
         """
         return frozenset(self._vectorizers.keys())
 
     def get(self, name: str) -> FeatureVectorizer:
         """Return the feature vectorizer named ``name``."""
         fv = self._vectorizers.get(name)
         # if we can't find the vectorizer, try using dot syntax to find it in
         # the parent manager set
         if name is not None and fv is None:
             idx = name.find(self.MANAGER_SEP)
             if self.manager_set is not None and idx > 0:
-                mng_name, vec = name[:idx], name[idx+1:]
+                mng_name, vec = name[:idx], name[idx + 1:]
                 if logger.isEnabledFor(logging.DEBUG):
                     logger.debug(f'looking up {mng_name}:{vec}')
                 mng = self.manager_set.get(mng_name)
                 if mng is not None:
                     fv = mng._vectorizers.get(vec)
         return fv
 
@@ -339,16 +338,16 @@
             for vec in vm.values():
                 if name == vec.name:
                     return vec
 
     @property
     @persisted('_feature_ids')
     def feature_ids(self) -> Set[str]:
-        """Return all feature IDs supported across all manager registered with the
-        manager set.
+        """Return all feature IDs supported across all manager registered with
+        the manager set.
 
         """
         return set(chain.from_iterable(
             map(lambda m: m.feature_ids, self.values())))
 
     def __getitem__(self, name: str) -> FeatureVectorizerManager:
         mng = self._managers.get(name)
```

## zensols/deeplearn/vectorize/util.py

```diff
@@ -17,16 +17,16 @@
     """Encode a sequence of tensors, each of arbitrary dimensionality, as a 1-D
     array.  Then decode the 1-D array back to the original.
 
     """
     torch_config: TorchConfig
 
     def encode(self, arrs: Sequence[Tensor]) -> Tensor:
-        """Encode a sequence of tensors, each of arbitrary dimensionality, as a 1-D
-        array.
+        """Encode a sequence of tensors, each of arbitrary dimensionality, as a
+        1-D array.
 
         """
         def map_tensor_meta(arr: Tensor) -> Tuple[int]:
             sz = arr.shape
             tm = [len(sz)]
             tm.extend(sz)
             return tm
```

## zensols/deeplearn/vectorize/vectorizers.py

```diff
@@ -1,18 +1,20 @@
 """Vectorizer implementations.
 
 """
 __author__ = 'Paul Landes'
 
 from typing import Set, List, Iterable, Union, Any, Tuple, Dict
 from dataclasses import dataclass, field
+import sys
 import logging
 import pandas as pd
 import numpy as np
 import itertools as it
+from io import TextIOBase
 from sklearn.preprocessing import LabelEncoder
 import torch
 from torch import Tensor
 from torch import nn
 from zensols.persist import persisted
 from zensols.deeplearn import TorchTypes, TorchConfig
 from . import (
@@ -77,17 +79,25 @@
         """Return the label string values for indexes ``nominals``.
 
         :param nominals: the integers that map to the respective string class
 
         """
         return self.label_encoder.inverse_transform(nominals)
 
+    def write(self, depth: int = 0, writer: TextIOBase = sys.stdout):
+        super().write(depth, writer)
+        le: LabelEncoder = self.label_encoder
+        self._write_line('labels:', depth, writer)
+        for cat, ix in zip(le.classes_, le.transform(le.classes_)):
+            self._write_line(f'{cat}: {ix}', depth + 1, writer)
+
 
 @dataclass
-class NominalEncodedEncodableFeatureVectorizer(CategoryEncodableFeatureVectorizer):
+class NominalEncodedEncodableFeatureVectorizer(
+        CategoryEncodableFeatureVectorizer):
     """Map each label to a nominal, which is useful for class labels.
 
     :shape: (1, 1)
 
     """
     DESCRIPTION = 'nominal encoder'
 
@@ -145,17 +155,18 @@
                 he[row][idx] = 1
             del arr
             arr = he
         return arr
 
 
 @dataclass
-class OneHotEncodedEncodableFeatureVectorizer(CategoryEncodableFeatureVectorizer):
-    """Vectorize from a list of nominals.  This is useful for encoding labels for
-    the categorization machine learning task.
+class OneHotEncodedEncodableFeatureVectorizer(
+        CategoryEncodableFeatureVectorizer):
+    """Vectorize from a list of nominals.  This is useful for encoding labels
+    for the categorization machine learning task.
 
     :shape: (1,) when optimizing bools and classes = 2, else (1, |categories|)
 
     """
     DESCRIPTION = 'category encoder'
 
     optimize_bools: bool = field()
@@ -287,50 +298,50 @@
             sz = max(map(lambda t: t.size(0), srcs))
         arr = self.create_padded_tensor((clen, sz, *mid_dims), dtype)
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'num contexts: {clen}, dtype={dtype}, ' +
                          f'src={first.shape}, dst={arr.shape}, ' +
                          f'mid_dims={mid_dims}')
         rowix = 0
-        ctx: TensorFeatureContext
         for carr in srcs:
             lsz = min(carr.size(0), sz)
             if carr.dim() == 1:
                 arr[rowix, :lsz] = carr[:lsz]
             elif carr.dim() == 2:
                 arr[rowix, :lsz, :] = carr[:lsz, :]
             elif carr.dim() == 3:
                 arr[rowix, :lsz, :, :] = carr[:lsz, :, :]
             rowix += 1
         return arr
 
 
 @dataclass
 class MaskFeatureContext(FeatureContext):
-    """A feature context used for the :class:`.MaskFeatureVectorizer` vectorizer.
+    """A feature context used for the :class:`.MaskFeatureVectorizer`
+    vectorizer.
 
     :param sequence_lengths: the lengths of all each row to mask
 
     """
     sequence_lengths: Tuple[int]
 
 
 @dataclass
 class MaskFeatureVectorizer(EncodableFeatureVectorizer):
-    """Creates masks where the first N elements of a vector are 1's with the rest
-    0's.
+    """Creates masks where the first N elements of a vector are 1's with the
+    rest 0's.
 
     :shape: (-1, size)
 
     """
     DESCRIPTION = 'mask'
 
     size: int = field(default=-1)
-    """The length of all mask vectors or ``-1`` make the length the max size of the
-    sequence in the batch.
+    """The length of all mask vectors or ``-1`` make the length the max size of
+    the sequence in the batch.
 
     """
     data_type: Union[str, None, torch.dtype] = field(default='bool')
     """The mask tensor type.  To use the int type that matches the resolution of
     the manager's :obj:`torch_config`, use ``DEFAULT_INT``.
 
     """
@@ -374,17 +385,17 @@
         for bix, slen in enumerate(lens):
             arr[bix, :slen] = ones[:slen]
         return arr
 
 
 @dataclass
 class SeriesEncodableFeatureVectorizer(EncodableFeatureVectorizer):
-    """Vectorize a Pandas series, such as a list of rows.  This vectorizer has an
-    undefined shape since both the number of columns and rows are not specified
-    at runtime.
+    """Vectorize a Pandas series, such as a list of rows.  This vectorizer has
+    an undefined shape since both the number of columns and rows are not
+    specified at runtime.
 
     :shape: (-1, 1)
 
     """
     DESCRIPTION = 'pandas series'
 
     def _get_shape(self):
```

## Comparing `zensols.deeplearn-1.8.1.dist-info/METADATA` & `zensols.deeplearn-1.9.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,34 @@
 Metadata-Version: 2.1
 Name: zensols.deeplearn
-Version: 1.8.1
+Version: 1.9.0
 Summary: General deep learing utility library
 Home-page: https://github.com/plandes/deeplearn
-Download-URL: https://github.com/plandes/deeplearn/releases/download/v1.8.1/zensols.deeplearn-1.8.1-py3-none-any.whl
+Download-URL: https://github.com/plandes/deeplearn/releases/download/v1.9.0/zensols.deeplearn-1.9.0-py3-none-any.whl
 Author: Paul Landes
 Author-email: landes@mailc.net
 Keywords: tooling
 Description-Content-Type: text/markdown
-Requires-Dist: numpy (~=1.23.1)
-Requires-Dist: scipy (~=1.9.3)
-Requires-Dist: scikit-learn (~=1.1.2)
-Requires-Dist: pandas (~=1.4.0)
-Requires-Dist: matplotlib (~=3.5.2)
-Requires-Dist: torch (~=1.13.1)
-Requires-Dist: torchvision (~=0.14.0)
-Requires-Dist: zensols.install (~=1.0.0)
-Requires-Dist: zensols.datdesc (~=0.1.0)
+Requires-Dist: tqdm
+Requires-Dist: numpy ~=1.25.2
+Requires-Dist: scipy ~=1.9.3
+Requires-Dist: scikit-learn ~=1.3.2
+Requires-Dist: pandas ~=2.1.3
+Requires-Dist: matplotlib ~=3.8.2
+Requires-Dist: torch ~=2.1.1
+Requires-Dist: torchvision ~=0.16.1
+Requires-Dist: zensols.util ~=1.14.0
+Requires-Dist: zensols.install ~=1.1.0
+Requires-Dist: zensols.datdesc ~=0.2.0
 
 # DeepZensols Deep Learning Framework
 
 [![PyPI][pypi-badge]][pypi-link]
-[![Python 3.9][python39-badge]][python39-link]
 [![Python 3.10][python310-badge]][python310-link]
+[![Python 3.11][python311-badge]][python311-link]
 [![Build Status][build-badge]][build-link]
 
 This deep learning library was designed to provide consistent and reproducible
 results.
 
 * See the [full documentation].
 * Paper on [arXiv](http://arxiv.org/abs/2109.03383).
@@ -153,53 +155,63 @@
 language processing that aids in feature engineering and embedding layers that
 builds on this project.
 
 
 ## Citation
 
 If you use this project in your research please use the following BibTeX entry:
+
 ```bibtex
-@article{Landes_DiEugenio_Caragea_2021,
-  title={DeepZensols: Deep Natural Language Processing Framework},
-  url={http://arxiv.org/abs/2109.03383},
-  note={arXiv: 2109.03383},
-  journal={arXiv:2109.03383 [cs]},
-  author={Landes, Paul and Di Eugenio, Barbara and Caragea, Cornelia},
-  year={2021},
-  month={Sep}
+@inproceedings{landes-etal-2023-deepzensols,
+    title = "{D}eep{Z}ensols: A Deep Learning Natural Language Processing Framework for Experimentation and Reproducibility",
+    author = "Landes, Paul  and
+      Di Eugenio, Barbara  and
+      Caragea, Cornelia",
+    editor = "Tan, Liling  and
+      Milajevs, Dmitrijs  and
+      Chauhan, Geeticka  and
+      Gwinnup, Jeremy  and
+      Rippeth, Elijah",
+    booktitle = "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
+    month = dec,
+    year = "2023",
+    address = "Singapore, Singapore",
+    publisher = "Empirical Methods in Natural Language Processing",
+    url = "https://aclanthology.org/2023.nlposs-1.16",
+    pages = "141--146"
 }
 ```
 
 
-## Community
+## Changelog
 
-Please star the project and let me know how and where you use this API.
-Contributions as pull requests, feedback and any input is welcome.
+An extensive changelog is available [here](CHANGELOG.md).
 
 
-## Changelog
+## Community
 
-An extensive changelog is available [here](CHANGELOG.md).
+Please star the project and let me know how and where you use this API.
+Contributions as pull requests, feedback and any input is welcome.
 
 
 ## License
 
 [MIT License]
 
 Copyright (c) 2020 - 2023 Paul Landes
 
 
 <!-- links -->
 [pypi]: https://pypi.org/project/zensols.deeplearn/
 [pypi-link]: https://pypi.python.org/pypi/zensols.deeplearn
 [pypi-badge]: https://img.shields.io/pypi/v/zensols.deeplearn.svg
-[python39-badge]: https://img.shields.io/badge/python-3.9-blue.svg
-[python39-link]: https://www.python.org/downloads/release/python-390
 [python310-badge]: https://img.shields.io/badge/python-3.10-blue.svg
-[python310-link]: https://www.python.org/downloads/release/python-310
+[python310-link]: https://www.python.org/downloads/release/python-3100
+[python311-badge]: https://img.shields.io/badge/python-3.11-blue.svg
+[python311-link]: https://www.python.org/downloads/release/python-3110
 [build-badge]: https://github.com/plandes/util/workflows/CI/badge.svg
 [build-link]: https://github.com/plandes/deeplearn/actions
 
 [MIT License]: LICENSE.md
 [PyTorch]: https://pytorch.org
 [Juypter]: https://jupyter.org
 [pycuda]: https://pypi.org/project/pycuda/
```

## Comparing `zensols.deeplearn-1.8.1.dist-info/RECORD` & `zensols.deeplearn-1.9.0.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,73 +1,73 @@
 zensols/dataframe/__init__.py,sha256=2z39y0_C7gGStJ7vxm7Djbb-wdRZvuW1sqgGTcwau1Q,43
 zensols/dataframe/config.py,sha256=Tmu1w5UTLUk2y1H_koTGWqfbkMnqcwHCgII_DU6I7Ss,2290
-zensols/dataframe/stash.py,sha256=qGTx23gV1pXg8mWZmtkwBzfSmE6wi7vCubv1bmkbkBE,8983
+zensols/dataframe/stash.py,sha256=DoBYyDgFi0rU_SZa71idR2pD5sSwMUQvC-I4A3IlfCg,9078
 zensols/dataset/__init__.py,sha256=zplEsCZ25J3zFHmnEes1ObtBcnd4ki-l3tIZUlO81hE,139
-zensols/dataset/dimreduce.py,sha256=Oj_MtelM8SWgpM_q_iQtxvPuDU6oSwxjJrQMYx2GjFg,6619
+zensols/dataset/dimreduce.py,sha256=66c6J4OctV7Qn28JgbMoygkzyCT3jRAwwPDAJOhNe_I,6618
 zensols/dataset/interface.py,sha256=lkyIHWd48JcVPj4phP1HFDKIQVG6evlWncY8nT0jW3Q,3266
-zensols/dataset/leaveout.py,sha256=mubHPcEwJzUoNycKEET57su0t2gb515ndcw4uZR3f5g,3211
-zensols/dataset/outlier.py,sha256=41dVfILnL0R3n5brr5063jWFkRQfD-IoGL4Jr04MXKo,8343
-zensols/dataset/split.py,sha256=rgMUe1Znv8jbZ42JB9K8ngXDKqq_ZkujCT-LcXBco58,10965
-zensols/dataset/stash.py,sha256=VRUZwyuI79UyFDoLAADeaddFOcDe4aGHfn04wS4hqWY,9725
+zensols/dataset/leaveout.py,sha256=qkRJPfkxwyYbmo4MGk-AWXSM0pZ5ik7mnZsBeFLhQgE,3211
+zensols/dataset/outlier.py,sha256=hOQWCucMfzGuZpDhcohN9aGOAUwV5uy75AuTGa_WmSE,8342
+zensols/dataset/split.py,sha256=xXEvEBIVKklzpePWpYeK19keKvAXx37KsCnZQCsRdw4,10967
+zensols/dataset/stash.py,sha256=8zwLHDjkayGHT2Q7bPgN0A9A9W_SMJGO7VCz2V870pc,9733
 zensols/deeplearn/__init__.py,sha256=MJ8nKg1_nGfg2NHVI6_XzXWducfsOV3FZ1-XgS5H72w,202
 zensols/deeplearn/domain.py,sha256=adJW0mOMO6opc632_UYpNd62dS6z_jE4TSLs1quLoDg,16052
 zensols/deeplearn/observer.py,sha256=BNS8tnzvHsEbN9s0BGSeRG5GbycTkKJlSU7PRSGc5RU,8475
-zensols/deeplearn/torchconfig.py,sha256=XWrwZlluboZfVPjjkSWUk4dsKuEGseJFXA6MvdDwEUA,22127
-zensols/deeplearn/torchtype.py,sha256=DgAgFqarbhn58XmAzllEwsW8x4cbGcJYBt7xVs7oo7g,5141
+zensols/deeplearn/torchconfig.py,sha256=6dEjEGac2vipa1XpznyBA0ADqdy2Aj78cTgvhJNI0BY,22566
+zensols/deeplearn/torchtype.py,sha256=i_uVc7ml2_jl37czosQVSYT6Bpgfm-HljGWPHdKVl0k,5141
 zensols/deeplearn/batch/__init__.py,sha256=4Xov58tGi7AARHdQj-XkLQ5q8_4Ad7hwArsmZl5tvw4,208
-zensols/deeplearn/batch/domain.py,sha256=5-6ovqRUoI17pSAQnFMt8qTYLdnRBIKnHxtyxx55dZQ,21605
+zensols/deeplearn/batch/domain.py,sha256=dVu9FdBb058igAXzynWSKlSizx29Gn2qroVz5gaqnm0,21604
 zensols/deeplearn/batch/interface.py,sha256=grrAPStPgN9kuBnTQhShdGhinlTyLvys8BB4wXfKs6E,1569
 zensols/deeplearn/batch/mapping.py,sha256=MEdjnoItEDe5Cg2HznGmXphW7qkrVt85h5ENCWi1u0A,7202
 zensols/deeplearn/batch/meta.py,sha256=yQQ2BNItpsuvQTB_zmCN28s-4fSmEpwHUgSoc0lReg8,2904
 zensols/deeplearn/batch/multi.py,sha256=TuHGvz8bsJDaZ6KX2JBzFUZ3xPIfPuJmIEcYdxUqR3M,1738
-zensols/deeplearn/batch/stash.py,sha256=PVDUJlePLoU0a-AsQ-s5fwWOFcL2W-AAcSxfDJVTIUo,15680
+zensols/deeplearn/batch/stash.py,sha256=a_oTWPNv8eHAZeXDkTtSJC-uQEMwnlpJDlhwk1CkKk4,15695
 zensols/deeplearn/cli/__init__.py,sha256=K3UdhLC275kO67S3w-Y3kweoAbrk9EegwNaCRpAx7d0,19
 zensols/deeplearn/cli/app.py,sha256=mL0yuLHWeetZEih2hkXFAz5X4EJjw_YLSURHwESmGYU,31746
 zensols/deeplearn/dataframe/__init__.py,sha256=dR6Uauwp-HnR2CGqOWRrUmGyfdsrFCyNNNS13P-oH8g,212
-zensols/deeplearn/dataframe/batch.py,sha256=KbNZJ56y3E3qVwD-wnPVywBS05p7k5YGhTr1M9AlzNo,3720
+zensols/deeplearn/dataframe/batch.py,sha256=GWzeQVIRw0xPmhIhf4B4yykncIvkgRuffCdQd2Xhmds,3721
 zensols/deeplearn/dataframe/util.py,sha256=GfHOrH-OYlIDR5WDw8pDfedfpM5QGZJDHh6ewita6MM,1122
-zensols/deeplearn/dataframe/vectorize.py,sha256=-HI-Ijcld7wm1q9Wg5STBzN8ehs2dWWSaBbDHqWopDU,9426
+zensols/deeplearn/dataframe/vectorize.py,sha256=kT9W1Lwgg2fxx0QX65LbjnyWti6Z4kEgj3VArBuwNlE,9427
 zensols/deeplearn/layer/__init__.py,sha256=nORiyKZGQwuvdCGJ3lYLkKihcvpaw4CrP42D2x4xUiM,359
 zensols/deeplearn/layer/conv.py,sha256=ciNMBVh8HJUMpQl5900YxHNuPR2bqsnkMSm9MrBUPFw,9186
 zensols/deeplearn/layer/crf.py,sha256=SrgnCJXZ6Jwtkj_Alg45tjUQyGmTP9ADdqHx9bP2L-U,17314
 zensols/deeplearn/layer/linear.py,sha256=wmfdPBhwL6gjAloDCJQqKDqYmirpvYPCfz0BoUmsXcI,8331
 zensols/deeplearn/layer/recur.py,sha256=G8kSZEndOna1ldZlximg0I40qjiqzhLnEU6a4yFBnw0,4297
 zensols/deeplearn/layer/recurcrf.py,sha256=QSE9VWjnV0j2d48Uwrsx56EyUBK8G-NWqmShYF1OI4s,7346
 zensols/deeplearn/model/__init__.py,sha256=5GntoQtMdTHqgKA-hLQ9cE-EIAZ9nI6BH6YPEdc-YMk,491
 zensols/deeplearn/model/analyze.py,sha256=wjiyWUhVJ0rLaPgEb_uCoWnNM-2zWDaDS0W5RBZxEIQ,4613
 zensols/deeplearn/model/batchiter.py,sha256=F8Hpv_nJRLN0swLYZql1XmlhovNHICzXjKvIFa6chDg,10586
 zensols/deeplearn/model/executor.py,sha256=1EfIjSvJ__U8rcldyQcHhuzPWccJ0eVY_vZi9yjnzws,36975
-zensols/deeplearn/model/facade.py,sha256=YI3AXAw49bR9PX0uWhTfm9RA8u8qpxwvYBNvHpjOI4U,32127
+zensols/deeplearn/model/facade.py,sha256=kDZp0wrTdT7M5rVl7iALDrCqZ30rKi8p2ONm9Fotdlc,32169
 zensols/deeplearn/model/format.py,sha256=2u7hyiUx11rStPAh8gZYav339ya0mbGkZ--yCxwuOIg,10075
 zensols/deeplearn/model/manager.py,sha256=-m4hBcRkGh7LTgbCKFCycIMbUrrWjRofzBbFDjeGeCU,11383
 zensols/deeplearn/model/meta.py,sha256=KjZ_4MWwWtWHY4-RoJ2MX5fMq4SBfO6uq2DbpsKSqQA,958
 zensols/deeplearn/model/module.py,sha256=DTgaGiwCbCgsxT2pXOv6Nu4n7CrIT7Te0UZWqmvOKrQ,8506
 zensols/deeplearn/model/optimizer.py,sha256=wB4liISNb3LeXmxCBadFGRlMOQ5istLGkdAup75oupY,516
 zensols/deeplearn/model/pack.py,sha256=IRM9kulQCPclN5COcpA8Y4NQsVa9O8Sh0GmUQc1qcwA,5010
 zensols/deeplearn/model/pred.py,sha256=in9PLQMDHCBxb8ml80YP1d8iPACUKZ__2aJnjNy5cQ0,3816
 zensols/deeplearn/model/sequence.py,sha256=LQl_I877uHZ06VxhmxBRHIjDn6FmHiKppnSHj6wAPfY,7204
 zensols/deeplearn/model/trainmng.py,sha256=4_RHmULxq8Tlj-y-iTaAaygTLJq_dpTgGkrXif-9fEo,10544
 zensols/deeplearn/model/wgtexecutor.py,sha256=gn6N9F1fbYZZV07VWdZzUsSgoPID_FkH4I4lH3pqcvA,3710
 zensols/deeplearn/resources/batch.conf,sha256=znXVhkD6CLwRZjOzbj8wV7PG6GxkXjxuPRBLmoXKJ4I,3865
 zensols/deeplearn/resources/cli-pack.conf,sha256=z9TQR62LQ7XXM2YVLP0MKSqPC4S4ynbjs-gs9QIS8OE,329
 zensols/deeplearn/resources/cli.conf,sha256=6sqtbeKX5EPWn5thCZCXXRdv3H-8pIsbpRcIOVGQX6M,1004
-zensols/deeplearn/resources/default.conf,sha256=HQpALN19aS7SdHXIdU4ghG6SawzI8vDTxJn5PyA18eI,297
+zensols/deeplearn/resources/default.conf,sha256=jN3RMXf-93XKIb8FHVSmk2IVTxoLJbB95c2igRbAVG4,315
 zensols/deeplearn/resources/model.conf,sha256=dy8KTpdC_JjQpWmp_QB8SQsZWhGVaDQbPku4T6CApyM,4391
 zensols/deeplearn/resources/obj.conf,sha256=XeCb43CSnoq2elrNVhhFqEgk9YXGL6Ia9Acrel3Abzo,316
 zensols/deeplearn/resources/observer.conf,sha256=m5ybJmtYWy16e_PJzDPPObaR6brL-qYwdnHXglXfW4k,1015
-zensols/deeplearn/resources/torch.conf,sha256=hgjacnDz3DdqIZJI-XwiqifPaO4G_KHSo3OOII8B5Hg,399
+zensols/deeplearn/resources/torch.conf,sha256=dwDkaIGEHP0PqjzcDeTuQz7oUVSyErTPSjPBB89wg8Q,461
 zensols/deeplearn/result/__init__.py,sha256=MIurqikLNDTvP6ponrF-zC1b2JAXfSBNQXCc7IZ6dkI,269
 zensols/deeplearn/result/compare.py,sha256=O5dToC5a2BA5NU6WopmvXUdyxx6DmbDDg9cov0Cv5_k,1371
-zensols/deeplearn/result/domain.py,sha256=00lX14nWGxiSJzZWvOmYhfAzOk5YzLkAaET-hI7phT8,34990
-zensols/deeplearn/result/manager.py,sha256=MxilsQvMZAl4xxoqBFrgqXZ0VzXVXEZWFBRcn6mtzDM,8940
-zensols/deeplearn/result/plot.py,sha256=VreEbGVNmH9L55kP2QCC-ob5ToG9HDaJmkI_WQAP4Dg,3631
-zensols/deeplearn/result/pred.py,sha256=WHb1G3ZLuf-IYm-x1-CcvT9UWaP4JbEkOBb8ZbDJeDE,13719
-zensols/deeplearn/result/report.py,sha256=ZFCYdhhRvMIZlEUM17wXA8k9AspIZTlxkFJ4vfsXoHY,5130
+zensols/deeplearn/result/domain.py,sha256=Y7Lz2NA5fDx6nQ2rdIeW80W-B8oFWeHitrFQTcwOxDg,34989
+zensols/deeplearn/result/manager.py,sha256=vGARsgZVQJtDRli31TD8OVFMqWwQnFhtts8Vz6L34-E,8939
+zensols/deeplearn/result/plot.py,sha256=YXTWHRth-WO0yR1GvBE1zki0L1FKd-XO209XTEjwcZQ,3782
+zensols/deeplearn/result/pred.py,sha256=d6JsUapw1iEvzKEPIhDflBz4jc0_DkRxyCRBt1r8Lrk,13689
+zensols/deeplearn/result/report.py,sha256=I3Ne2KgMGY3qT66D4x03NSwVV1dq54Em1zYzDRhkJ-Y,5188
 zensols/deeplearn/vectorize/__init__.py,sha256=hKXSHSnZmp_3HeP6rkCg_Pxcu-IJvx1iAIwz86iTiAE,198
-zensols/deeplearn/vectorize/domain.py,sha256=x4atKTbQqheFBirWsYEYiEMLTUpA3AayE5jKziNqSqA,6704
-zensols/deeplearn/vectorize/manager.py,sha256=w3m_O_PvugPAcAawLnTpBVPJ2XYm0L6RWdeLmW1IwL0,13990
-zensols/deeplearn/vectorize/util.py,sha256=KYjRXUsK7G3QZdpP31QN3Ad8O3Omf4Rq4KhVKZeYi-w,2061
-zensols/deeplearn/vectorize/vectorizers.py,sha256=-rHDN33Pj59V_d0ULbV3RcZmNGD2NPCW4Z4jBUfDhOE,14594
-zensols.deeplearn-1.8.1.dist-info/METADATA,sha256=66Bi5bjF_VvCSA_t3GbjDh5CLLz5NRQDcV4uRzuay90,10565
-zensols.deeplearn-1.8.1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-zensols.deeplearn-1.8.1.dist-info/top_level.txt,sha256=IEecO7gsWiRRRw2vk-fUv1-u_lJJL7oZyNSqqj1LqOI,52
-zensols.deeplearn-1.8.1.dist-info/RECORD,,
+zensols/deeplearn/vectorize/domain.py,sha256=oZ6yI_R1tm7qBUJUxhx4MGpDcIYaplpxztKEtYmzUc8,8251
+zensols/deeplearn/vectorize/manager.py,sha256=US01-6W9KDSqNccpVVZY4zRQztBWGZrxAZJsmoFz6iA,13990
+zensols/deeplearn/vectorize/util.py,sha256=8edr5EyjdotnsWAyMZinigppc0v9OvtuaSEAshkWrMQ,2061
+zensols/deeplearn/vectorize/vectorizers.py,sha256=tswDXxyoZdeuh81Dc0dI5RB16p3uqUSh4ZGHQOA9rZ0,14956
+zensols.deeplearn-1.9.0.dist-info/METADATA,sha256=5R_yXQB1tou7kAAqD1DpX5K_YzHAI1scISbY50ftEBI,11041
+zensols.deeplearn-1.9.0.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
+zensols.deeplearn-1.9.0.dist-info/top_level.txt,sha256=IEecO7gsWiRRRw2vk-fUv1-u_lJJL7oZyNSqqj1LqOI,52
+zensols.deeplearn-1.9.0.dist-info/RECORD,,
```

